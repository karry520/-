\section{图模型中的推断}
我们现在考虑图模型中的推断问题，图中的一些结点被限制为观测值，我们想要计算其他结点中的一个或多个子集的后验概率分布。正如我们看到的那样，我们可以利用图结构找到高效的推断算法，也可以让这些算法的结构变得透明。具体地说，我们会看到许多算法可以用图中局部信息传播的方式表示。本节中，我们会把注意力主要集中于精确推断的方法。后面的章节中，我们会考虑许多近似推断的算法。

首先，考虑贝叶斯定理的图表示。假设我们将两个变量$x$和$y$上的联合概率分布$p(x,y)$分解为因子的乘积的形式$p(x,y)=p(x)p(y|x)$。这可以用图表示。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\node[state,red] (A) {$x$};
		\node[state,red] (B) [below of=A]{$y$};
		\path (A) edge[->,semithick] (B);
	\end{tikzpicture}
\end{center}
现在假设我们观测到了$y$的值，如图所示
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\node[state,red] (A) {$x$};
		\node[state,red,fill=gray!50] (B) [below of=A] {$y$};
		
		\path (A) edge[->] (B);
	\end{tikzpicture}
\end{center}
我们可以将边缘概率分布$p(x)$看成潜在变量$x$上的先验概率分布，我们的目标是推断$x$上对应的后验概率分布。使用概率的加和规则和乘积规则，我们可以计算 
\begin{equation}
	p(y) =\sum_{x^{'}}p(y|x^{'})p(x^{'})
\end{equation}
这个式子然后被用于贝叶斯定理中，计算 
\begin{equation}
	p(x|y)=\frac{p(y|x)p(x)}{p(y)}
\end{equation}
因此现在联合概率分布可以通过$p(y)$和$p(x|y)$。从图的角度看，联合概率分布$p(x,y)$现在可以表示为下图，其中箭头的方向翻转了。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
	\node[state,red] (A) {$x$};
	\node[state,red,fill=gray!50] (B) [below of=A] {$y$};
	
	\path (B) edge[->] (A);
	\end{tikzpicture}
\end{center}
这是图模型中推断问题的最简单的例子。
\subsection*{链推断}
现在考虑一个更加复杂的问题，涉及到图示中的结点链。这个例子是本节中对更一般的图的精确推断的讨论的基础。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\tikzstyle{every node} = [color=red]
		
		\node[state] (A) {};
		\node[state] (B) [right of=A]{};
		\node (C) [right of=B]{$\dots$};
		\node[state] (D) [right of=C]{};
		\node[state] (E) [right of=D]{};
		
		\path (A) edge (B)
			  (B) edge (C)
			  (C) edge (D)
			  (D) edge (E);
	\end{tikzpicture}
\end{center}
这个图的联合概率分布形式为
\begin{equation}
\label{ruf}
	p(\boldsymbol{x})=\frac{1}{Z}\psi_{1,2}(x_1,x_2)\psi_{2,3}(x_2,x_3)\dots\psi_{N-1,N}(x_{N-1},x_N)
\end{equation}
我们考虑一个具体的情形，即N个结点表示N个离散变量，每个变量都有K个状态。这种情况下的势函数$\psi_{n-1,n}(x_{n-1},x_n)$由一个$K\times K$的表组成，因此联合概率分布有$(N-1)K^2$个参数。

让我们考虑寻找边缘概率分布$p(x_n)$这一推断问题，其中$x_n$是链上的一个具体的结点。注意，现阶段，没有观测结点。根据定义，这个边缘概率分布可以通过对联合概率分布在除$x_n$以外的所有变量上进行求和的方式得到。
\begin{equation}
\label{daif}
	p(x_n)=\sum_{x_1}\dots\sum_{x_{n-1}}\sum_{x_{n+1}}\dots\sum_{x_N}p(\boldsymbol{x})
\end{equation}
在一个朴素的实现中，我们首先计算联合概率分布，然后显式地进行求和。联合概率分布可以表示为一组数，对应于$\boldsymbol{x}$的每个可能的值。因为有N个变量，每个变量有K个可能的状态，因此$\boldsymbol{x}$有$K^N$个可能的值，从而联合概率的计算和存储以及得到$p(x_n)$所需的求和过程，涉及到的存储量和计算量都会随着链的长度N而指数增长。

然而，通过利用图模型的条件独立性质，我们可以得到一个更加高效的算法。如果我们将联合概率分布的分解表达式$\ref{ruf}$代入到公式$\ref{daif}$中，那么我们可以重新整理加和与乘积的顺序，使得需要求解的边缘分布可以更加高效地计算。例如，考虑对$x_N$的求和。势函数$\psi_{N-1,N}(x_{N-1},x_N)$是唯一与$x_N$有关系的势函数，因此我们可以进行下面的求和
\begin{equation}
	\sum_{x_N}\psi_{N-1,N}(x_{N-1},x_N)
\end{equation}
得到一个关于$x_{N-1}$的函数。$x_1$上的求和式只涉及到势函数$\psi_{1,2}(x_1,x_2)$，因此可以单独进行，得到$x_2$的函数，以此类推。因为每个求和式都移除了概率分布中的一个变量，因此这可以被看成从图中移除一个结点。

如果我们使用这种方式对势函数和求和式进行分组，那么我们可以将需要求解的边缘概率密度写成下面的形式
\begin{equation}
\label{www}
	\begin{aligned}
	p(x_n)=&\frac{1}{Z}\\
	&\underbrace{\left[\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1},x_n)\dots\left[\sum_{x_2}\psi_{2,3}(x_2,x_3)\left[\sum_{x_1}\psi_{1,2}(x_1,x_2) \right] \right]\dots \right]}_{\mu_\alpha(x_n)}\\
	&\underbrace{\left[\sum_{x_{n+1}}\psi_{n,n+1}(x_{n},x_{n+1})\dots \left[\sum_{x_N}\psi_{N-1,N}(x_{N-1},x_N) \right]\dots \right]}_{\mu_\beta(x_n)}
	\end{aligned}
\end{equation}
这个重排列的方式，背后的思想组成了后续对于一般的加和-乘积算法的讨论的基础，这里，我们利用的关键概率是乘法对加法的分配率，即
\begin{equation}
	ab+ac=a(b+c)
\end{equation}
使用这种重排序的表达式之后，计算边缘概率分布所需的计算总代价是$O(NK^2)$。这是链长度的一个线性函数，与朴素方法的指数代价不同。

现在使用图中局部信息传递的思想，给出这种计算的一个强大的直观意义。根据公式$\ref{www}$，我们看到边缘概率分布$p(x_n)$的表达式分解成了两个因子的乘积乘以归一化常数
\begin{equation}
	p(x_n)=\frac{1}{Z}\mu_\alpha(x_n)\mu_\beta(x_n)
\end{equation}
我们把$\mu_\alpha(x_n)$看成从结点$x_{n-1}$到结点$x_n$的沿着链向前传递的信息。类似地，$\mu_\beta(x_n)$可以看成从结点$x_{n+1}$到结点$x_n$的沿着链向后传递的信息。

信息$\mu_\alpha(x_n)$可以递归地计算，因为 
\begin{equation}
\label{rang}
	\begin{aligned}
	\mu_\alpha(x_n)&=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1},x_n)\left[\sum_{x_{n-2}}\dots \right]\\
	&=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1},x_n)\mu_\alpha(x_{n-1})
	\end{aligned}
\end{equation}
因此我们首先计算 
\begin{equation}
	\mu_\alpha(x_2)=\sum_{x_1}\psi_{1,2}(x_1,x_2)
\end{equation}
然后重复应用公式$\ref{rang}$直到我们到达需要求解的结点。

类似地，信息$\mu_\beta(x_n)$可以递归的计算。计算方法为：从结点$x_N$开始，使用
\begin{equation}
	\begin{aligned}
	\mu_\beta(x_n)&=\sum_{x_{n+1}}\psi_{n,n+1}(x_n,x_{n+1})\left[\sum_{x_{n+2}}\dots \right]\\
	&=\sum_{x_{n+1}}\psi_{n,n+1}(x_n,x_{n+1})\mu_\beta(x_{n+1})
	\end{aligned}
\end{equation}
这种递归的信息传递如图所示。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\tikzstyle{every node} = [color=red]
		\node[state] (A) {$x_1$};
		\node (B) [right of=A]{$\dots$};
		\node[state] (C) [right of=B]{$x_{n-1}$};
		\node[state] (D) [right of=C]{$x_n$};
		\node[state] (E) [right of=D]{$x_{n+1}$};
		\node (F) [right of=E]{$\dots$};
		\node[state] (G) [right of=F]{$x_N$};
		
		\path (A) edge (B)
			(B) edge (C)
			(C) edge node[yshift=0.5cm,label=above:$\underrightarrow{\mu_\alpha(x_{n-1})}$]{} (D)
			(D) edge node[yshift=0.5cm,label=above:$\underleftarrow{\mu_\beta(x_n)}$]{}(E)
			(E) edge (F)
			(F) edge (G);
	\end{tikzpicture}
\end{center}
上图被称为马尔可夫链。对应的信息传递方程是马尔可夫过程的Chapman-Kolmogorov方程的一个例子。

现在我们想计算结点链中两个相邻结点的联合概率分布$p(x_{n-1},x_n)$。这类似于计算单一结点的边缘概率分布，区别在于现在有两个变量没有被求和出来。需要求解的边缘概率分布可以写成下面的形式
\begin{equation}
	p(x_{n-1},x_n)=\frac{1}{Z}\mu_\alpha(x_{n-1})\psi_{n-1,n}(x_{n-1},x_n)\mu_\beta(x_n)
\end{equation}
因此一旦我们完成了计算边缘概率分布所需的信息传递，我们就可以直接得到每个势函数中的所有变量上的联合概率分布。
\subsection*{树}
我们已经看到，一个由结点链组成的图的精确推断可以在关于结点数量的线性时间内完成，方法是使用通过链中信息传递表示的算法。更一般地，通过局部信息在更大的一类图中的传递，我们可以高效地进行推断。这类图被称为树(tree)。特别地，我们会对之前在结点链的情形中得到的信息传递公式进行简单的推广，得到加和-乘积算法(sum-product algorithm)，它为树结构图的精确推断提供了一个高效的框架。

三个树结构的例子。(a)一个无向树，(b)一个有向树，(c)一个有向多树
\begin{center}
	\begin{tikzpicture}[node distance=1.5cm]
	\tikzstyle{every node} = [color=red]
	
	\node[state] (A1) {};
	\node[state] (B1) [above left of=A1] {};
	\node[state] (C1) [above right of=A1]{};
	\node[state] (D1) [below left of=A1]{};
	\node[state] (E1) [below right of=A1]{};
	
	\path (A1) edge (B1) 
			   edge (C1) 
			   edge (D1) 
			   edge (E1);
	
	\node (N1) [right of=C1]{};
	\node (M1) [right of=N1]{};
	
	\node[state] (A2) [right of=M1]{};
	\node[state] (B2) [below left of=A2]{};
	\node[state] (C2) [below right of=A2]{};
	\node[state] (D2) [below left of=B2]{};
	\node[state,xshift=-0.5cm] (E2) [below right of=B2]{};
	\node[state,xshift=0.5cm] (F2) [below left of=C2]{};
	\node[state] (G2) [below right of=C2]{};
	
	
	\path (A2) edge[->] (B2)
		  	   edge[->] (C2)
		  (B2) edge[->] (D2)
		  	   edge[->] (E2)
		  (C2) edge[->] (F2)
		  	   edge[->] (G2);
	
	\node (N2) [right of=A2]{};
	\node (M2) [right of=N2]{};
	
	\node[state] (A3) [right of=M2]{};
	\node[state] (B3) [below right of=A3]{};
	\node[state] (C3) [above right of=B3]{};
	\node[state] (D3) [below left of=B3]{};
	\node[state] (E3) [below right of=B3]{};
	
	\path (A3) edge[->] (B3)
		  (C3) edge[->] (B3)
		  (B3) edge[->] (D3) edge[->] (E3);
	\end{tikzpicture}
\end{center}
\subsection*{因子图}
在推导加和-乘积算法之前，引入一个新的图结构，被称为因子图(factor graph)，那么算法的形式会变得特别简单并且具有一般性。

有向图和无向图都使得若干个变量的一个全局函数能够表示为这些变量的子集上的因子图的乘积。因子图显式地表示出了这个分解，方法是：在表示变量的结点的基础上，引入额外的结点表示因子本身。因子图也使我们能够更加清晰地了解分解的细节。

让我们将一组变量上的联合概率分布写成因子的乘积形式
\begin{equation}
\label{fasheng}
	p(\boldsymbol{x})=\prod_{s}f_s(\boldsymbol{x}_s)
\end{equation}
其中$\boldsymbol{x}_s$表示变量的一个子集。每个因子$f_s$是对应的变量集合$\boldsymbol{x}_s$的函数。

在因子图中，概率分布中的每个变量都有一个结点(与之前一样，用圆圈表示)，这与有向图和无向图的情形相同。还存在其他的结点(用小正方形表示)，表示联合概率分布中的每个因子$f_s(\boldsymbol{x}_s)$。最后，在每个因子结点和因子所依赖的变量结点之间，存在无向链接。例如，考虑一个表示为因子图形式的概率分布
\begin{equation}
	p(\boldsymbol{x})=f_a(x_1,x_2)f_b(x1,x_2)f_c(x_2,x_3)f_d(x_3) 
\end{equation}
这可表示为下图所示的因子图。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
	
		\node[state,color=red] (A) {$x_1$};
		\node[state,color=red] (B) [right of=A] {$x_2$};
		\node[state,color=red] (C) [right of=B] {$x_3$};
		
		\node[rectangle,color=red,fill=red,minimum size=0.5cm,label=below:$f_a$] (D) at (-1,-2) {};
		\node[rectangle,color=red,fill=red,minimum size=0.5cm,label=below:$f_b$] (E) at (1,-2) {};
		\node[rectangle,color=red,fill=red,minimum size=0.5cm,label=below:$f_c$] (F) at (3,-2) {};
		\node[rectangle,color=red,fill=red,minimum size=0.5cm,label=below:$f_d$] (G) at (5,-2) {};	
		
		\path (A) edge (D) edge (E)
			  (B) edge (D) edge (E) edge (F)
			  (C) edge (F) edge (G); 	
	\end{tikzpicture}
\end{center}
注意有两个因子$f_a(x_1,x_2)$和$f_b(x_1,x_2)$定义在同一个变量集合上。在一个无向图中，两个这样的因子的乘积被简单地合并到同一个团块势函数中。类似地，$f_c(x_2,x_3)$和$f_d(x_3)$可以结合到$x_2$和$x_3$上的一个单一势函数中。然而，因子图显示地写出这些因子，因此能够表达出关于分解本身的更加细节的信息。

如果我们有一个通过无向图表示的概率分布，那么我们可以将其转化为因子图。为了完成这一点，我们构造变量结点，对应于原始无向图，然后构造额外的因子结点，对应于最大团块$\boldsymbol{x}_s$。因子$f_s(\boldsymbol{x}_s)$被设置为与团块势函数相等。注意，对于同一个无向图，可能存在几个不同的因子图。下图说明了这些概念。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\tikzstyle{every node} = [color=red]
		
		\node[state] (A1) {$x_1$};
		\node[state,yshift=-0.5cm] (B1) [below right of=A1]{$x_2$};
		\node[state,yshift=0.5cm] (C1) [above right of=B1]{$x_3$};
		
		\path (A1) edge (B1) edge (C1)
			  (B1) edge (C1);
			
		\node[state] (A2) [right of=C1]{$x_1$};
		\node[state,yshift=-0.5cm] (B2) [below right of=A2]{$x_2$};
		\node[state,yshift=0.5cm] (C2) [above right of=B2]{$x_3$};
		
		\node[rectangle,draw,fill=red,minimum size=0.5cm,above of=B2,yshift=-0.7cm,label=above:$f$](R1){}; 
			
		\path (R1) edge (A2) edge (B2) edge (C2);	
			
		\node[state] (A3) [right of=C2]{$x_1$};
		\node[state,yshift=-0.5cm] (B3) [below right of=A3]{$x_2$};
		\node[state,yshift=0.5cm] (C3) [above right of=B3]{$x_3$};
		
		\node[rectangle,draw,fill=red,minimum size=0.5cm,above of=B3,yshift=-0.7cm,label=above:$f_a$](R2){}; 
		\node[rectangle,draw,fill=red,minimum size=0.5cm,below of=C3,yshift=0.7cm,label=right:$f_b$](R3){}; 
		
		\path (R2) edge (A3) edge (B3) edge (C3)
			  (R3) edge (B3) edge (C3);
	\end{tikzpicture}
\end{center}
(a)一个无向图，有一个单一的团块势函数$\psi(x_1,x_2,x_3)$。(b)一个因子图，因子$f(x_1,x_2,x_3)=\psi(x_1,x_2,x_3)$。(c)一个不同的因子图，表示相同的概率分布，它的因子满足$f_a(x_1,x_2,x_3)f_b(x_2,x_3)=\psi(x_1,x_2,x_3)$。

类似地，为了将有向图转化为因子图，我们构造变量结点对应于有向图中的结点，然后构造因子结点，对应于条件概率分布，最后添加上合适的链接。与之前一样，同一个有向图可能对应于多个因子图。有向图到因子图的转化如图所示。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
	\tikzstyle{every node} = [color=red]
	
	\node[state] (A1) {$x_1$};
	\node[state,yshift=-0.5cm] (B1) [below right of=A1]{$x_2$};
	\node[state,yshift=0.5cm] (C1) [above right of=B1]{$x_3$};
	
	\path (A1) edge[->] (B1) 
		  (C1) edge[->] (B1);
	
	\node[state] (A2) [right of=C1]{$x_1$};
	\node[state,yshift=-0.5cm] (B2) [below right of=A2]{$x_2$};
	\node[state,yshift=0.5cm] (C2) [above right of=B2]{$x_3$};
	
	\node[rectangle,draw,fill=red,minimum size=0.5cm,above of=B2,yshift=-0.7cm,label=above:$f$](R1){}; 
	
	\path (R1) edge (A2) edge (B2) edge (C2);	
	
	\node[state] (A3) [right of=C2]{$x_1$};
	\node[state,yshift=-0.5cm] (B3) [below right of=A3]{$x_2$};
	\node[state,yshift=0.5cm] (C3) [above right of=B3]{$x_3$};
	
	\node[rectangle,draw,fill=red,minimum size=0.5cm,above of=B3,yshift=-0.7cm,label=above:$f_c$](R2){}; 
	\node[rectangle,draw,fill=red,minimum size=0.5cm,below of=C3,label=right:$f_b$](R3){}; 
	\node[rectangle,draw,fill=red,minimum size=0.5cm,below of=A3,label=left:$f_a$](R4){}; 
		
	\path (R2) edge (A3) edge (B3) edge (C3)
		  (R3) edge (C3) 
		  (R4) edge (A3);
	\end{tikzpicture}
\end{center}
(a)一个有向图，可以分解为$p(x_1)p(x_2)p(x_3|x_1,x_2)$。(b)一个因子图，表示与有向图相同的概率分布，它的因子满足$f(x_1,x_2,x_3)=p(x_1)p(x_2)p(x_3|x_1,x_2)$。(c)一个不同的因子图，表示同样的概率分布，因子为$f_a(x_1)=p(x_1),f_b(x_2)=p(x_2),f_c(x_1,x_2,x_3)=p(x_3|x_1,x_2)$。

我们已经看到了树结构图对于进行高维推断的重要性。如果我们将一个有向树或者无向树转化为因子图，那么生成的因子图也是树(即，因子图没有环，且任意两个结点之间有且只有一条路径)。在有向多树的情形中，由于“伦理”的步骤的存在，转化为无向图会引入环，而转化后的因子图仍然是树。事实上，有向图中由于链接父结点和子结点产生的局部环可以转换到因子图时被移除，只需定义合适的因子函数即可。
\subsection*{加和-乘积算法}
我们会使用因子图框架推导一类强大的、高效的精确推断算法，这些算法适用于树结构的图。这里，我们把注意力集中于计算结点或者结点子集上的局部边缘概率分布，这会引出加和-乘积算法(sum-product algorithm)。稍后，我们会修改这个方法，使得概率最大的状态被找到，这就引出了最大加和算法(max-sum algorithm)。

我们假设原始的图是一个无向树或者有向树或者多树，从而对应的因子图有一个树结构。首先，我们将原始的图转化为因子图，使得我们可以使用同样的框架处理有向模型和无向模型。我们的目标是利用图的结构完成两件事：
\begin{enumerate}
	\item 得到一个高效的精确推断算法来寻找边缘概率；
	\item 在需要求解多个边缘概率的情形，计算可以高效地共享。
\end{enumerate}
首先，对于特定的变量结点$x$，我们寻找边缘概率$p(x)$。现阶段，我们假设所有的变量都是隐含变量。稍后我们会看到如何修改这个算法，使得观测变量被整合到算法中。根据定义，边缘概率分布通过对所有$x$之外的变量上的联合概率分布进行求和的方式得到，即
\begin{equation}
\label{jiao}
	p(x)=\sum_{\boldsymbol{x}\backslash x}p(\boldsymbol{x})
\end{equation}
其中$\boldsymbol{x}\backslash x$表示变量$\boldsymbol{x}$的集合去掉变量$x$。算法的思想是使用因子图的表达式$\ref{fasheng}$替换$p(\boldsymbol{x})$，然后交换加和与乘积的顺序，得到一个高效的算法。

考虑下图
\begin{center}
	\begin{tikzpicture}[node distance=1.5cm]
		\tikzstyle{every node} = [color=red]
		
		\node[rectangle,minimum size=0.5cm,fill=red,draw,label=below:$f_s$](A) {};
		
		\node[state,above left of=A](C) {};
		\node[state,below left of=A](D) {};
		
		\path (A) edge (C) edge (D)
			  (C) edge[dashed] (D);
		
		\begin{pgfonlayer}{background}
			\node[fill=gray!25,ellipse,draw,fit=(A)(C)(D)](P){};
		\end{pgfonlayer}
		\node[color=black,left of=A,xshift=-0.2cm]{\rotatebox{90}{$F_s(x,X_s)$}};
		
		\node[state,right of=A](E) {$x$};
		\path (A) edge node[color=black,above,yshift=0.3cm] {$\underrightarrow{\mu_{f_{s\to x}}(x)}$} (E);
		
		\node[rectangle,fill=red,draw,minimum size=0.5cm,above right of=E] (F) {};
		\node[rectangle,fill=red,draw,minimum size=0.5cm,below right of=E] (G) {};
		
		\path (E) edge (F) edge (G)
			  (F) edge[dashed] (G);		
	\end{tikzpicture}
\end{center}
我们看到图的树结构使得我们可以将联合概率分布中的因子划分为若干组，每组对应于变量结点$x$的相邻结点组成的因子结点集合。我们看到联合概率分布可以写成乘积的形式
\begin{equation}
\label{huan}
	p(\boldsymbol{x})=\prod_{s\in \mathrm{ne}(x)}F_s(x,X_s)
\end{equation}
其中$\mathrm{ne}(x)$表示与$x$相邻的因子结点的集合，$X_s$表示子树中通过因子结点$f_s$与变量结点$x$相连的所有变量的集合，$F_s(x,X_s)$表示分组中与因子$f_s$相关联的所有因子的乘积。

将公式$\ref{huan}$代入$\ref{jiao}$，交换加和与乘积的顺序，我们有
\begin{equation}
	\begin{aligned}
		p(x)&=\prod_{s\in \mathrm{ne}(x)}\left[\sum_{\boldsymbol{X}_s}F_s(x,X_s) \right]\\
		&=\prod_{s\in \mathrm{ne}(x)}\mu_{f_{s\to x}}(x)
	\end{aligned}
\end{equation}
这里我们引入了一组新的函数$\mu_{f_{s\to x}}(x)$，定义为
\begin{equation}
\label{juli}
	\mu_{f_{s\to x}}(x)\equiv \sum_{\boldsymbol{X}_s}F_s(x,X_s)
\end{equation}
这可以被看做从因子结点$f_s$到变量结点$x$的信息(message)。我们看到，需要求解的边缘概率分布$p(x)$等于所有到达结点$x$的输入信息的乘积。

为了计算这些信息，我们再次回到上图。我们注意到每个因子$F_s(x,X_s)$有一个因子图(因子子图)，因此本身可以被分解。特别地，我们有
\begin{equation}
\label{MMa}
	F_s(x,X_s)=f_s(x,x_1,\dots,x_M)G_1(x_1,X_{s1})\dots,G_M(x_M,X_{sM})
\end{equation}
其中，为了方便，我们将$x$之外的与因子$f_s$相关的变量记作$x_1,\dots,x_M$。因此也可以记作$\boldsymbol{x}_s$。

将公式$\ref{MMa}$代入公式$\ref{juli}$，我们有
\begin{equation}
\label{lin}
	\begin{aligned}
	\mu_{f_{s\to x}}(x)&=\sum_{x_1}\dots\sum_{x_M}f_s(x,x_1,\dots,x_M)\prod_{m\in \mathrm{ne}(f_s)\backslash x}\left[\sum_{X_{sm}}G_m(x_m,X_{sm}) \right]\\
	&=\sum_{x_1}\dots\sum_{x_M}f_s(x,x_1,\dots,x_M)\prod_{m\in \mathrm{ne}(f_s)\backslash x}\mu_{x_{m}\to f_s}(x_m)
	\end{aligned}
\end{equation}
其中$\mathrm{ne}(f_s)$表示因子结点$f_s$的相邻变量结点的集合，$\mathrm{ne}(f_s)\backslash x$表示同样的集合，但是移除了结点$x$。这里，我们定义了下面的从变量结点到因子结点的信息
\begin{equation}
	\mu_{x_{m}\to f_s}(x_m)\equiv \sum_{X_{sm}}G_m(x_m,X_{sm})
\end{equation}
于是，我们引入了两类不同的信息。一类信息是从因子结点到变量结点的信息，记作$\mu_{f\to x}(x)$，另一类信息是从变量结点到因子结点的信息，记作$\mu_{x\to f}(x)$。在任何一种情况下，我们看到沿着一条链接传递的信息总是一个函数，这个函数是与那个链接相连的变量结点相关的变量的函数。

公式$\ref{lin}$给出的结果表明，一个变量结点通过一个链接发送到一个因子结点的信息可以按照如下的方式计算：计算沿着所有进行因子结点的其他链接的输入信息的乘积，乘以与那个结点关联的因子，然后对所有与输入信息关联的变量进行求和。

\textbf{测试2}
\subsection*{一般图的精确推断}
\subsection*{循环置信传播}
\subsection*{学习图结构}