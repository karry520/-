\section{图模型中的推断}
我们现在考虑图模型中的推断问题，图中的一些结点被限制为观测值，我们想要计算其他结点中的一个或多个子集的后验概率分布。正如我们看到的那样，我们可以利用图结构找到高效的推断算法，也可以让这些算法的结构变得透明。具体地说，我们会看到许多算法可以用图中局部信息传播的方式表示。本节中，我们会把注意力主要集中于精确推断的方法。后面的章节中，我们会考虑许多近似推断的算法。

首先，考虑贝叶斯定理的图表示。假设我们将两个变量$x$和$y$上的联合概率分布$p(x,y)$分解为因子的乘积的形式$p(x,y)=p(x)p(y|x)$。这可以用图表示。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\node[state,red] (A) {$x$};
		\node[state,red] (B) [below of=A]{$y$};
		\path (A) edge[->,semithick] (B);
	\end{tikzpicture}
\end{center}
现在假设我们观测到了$y$的值，如图所示
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\node[state,red] (A) {$x$};
		\node[state,red,fill=gray!50] (B) [below of=A] {$y$};
		
		\path (A) edge[->] (B);
	\end{tikzpicture}
\end{center}
我们可以将边缘概率分布$p(x)$看成潜在变量$x$上的先验概率分布，我们的目标是推断$x$上对应的后验概率分布。使用概率的加和规则和乘积规则，我们可以计算 
\begin{equation}
	p(y) =\sum_{x^{'}}p(y|x^{'})p(x^{'})
\end{equation}
这个式子然后被用于贝叶斯定理中，计算 
\begin{equation}
	p(x|y)=\frac{p(y|x)p(x)}{p(y)}
\end{equation}
因此现在联合概率分布可以通过$p(y)$和$p(x|y)$。从图的角度看，联合概率分布$p(x,y)$现在可以表示为下图，其中箭头的方向翻转了。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
	\node[state,red] (A) {$x$};
	\node[state,red,fill=gray!50] (B) [below of=A] {$y$};
	
	\path (B) edge[->] (A);
	\end{tikzpicture}
\end{center}
这是图模型中推断问题的最简单的例子。
\subsection*{链推断}
现在考虑一个更加复杂的问题，涉及到图示中的结点链。这个例子是本节中对更一般的图的精确推断的讨论的基础。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\tikzstyle{every node} = [color=red]
		
		\node[state] (A) {};
		\node[state] (B) [right of=A]{};
		\node (C) [right of=B]{$\dots$};
		\node[state] (D) [right of=C]{};
		\node[state] (E) [right of=D]{};
		
		\path (A) edge (B)
			  (B) edge (C)
			  (C) edge (D)
			  (D) edge (E);
	\end{tikzpicture}
\end{center}
这个图的联合概率分布形式为
\begin{equation}
\label{ruf}
	p(\boldsymbol{x})=\frac{1}{Z}\psi_{1,2}(x_1,x_2)\psi_{2,3}(x_2,x_3)\dots\psi_{N-1,N}(x_{N-1},x_N)
\end{equation}
我们考虑一个具体的情形，即N个结点表示N个离散变量，每个变量都有K个状态。这种情况下的势函数$\psi_{n-1,n}(x_{n-1},x_n)$由一个$K\times K$的表组成，因此联合概率分布有$(N-1)K^2$个参数。

让我们考虑寻找边缘概率分布$p(x_n)$这一推断问题，其中$x_n$是链上的一个具体的结点。注意，现阶段，没有观测结点。根据定义，这个边缘概率分布可以通过对联合概率分布在除$x_n$以外的所有变量上进行求和的方式得到。
\begin{equation}
\label{daif}
	p(x_n)=\sum_{x_1}\dots\sum_{x_{n-1}}\sum_{x_{n+1}}\dots\sum_{x_N}p(\boldsymbol{x})
\end{equation}
在一个朴素的实现中，我们首先计算联合概率分布，然后显式地进行求和。联合概率分布可以表示为一组数，对应于$\boldsymbol{x}$的每个可能的值。因为有N个变量，每个变量有K个可能的状态，因此$\boldsymbol{x}$有$K^N$个可能的值，从而联合概率的计算和存储以及得到$p(x_n)$所需的求和过程，涉及到的存储量和计算量都会随着链的长度N而指数增长。

然而，通过利用图模型的条件独立性质，我们可以得到一个更加高效的算法。如果我们将联合概率分布的分解表达式$\ref{ruf}$代入到公式$\ref{daif}$中，那么我们可以重新整理加和与乘积的顺序，使得需要求解的边缘分布可以更加高效地计算。例如，考虑对$x_N$的求和。势函数$\psi_{N-1,N}(x_{N-1},x_N)$是唯一与$x_N$有关系的势函数，因此我们可以进行下面的求和
\begin{equation}
	\sum_{x_N}\psi_{N-1,N}(x_{N-1},x_N)
\end{equation}
得到一个关于$x_{N-1}$的函数。$x_1$上的求和式只涉及到势函数$\psi_{1,2}(x_1,x_2)$，因此可以单独进行，得到$x_2$的函数，以此类推。因为每个求和式都移除了概率分布中的一个变量，因此这可以被看成从图中移除一个结点。

如果我们使用这种方式对势函数和求和式进行分组，那么我们可以将需要求解的边缘概率密度写成下面的形式
\begin{equation}
\label{www}
	\begin{aligned}
	p(x_n)=&\frac{1}{Z}\\
	&\underbrace{\left[\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1},x_n)\dots\left[\sum_{x_2}\psi_{2,3}(x_2,x_3)\left[\sum_{x_1}\psi_{1,2}(x_1,x_2) \right] \right]\dots \right]}_{\mu_\alpha(x_n)}\\
	&\underbrace{\left[\sum_{x_{n+1}}\psi_{n,n+1}(x_{n},x_{n+1})\dots \left[\sum_{x_N}\psi_{N-1,N}(x_{N-1},x_N) \right]\dots \right]}_{\mu_\beta(x_n)}
	\end{aligned}
\end{equation}
这个重排列的方式，背后的思想组成了后续对于一般的加和-乘积算法的讨论的基础，这里，我们利用的关键概率是乘法对加法的分配率，即
\begin{equation}
	ab+ac=a(b+c)
\end{equation}
使用这种重排序的表达式之后，计算边缘概率分布所需的计算总代价是$O(NK^2)$。这是链长度的一个线性函数，与朴素方法的指数代价不同。

现在使用图中局部信息传递的思想，给出这种计算的一个强大的直观意义。根据公式$\ref{www}$，我们看到边缘概率分布$p(x_n)$的表达式分解成了两个因子的乘积乘以归一化常数
\begin{equation}
	p(x_n)=\frac{1}{Z}\mu_\alpha(x_n)\mu_\beta(x_n)
\end{equation}
我们把$\mu_\alpha(x_n)$看成从结点$x_{n-1}$到结点$x_n$的沿着链向前传递的信息。类似地，$\mu_\beta(x_n)$可以看成从结点$x_{n+1}$到结点$x_n$的沿着链向后传递的信息。

信息$\mu_\alpha(x_n)$可以递归地计算，因为 
\begin{equation}
\label{rang}
	\begin{aligned}
	\mu_\alpha(x_n)&=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1},x_n)\left[\sum_{x_{n-2}}\dots \right]\\
	&=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1},x_n)\mu_\alpha(x_{n-1})
	\end{aligned}
\end{equation}
因此我们首先计算 
\begin{equation}
	\mu_\alpha(x_2)=\sum_{x_1}\psi_{1,2}(x_1,x_2)
\end{equation}
然后重复应用公式$\ref{rang}$直到我们到达需要求解的结点。

类似地，信息$\mu_\beta(x_n)$可以递归的计算。计算方法为：从结点$x_N$开始，使用
\begin{equation}
	\begin{aligned}
	\mu_\beta(x_n)&=\sum_{x_{n+1}}\psi_{n,n+1}(x_n,x_{n+1})\left[\sum_{x_{n+2}}\dots \right]\\
	&=\sum_{x_{n+1}}\psi_{n,n+1}(x_n,x_{n+1})\mu_\beta(x_{n+1})
	\end{aligned}
\end{equation}
这种递归的信息传递如图所示。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\tikzstyle{every node} = [color=red]
		\node[state] (A) {$x_1$};
		\node (B) [right of=A]{$\dots$};
		\node[state] (C) [right of=B]{$x_{n-1}$};
		\node[state] (D) [right of=C]{$x_n$};
		\node[state] (E) [right of=D]{$x_{n+1}$};
		\node (F) [right of=E]{$\dots$};
		\node[state] (G) [right of=F]{$x_N$};
		
		\path (A) edge (B)
			(B) edge (C)
			(C) edge node[yshift=0.5cm,label=above:$\underrightarrow{\mu_\alpha(x_{n-1})}$]{} (D)
			(D) edge node[yshift=0.5cm,label=above:$\underleftarrow{\mu_\beta(x_n)}$]{}(E)
			(E) edge (F)
			(F) edge (G);
	\end{tikzpicture}
\end{center}
上图被称为马尔可夫链。对应的信息传递方程是马尔可夫过程的Chapman-Kolmogorov方程的一个例子。

现在我们想计算结点链中两个相邻结点的联合概率分布$p(x_{n-1},x_n)$。这类似于计算单一结点的边缘概率分布，区别在于现在有两个变量没有被求和出来。需要求解的边缘概率分布可以写成下面的形式
\begin{equation}
	p(x_{n-1},x_n)=\frac{1}{Z}\mu_\alpha(x_{n-1})\psi_{n-1,n}(x_{n-1},x_n)\mu_\beta(x_n)
\end{equation}
因此一旦我们完成了计算边缘概率分布所需的信息传递，我们就可以直接得到每个势函数中的所有变量上的联合概率分布。
\subsection*{树}
我们已经看到，一个由结点链组成的图的精确推断可以在关于结点数量的线性时间内完成，方法是使用通过链中信息传递表示的算法。更一般地，通过局部信息在更大的一类图中的传递，我们可以高效地进行推断。这类图被称为树(tree)。特别地，我们会对之前在结点链的情形中得到的信息传递公式进行简单的推广，得到加和-乘积算法(sum-product algorithm)，它为树结构图的精确推断提供了一个高效的框架。

三个树结构的例子。(a)一个无向树，(b)一个有向树，(c)一个有向多树
\begin{center}
	\begin{tikzpicture}[node distance=1.5cm]
	\tikzstyle{every node} = [color=red]
	
	\node[state] (A1) {};
	\node[state] (B1) [above left of=A1] {};
	\node[state] (C1) [above right of=A1]{};
	\node[state] (D1) [below left of=A1]{};
	\node[state] (E1) [below right of=A1]{};
	
	\path (A1) edge (B1) 
			   edge (C1) 
			   edge (D1) 
			   edge (E1);
	
	\node (N1) [right of=C1]{};
	\node (M1) [right of=N1]{};
	
	\node[state] (A2) [right of=M1]{};
	\node[state] (B2) [below left of=A2]{};
	\node[state] (C2) [below right of=A2]{};
	\node[state] (D2) [below left of=B2]{};
	\node[state,xshift=-0.5cm] (E2) [below right of=B2]{};
	\node[state,xshift=0.5cm] (F2) [below left of=C2]{};
	\node[state] (G2) [below right of=C2]{};
	
	
	\path (A2) edge[->] (B2)
		  	   edge[->] (C2)
		  (B2) edge[->] (D2)
		  	   edge[->] (E2)
		  (C2) edge[->] (F2)
		  	   edge[->] (G2);
	
	\node (N2) [right of=A2]{};
	\node (M2) [right of=N2]{};
	
	\node[state] (A3) [right of=M2]{};
	\node[state] (B3) [below right of=A3]{};
	\node[state] (C3) [above right of=B3]{};
	\node[state] (D3) [below left of=B3]{};
	\node[state] (E3) [below right of=B3]{};
	
	\path (A3) edge[->] (B3)
		  (C3) edge[->] (B3)
		  (B3) edge[->] (D3) edge[->] (E3);
	\end{tikzpicture}
\end{center}
\subsection*{因子图}
在推导加和-乘积算法之前，引入一个新的图结构，被称为因子图(factor graph)，那么算法的形式会变得特别简单并且具有一般性。

有向图和无向图都使得若干个变量的一个全局函数能够表示为这些变量的子集上的因子图的乘积。因子图显式地表示出了这个分解，方法是：在表示变量的结点的基础上，引入额外的结点表示因子本身。因子图也使我们能够更加清晰地了解分解的细节。

让我们将一组变量上的联合概率分布写成因子的乘积形式
\begin{equation}
\label{fasheng}
	p(\boldsymbol{x})=\prod_{s}f_s(\boldsymbol{x}_s)
\end{equation}
其中$\boldsymbol{x}_s$表示变量的一个子集。每个因子$f_s$是对应的变量集合$\boldsymbol{x}_s$的函数。

在因子图中，概率分布中的每个变量都有一个结点(与之前一样，用圆圈表示)，这与有向图和无向图的情形相同。还存在其他的结点(用小正方形表示)，表示联合概率分布中的每个因子$f_s(\boldsymbol{x}_s)$。最后，在每个因子结点和因子所依赖的变量结点之间，存在无向链接。例如，考虑一个表示为因子图形式的概率分布
\begin{equation}
	p(\boldsymbol{x})=f_a(x_1,x_2)f_b(x1,x_2)f_c(x_2,x_3)f_d(x_3) 
\end{equation}
这可表示为下图所示的因子图。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
	
		\node[state,color=red] (A) {$x_1$};
		\node[state,color=red] (B) [right of=A] {$x_2$};
		\node[state,color=red] (C) [right of=B] {$x_3$};
		
		\node[rectangle,color=red,fill=red,minimum size=0.5cm,label=below:$f_a$] (D) at (-1,-2) {};
		\node[rectangle,color=red,fill=red,minimum size=0.5cm,label=below:$f_b$] (E) at (1,-2) {};
		\node[rectangle,color=red,fill=red,minimum size=0.5cm,label=below:$f_c$] (F) at (3,-2) {};
		\node[rectangle,color=red,fill=red,minimum size=0.5cm,label=below:$f_d$] (G) at (5,-2) {};	
		
		\path (A) edge (D) edge (E)
			  (B) edge (D) edge (E) edge (F)
			  (C) edge (F) edge (G); 	
	\end{tikzpicture}
\end{center}
注意有两个因子$f_a(x_1,x_2)$和$f_b(x_1,x_2)$定义在同一个变量集合上。在一个无向图中，两个这样的因子的乘积被简单地合并到同一个团块势函数中。类似地，$f_c(x_2,x_3)$和$f_d(x_3)$可以结合到$x_2$和$x_3$上的一个单一势函数中。然而，因子图显示地写出这些因子，因此能够表达出关于分解本身的更加细节的信息。

如果我们有一个通过无向图表示的概率分布，那么我们可以将其转化为因子图。为了完成这一点，我们构造变量结点，对应于原始无向图，然后构造额外的因子结点，对应于最大团块$\boldsymbol{x}_s$。因子$f_s(\boldsymbol{x}_s)$被设置为与团块势函数相等。注意，对于同一个无向图，可能存在几个不同的因子图。下图说明了这些概念。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\tikzstyle{every node} = [color=red]
		
		\node[state] (A1) {$x_1$};
		\node[state,yshift=-0.5cm] (B1) [below right of=A1]{$x_2$};
		\node[state,yshift=0.5cm] (C1) [above right of=B1]{$x_3$};
		
		\path (A1) edge (B1) edge (C1)
			  (B1) edge (C1);
			
		\node[state] (A2) [right of=C1]{$x_1$};
		\node[state,yshift=-0.5cm] (B2) [below right of=A2]{$x_2$};
		\node[state,yshift=0.5cm] (C2) [above right of=B2]{$x_3$};
		
		\node[rectangle,draw,fill=red,minimum size=0.5cm,above of=B2,yshift=-0.7cm,label=above:$f$](R1){}; 
			
		\path (R1) edge (A2) edge (B2) edge (C2);	
			
		\node[state] (A3) [right of=C2]{$x_1$};
		\node[state,yshift=-0.5cm] (B3) [below right of=A3]{$x_2$};
		\node[state,yshift=0.5cm] (C3) [above right of=B3]{$x_3$};
		
		\node[rectangle,draw,fill=red,minimum size=0.5cm,above of=B3,yshift=-0.7cm,label=above:$f_a$](R2){}; 
		\node[rectangle,draw,fill=red,minimum size=0.5cm,below of=C3,yshift=0.7cm,label=right:$f_b$](R3){}; 
		
		\path (R2) edge (A3) edge (B3) edge (C3)
			  (R3) edge (B3) edge (C3);
	\end{tikzpicture}
\end{center}
(a)一个无向图，有一个单一的团块势函数$\psi(x_1,x_2,x_3)$。(b)一个因子图，因子$f(x_1,x_2,x_3)=\psi(x_1,x_2,x_3)$。(c)一个不同的因子图，表示相同的概率分布，它的因子满足$f_a(x_1,x_2,x_3)f_b(x_2,x_3)=\psi(x_1,x_2,x_3)$。

类似地，为了将有向图转化为因子图，我们构造变量结点对应于有向图中的结点，然后构造因子结点，对应于条件概率分布，最后添加上合适的链接。与之前一样，同一个有向图可能对应于多个因子图。有向图到因子图的转化如图所示。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
	\tikzstyle{every node} = [color=red]
	
	\node[state] (A1) {$x_1$};
	\node[state,yshift=-0.5cm] (B1) [below right of=A1]{$x_2$};
	\node[state,yshift=0.5cm] (C1) [above right of=B1]{$x_3$};
	
	\path (A1) edge[->] (B1) 
		  (C1) edge[->] (B1);
	
	\node[state] (A2) [right of=C1]{$x_1$};
	\node[state,yshift=-0.5cm] (B2) [below right of=A2]{$x_2$};
	\node[state,yshift=0.5cm] (C2) [above right of=B2]{$x_3$};
	
	\node[rectangle,draw,fill=red,minimum size=0.5cm,above of=B2,yshift=-0.7cm,label=above:$f$](R1){}; 
	
	\path (R1) edge (A2) edge (B2) edge (C2);	
	
	\node[state] (A3) [right of=C2]{$x_1$};
	\node[state,yshift=-0.5cm] (B3) [below right of=A3]{$x_2$};
	\node[state,yshift=0.5cm] (C3) [above right of=B3]{$x_3$};
	
	\node[rectangle,draw,fill=red,minimum size=0.5cm,above of=B3,yshift=-0.7cm,label=above:$f_c$](R2){}; 
	\node[rectangle,draw,fill=red,minimum size=0.5cm,below of=C3,label=right:$f_b$](R3){}; 
	\node[rectangle,draw,fill=red,minimum size=0.5cm,below of=A3,label=left:$f_a$](R4){}; 
		
	\path (R2) edge (A3) edge (B3) edge (C3)
		  (R3) edge (C3) 
		  (R4) edge (A3);
	\end{tikzpicture}
\end{center}
(a)一个有向图，可以分解为$p(x_1)p(x_2)p(x_3|x_1,x_2)$。(b)一个因子图，表示与有向图相同的概率分布，它的因子满足$f(x_1,x_2,x_3)=p(x_1)p(x_2)p(x_3|x_1,x_2)$。(c)一个不同的因子图，表示同样的概率分布，因子为$f_a(x_1)=p(x_1),f_b(x_2)=p(x_2),f_c(x_1,x_2,x_3)=p(x_3|x_1,x_2)$。

我们已经看到了树结构图对于进行高维推断的重要性。如果我们将一个有向树或者无向树转化为因子图，那么生成的因子图也是树(即，因子图没有环，且任意两个结点之间有且只有一条路径)。在有向多树的情形中，由于“伦理”的步骤的存在，转化为无向图会引入环，而转化后的因子图仍然是树。事实上，有向图中由于链接父结点和子结点产生的局部环可以转换到因子图时被移除，只需定义合适的因子函数即可。
\subsection*{加和-乘积算法}
我们会使用因子图框架推导一类强大的、高效的精确推断算法，这些算法适用于树结构的图。这里，我们把注意力集中于计算结点或者结点子集上的局部边缘概率分布，这会引出加和-乘积算法(sum-product algorithm)。稍后，我们会修改这个方法，使得概率最大的状态被找到，这就引出了最大加和算法(max-sum algorithm)。

我们假设原始的图是一个无向树或者有向树或者多树，从而对应的因子图有一个树结构。首先，我们将原始的图转化为因子图，使得我们可以使用同样的框架处理有向模型和无向模型。我们的目标是利用图的结构完成两件事：
\begin{enumerate}
	\item 得到一个高效的精确推断算法来寻找边缘概率；
	\item 在需要求解多个边缘概率的情形，计算可以高效地共享。
\end{enumerate}
首先，对于特定的变量结点$x$，我们寻找边缘概率$p(x)$。现阶段，我们假设所有的变量都是隐含变量。稍后我们会看到如何修改这个算法，使得观测变量被整合到算法中。根据定义，边缘概率分布通过对所有$x$之外的变量上的联合概率分布进行求和的方式得到，即
\begin{equation}
\label{jiao}
	p(x)=\sum_{\boldsymbol{x}\backslash x}p(\boldsymbol{x})
\end{equation}
其中$\boldsymbol{x}\backslash x$表示变量$\boldsymbol{x}$的集合去掉变量$x$。算法的思想是使用因子图的表达式$\ref{fasheng}$替换$p(\boldsymbol{x})$，然后交换加和与乘积的顺序，得到一个高效的算法。

考虑下图
\begin{center}
	\begin{tikzpicture}[node distance=1.5cm]
		\tikzstyle{every node} = [color=red]
		
		\node[rectangle,minimum size=0.5cm,fill=red,draw,label=below:$f_s$](A) {};
		
		\node[state,above left of=A](C) {};
		\node[state,below left of=A](D) {};
		
		\path (A) edge (C) edge (D)
			  (C) edge[dashed] (D);
		
		\begin{pgfonlayer}{background}
			\node[fill=gray!25,ellipse,draw,fit=(A)(C)(D)](P){};
		\end{pgfonlayer}
		\node[color=black,left of=A,xshift=-0.2cm]{\rotatebox{90}{$F_s(x,X_s)$}};
		
		\node[state,right of=A](E) {$x$};
		\path (A) edge node[color=black,above,yshift=0.3cm] {$\underrightarrow{\mu_{f_{s\to x}}(x)}$} (E);
		
		\node[rectangle,fill=red,draw,minimum size=0.5cm,above right of=E] (F) {};
		\node[rectangle,fill=red,draw,minimum size=0.5cm,below right of=E] (G) {};
		
		\path (E) edge (F) edge (G)
			  (F) edge[dashed] (G);		
	\end{tikzpicture}
\end{center}
我们看到图的树结构使得我们可以将联合概率分布中的因子划分为若干组，每组对应于变量结点$x$的相邻结点组成的因子结点集合。我们看到联合概率分布可以写成乘积的形式
\begin{equation}
\label{huan}
	p(\boldsymbol{x})=\prod_{s\in \mathrm{ne}(x)}F_s(x,X_s)
\end{equation}
其中$\mathrm{ne}(x)$表示与$x$相邻的因子结点的集合，$X_s$表示子树中通过因子结点$f_s$与变量结点$x$相连的所有变量的集合，$F_s(x,X_s)$表示分组中与因子$f_s$相关联的所有因子的乘积。

将公式$\ref{huan}$代入$\ref{jiao}$，交换加和与乘积的顺序，我们有
\begin{equation}
\label{zui}
	\begin{aligned}
		p(x)&=\prod_{s\in \mathrm{ne}(x)}\left[\sum_{\boldsymbol{X}_s}F_s(x,X_s) \right]\\
		&=\prod_{s\in \mathrm{ne}(x)}\mu_{f_{s\to x}}(x)
	\end{aligned}
\end{equation}
这里我们引入了一组新的函数$\mu_{f_{s\to x}}(x)$，定义为
\begin{equation}
\label{juli}
	\mu_{f_{s\to x}}(x)\equiv \sum_{\boldsymbol{X}_s}F_s(x,X_s)
\end{equation}
这可以被看做从因子结点$f_s$到变量结点$x$的信息(message)。我们看到，需要求解的边缘概率分布$p(x)$等于所有到达结点$x$的输入信息的乘积。

为了计算这些信息，我们再次回到上图。我们注意到每个因子$F_s(x,X_s)$有一个因子图(因子子图)，因此本身可以被分解。特别地，我们有
\begin{equation}
\label{MMa}
	F_s(x,X_s)=f_s(x,x_1,\dots,x_M)G_1(x_1,X_{s1})\dots,G_M(x_M,X_{sM})
\end{equation}
其中，为了方便，我们将$x$之外的与因子$f_s$相关的变量记作$x_1,\dots,x_M$。因此也可以记作$\boldsymbol{x}_s$。

将公式$\ref{MMa}$代入公式$\ref{juli}$，我们有
\begin{equation}
\label{lin}
	\begin{aligned}
	\mu_{f_{s\to x}}(x)&=\sum_{x_1}\dots\sum_{x_M}f_s(x,x_1,\dots,x_M)\prod_{m\in \mathrm{ne}(f_s)\backslash x}\left[\sum_{X_{sm}}G_m(x_m,X_{sm}) \right]\\
	&=\sum_{x_1}\dots\sum_{x_M}f_s(x,x_1,\dots,x_M)\prod_{m\in \mathrm{ne}(f_s)\backslash x}\mu_{x_{m}\to f_s}(x_m)
	\end{aligned}
\end{equation}
其中$\mathrm{ne}(f_s)$表示因子结点$f_s$的相邻变量结点的集合，$\mathrm{ne}(f_s)\backslash x$表示同样的集合，但是移除了结点$x$。这里，我们定义了下面的从变量结点到因子结点的信息
\begin{equation}
\label{de}
	\mu_{x_{m}\to f_s}(x_m)\equiv \sum_{X_{sm}}G_m(x_m,X_{sm})
\end{equation}
于是，我们引入了两类不同的信息。一类信息是从因子结点到变量结点的信息，记作$\mu_{f\to x}(x)$，另一类信息是从变量结点到因子结点的信息，记作$\mu_{x\to f}(x)$。在任何一种情况下，我们看到沿着一条链接传递的信息总是一个函数，这个函数是与那个链接相连的变量结点相关的变量的函数。

公式$\ref{lin}$给出的结果表明，一个变量结点通过一个链接发送到一个因子结点的信息可以按照如下的方式计算：计算沿着所有进行因子结点的其他链接的输入信息的乘积，乘以与那个结点关联的因子，然后对所有与输入信息关联的变量进行求和。如图所示
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\tikzstyle{every node} = [color=red]
		
		\node[rectangle,fill=red,draw,label=left:$f_s$,minimum size=0.5cm] (A){};
		\node[state,above left of=A,xshift=-0.5cm](B) {$x_M$};
		\node[state,below left of=A,xshift=-0.5cm](C) {$x_m$};
		\node[state,right of=A,xshift=0.5cm](D) {$x$};
		\begin{pgfonlayer}{background}
			\node[fill=gray!25,inner sep=0.6cm,ellipse,fit=(C)](E)[label=below:$G_m(x_m\text{,}X_{sm})$]{};
		\end{pgfonlayer}
		
		\path (A) edge (B) edge (C) edge (D)
			  (B) edge[dashed] (C);
	\end{tikzpicture}
\end{center}
值得注意的是，一旦一个因子结点从所有其他的相邻变量结点的输入信息，那么这个因子结点就可以向变量结点发送信息。

最后，我们推导变量结点到因子结点的信息的表达式，再次使用图分解(子图分解)。根据下图
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
	\tikzstyle{every node} = [color=red]
	
	\node[state] (A){$x_m$};
	\node[rectangle,fill=red,draw,label=left:$f_L$,minimum size=0.5cm,above left of=A,xshift=-0.5cm](B) {$x_M$};
	\node[rectangle,fill=red,draw,label=left:$f_t$,minimum size=0.5cm,below left of=A,xshift=-0.5cm](C) {$x_m$};
	\node[rectangle,fill=red,draw,label=left:$f_s$,minimum size=0.5cm,right of=A,xshift=0.5cm](D) {$x$};
	\begin{pgfonlayer}{background}
	\node[fill=gray!25,inner sep=0.6cm,ellipse,fit=(C)](E)[label=below:$F_t(x_m\text{,}X_{sm})$]{};
	\end{pgfonlayer}
	
	\path (A) edge (B) edge (C) edge (D)
	(B) edge[dashed] (C);
	\end{tikzpicture}
\end{center}
我们看到与结点$x_m$关联的项$G_m(x_m,X_{sm})$由项$F_l(x_m,X_{lm})$的乘积组成，每一个这样的项都与连接到结点$x_m$的一个因子结点$f_l$相关联(不包括结点$f_s$)，即
\begin{equation}
\label{ni}
	G_m(x_m,X_{sm})=\prod_{l\in \mathrm{ne}(x_m)\backslash f_s}F_l(x_m,X_{lm})
\end{equation}
其中求乘积的对象是结点$x_{m}$的所有相邻结点，排除结点$f_s$。注意，每个因子$F_l(x_m,X_{lm})$表示原始图的一个子树，这个原始图与公式$\ref{huan}$表示的图的形式完全相同。将公式$\ref{ni}$代入$\ref{de}$，我们可以得到 
\begin{equation}
\label{ai}
	\begin{aligned}
	\mu_{x_m\to f_s}(x_m)&=\prod_{l\in \mathrm{ne}(x_m)\backslash f_s}\left[\sum_{X_{lm}}F_l(x_m,X_{lm}) \right]\\
	&=\sum_{l\in \mathrm{ne}(x_m)\backslash f_s} \mu_{f_l\to x_m}(x_m)
	\end{aligned}
\end{equation}
其中我们使用了因子结点到变量结点的信息传递的表达式$\ref{juli}$。

因此，为了计算从一个变量结点到相邻因子结点沿着链接传递的信息，我们只需简单地在其他所有结点上对输入信息取乘积。注意，任何只有两个相邻结点的变量结点无需参与计算，只需将信息不变地传递过去即可。此外，我们注意到，一旦一个变量结点接收到了来自所有其他相邻因子结点的输入信息，那么这个变量结点就可以给因子结点发送信息。

我们的目标是计算变量结点$x$的边缘概率分布，这个边缘概率分布等于沿着所有到达这个结点的链接的输入信息的乘积。这些信息中的每一条信息都可以使用其他的信息递归地计算。为了开始这个递归计算的过程，我们可以将结点$x$看成树的根结点，然后从叶结点开始计算。根据公式$\ref{ai}$的定义，我们看到如果一个叶结点是一个变量结点，那么沿着与它唯一相连的链接发送的信息为
\begin{equation}
\label{70}
	\mu_{x\to f}(x)=1
\end{equation}
如图所示
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\tikzstyle{every node} = [color=red]
		\node[state] (a) {$x$};
		\node[rectangle,fill=red,minimum size=0.5cm,right of=A,label=below:$f$](B){};
		\path (a) edge node[above,yshift=0.4cm,text =black]{$\underrightarrow{\mu_{x\to f}(x)=1}$} (B);
	\end{tikzpicture}
\end{center}
类似地，如果叶结点是一个因子结点，那么我们根据公式$\ref{lin}$可以看到，发送的信息的形式为
\begin{equation}
\label{71}
	\mu_{f\to x}(x)=f(x)
\end{equation}
如果所示

\begin{center}
\begin{tikzpicture}[node distance=2cm]
	\tikzstyle{every node} = [color=red]
	
	\node[state] (a) {$x$};
	\node[rectangle,fill=red,minimum size=0.5cm,left of=A,label=below:$f$](B){};
	\path (a) edge node[above,yshift=0.4cm,text =black]{$\underrightarrow{\mu_{f\to x}(x)=f(x)}$} (B);
\end{tikzpicture}
\end{center}
现在，我们停下来，总结一下计算边缘概率分布$p(x)$时得到的加和-乘积算法。首先，我们将变量结点$x$看成因子图的根结点，使用公式$\ref{70}$和公式$\ref{71}$，初始化图的叶结点的信息。之后，递归地应用信息传递步骤$\ref{lin}$和$\ref{ai}$，直到信息被沿着每一个链接传递完毕，并且根结点收到了所有相邻结点的信息。每个结点都可以向根结点发送信息。一旦结点收到了所有其他相邻结点的信息，那么它就可以向根结点发送信息。一旦根结点收到了所有相邻结点的信息，需要求解的边缘概率分布就可以使用公式$\ref{zui}$进行计算。

现在考虑一个简单的例子来说明加和-乘积算法。如图所示
\begin{center}
	\begin{tikzpicture}[node distance=1.5cm]
		\tikzstyle{every node} = [color=red]
		
		\node[state] (A) {$x_2$};
		\node[rectangle,text=white,fill=red,minimum size=0.5cm,left of=A](B){$f_a$};
		\node[state,left of=B](C) {$x_1$};
		\node[rectangle,text=white,fill=red,minimum size=0.5cm,below of=A](D){$f_c$};
		\node[state,below of=D](E) {$x_4$};
		\node[rectangle,text=white,fill=red,minimum size=0.5cm,right of=A](F){$f_b$};
		\node[state,right of=F](G) {$x_3$};
		
		\path (A) edge (B) edge (F) edge (D)
			  (B) edge (C)
			  (D) edge (E)
			  (F) edge (G);
	\end{tikzpicture}
\end{center}
图示给出了一个简单的4节点因子图，它的示归一化联合概率分布为
\begin{equation}
	\tilde{p}(\boldsymbol{x})=f_a(x_1,x_2)f_b(x_2,x_3)f_c(x_2,x_4)
\end{equation}
为了对这个图应用加和-乘积算法，让我们令结点$x_3$为根结点，此时有两个叶结点$x_1$和$x_4$。从叶结点开始，我们有下面六个信息组成的序列 
\begin{flalign}
	\mu_{x_1\to f_a}(x_1) &= 1\\
	\mu_{f_a\to x_2}(x_2) &= \sum_{x_1}f_a(x_1,x_2)\\
	\mu_{x_4\to f_c}(x_4) &= 1\\
	\mu_{f_c\to x_2}(x_2) &= \sum_{x_4}f_c(x_2,x_4)\\
	\mu_{x_2\to f_b}(x_2) &= \mu_{f_a\to x_2}(x_2)\mu_{f_c\to x_2}(x_2)\\
	\mu_{f_b\to x_3}(x_3) &= \sum_{x_2}f_b(x_2,x_3)\mu_{x_2\to f_b}(x_2)
\end{flalign}
信息流的方向如图所示
\begin{center}
	\begin{tikzpicture}[node distance=1.2cm]
	\tikzstyle{every node} = [color=red]
	
	\node[state] (A1) {$x_2$};
	\node[rectangle,text=white,fill=red,minimum size=0.5cm,left of=A1](B1){$f_a$};
	\node[state,left of=B1](C1) {$x_1$};
	\node[rectangle,text=white,fill=red,minimum size=0.5cm,below of=A1](D1){$f_c$};
	\node[state,below of=D1](E1) {$x_4$};
	\node[rectangle,text=white,fill=red,minimum size=0.5cm,right of=A1](F1){$f_b$};
	\node[state,right of=F1](G1) {$x_3$};
	
	\path (A1) edge node[above,blue]{$\leftarrow$} (B1) edge node[above,blue]{$\leftarrow$} (F1) edge node[left,blue]{$\downarrow$}(D1)
	(B1) edge node[above,blue]{$\leftarrow$}(C1)
	(D1) edge node[left,blue]{$\downarrow$}(E1)
	(F1) edge node[above,blue]{$\leftarrow$}(G1);

	%%%%%%%
	\node[state,right of=G1,xshift=3cm] (A) {$x_2$};
	\node[rectangle,text=white,fill=red,minimum size=0.5cm,left of=A](B){$f_a$};
	\node[state,left of=B](C) {$x_1$};
	\node[rectangle,text=white,fill=red,minimum size=0.5cm,below of=A](D){$f_c$};
	\node[state,below of=D](E) {$x_4$};
	\node[rectangle,text=white,fill=red,minimum size=0.5cm,right of=A](F){$f_b$};
	\node[state,right of=F](G) {$x_3$};
	
	\path (A) edge node[above,blue]{$\rightarrow$} (B) edge node[above,blue]{$\rightarrow$} (F) edge node[left,blue]{$\uparrow$}(D)
	(B) edge node[above,blue]{$\rightarrow$}(C)
	(D) edge node[left,blue]{$\uparrow$}(E)
	(F) edge node[above,blue]{$\rightarrow$}(G);
		\end{tikzpicture}
\end{center}
(a)从根结点向叶结点传递。(b)从叶结点向根结点传递。

现在一个信息已经在两个方向上通过了每个链接，因此我们现在可以计算边缘概率分布。作为一个简单的检验，让我们验证边缘概率分布$p(x_2)$由正确的表达式给出。使用公式$\ref{zui}$，使用上面的结果将信息替换掉，我们有
\begin{equation}
	\begin{aligned}
		\tilde{p}(x_2)&=\mu_{f_a\to x_2}(x_2)\mu_{f_b\to x_2}(x_2)\mu_{f_c\to x_2}(x_2)\\
		&=\left[\sum_{x_1}f_a(x_1,x_2) \right]\left[\sum_{x_3}f_b(x_2,x_3) \right]\left[\sum_{x_4}f_c(x_2,x_4) \right]\\
		&=\sum_{x_1}\sum_{x_3}\sum_{x_4}\tilde{p}(\boldsymbol{x})
	\end{aligned}
\end{equation}
这与我们预期的结果相同。

目前为止，我们已经假定图中所有的变量都是隐含变量。在大多数实际应用中，变量的一个子集会被观测到，我们希望计算以这些观测为条件的后验概率分布。观测结点在加和-乘积算法中很容易处理，如下所述。假设我们将$\boldsymbol{x}$划分为隐含变量$\boldsymbol{h}$和观测变量$\boldsymbol{v}$，且$\boldsymbol{v}$的观测值被记作$\hat{\boldsymbol{v}}$。然后，我们简单地将联合概率分布$p(\boldsymbol{x})$乘以$\prod_iI(v_i,\hat{v}_i)$，其中如果$v=\hat{v}$，则$I(v_i,\hat{v}_i)=1$，否则$I(v,\hat{v})=0$。这个乘积对应于$p(\boldsymbol{h},\boldsymbol{v}=\hat{\boldsymbol{v}})$，因此$p(\boldsymbol{h}|\boldsymbol{v}=\hat{\boldsymbol{v}})$的一个未归一化版本。通过运行加和-乘积算法，我们可以高效地计算后验边缘概率$p(h_i|\boldsymbol{v}=\hat{\boldsymbol{v}})$，忽略归一化系数。归一化系数的值可以使用一个局部的计算高效地计算出来。$\boldsymbol{v}$中变量上的任意求和式就退化成了单一的项。
\subsection*{最大加和算法}
加和-乘积算法使得我们能够将联合概率分布$p(\boldsymbol{x})$表示为一个因子图，并且高效地求出成分变量上的边缘概率分布。有两个其他的比较常见的任务，即找到变量的具有最大概率的一个设置，以及找到这个概率的值。这两个任务可以通过一个密切相关的算法完成，这个算法被称为最大加和(max-sum)，可以被看成动态规划(dynamic programming)在图模型中的一个应用。

一个简单的寻找具有最大概率的潜在变量值的方法是，运行加和-乘积算法，得到每个变量的边缘概率分布$p(x_i)$，然后，反过来对于每个边缘概率分布，找到使边缘概率最大的$x_i^*$。然而，这会给出一组值，每个值都单独取得最大的概率。在实际应用中，我们通常希望找到联合起来具有最大概率的值的集合，换句话说，找到向量$\boldsymbol{x}^{\text{最大}}$，使得联合概率分布达到最大值，即
\begin{equation}
	\boldsymbol{x}^{\text{最大}}=\mathrm{arg}\mathop{\mathrm{max}}\limits_{\boldsymbol{x}}p(\boldsymbol{x})
\end{equation}
这样，联合概率分布的对应值为
\begin{equation}
p(\boldsymbol{x}^{\text{最大}})=\mathop{\mathrm{max}}\limits_{\boldsymbol{x}}p(\boldsymbol{x})
\end{equation}
通常，$\boldsymbol{x}^{\text{最大}}$与$x_i^*$的集合不同。于是，我们寻找一个高效的算法，来求出最大化联合概率分布$p(\boldsymbol{x})$的$\boldsymbol{x}$的值，这会使得我们得到在最大值处的联合概率分布的值。为了解决第二个问题，我们只需简单地写出分量的最大值算符，即
\begin{equation}
\label{zhengzha}
	\mathop{\mathrm{max}}\limits_{\boldsymbol{x}}p(\boldsymbol{x})=\mathop{\mathrm{max}}\limits_{x_1}\dots\mathop{\mathrm{max}}\limits_{x_M}p(\boldsymbol{x})
\end{equation}
其中M是变量的总数。之后，使用$p(\boldsymbol{x})$的用因子乘积形式表示的展开式替换$p(\boldsymbol{x})$即可。

在推导加和-乘积臭汗时，我们使用了乘法的分配律。这里，我们使用最大化算符的类似定律
\begin{equation}
	\mathrm{max}(ab,ac)=a\mathrm{max}(b,c)
\end{equation}
这对于$a\geqslant 0$的情形成立(这对于图模型的因子总成立)。这使得我们交换乘积与最大化的顺序。

首先考虑公式$\ref{ruf}$描述的结点链这一简单的例子。概率最大值的计算可以写成
\begin{equation}
	\begin{aligned}
	\mathop{\mathrm{max}}\limits_{\boldsymbol{x}}p(\boldsymbol{x})&=\frac{1}{Z}\mathop{\mathrm{max}}\limits_{x_1}\dots\mathop{\mathrm{max}}\limits_{x_N}[\psi_{1,2}(x_1,x_2)\dots\psi_{N-1,N}(x_{N-1},x_N)]\\
	&=\frac{1}{Z}\mathop{\mathrm{max}}\limits_{x_1}\left[\mathop{\mathrm{max}}\limits_{x_2}\left[\psi_{1,2}(x_1,x_2)\left[\dots\mathop{\mathrm{max}}\limits_{x_N}\psi_{N-1,N}(x_{N-1},x_N) \right]\dots \right] \right]
	\end{aligned}
\end{equation}
正如边缘概率的计算一样，我们看到交换最大值算符和乘积算法会产生一个更高效的计算，并且更容易表示为从结点$x_N$沿着结点链传递回结点$x_1$的信息。

我们可以将这个结点推广到任意树结构的因子图上，推广的方法为：将因子图表达式$\ref{fasheng}$代入公式$\ref{zhengzha}$中，然后交换乘积与最大化的计算顺序。这种计算的结构与加和-乘积算法完全相同，因此我们能够简单地将那些结果转化到当前的问题中。特别地，假设我们令图中的一个特定的变量结点为根结点。之后，我们计算起始的一组信息，然后从树的叶结点向内部传递到根结点。对于每个结点，一旦它接收到来自其他相邻结点的输入信息，那么它就向根结点发送信息。最后对所有到达根结点的信息的乘积进行最大化，得出$p(\boldsymbol{x})$的最大值。这可以被称为最大化乘积算法(max-produce algorithm)，与加和-乘积算法完全相同，唯一的区别是求和被替换为了求最大值。注意，现阶段，信息被从叶结点发送到根结点，而没有相反的方向。

在实际应用中，许多小概率的乘积可以产生数值下溢的问题，因此更方便的做法是对联合概率分布的对数进行操作。对数函数是一个单调函数，因此求最大值的运算符可以与取对数的运算交换顺序，即
\begin{equation}
	\ln \left(\mathop{\mathrm{max}}\limits_{\boldsymbol{x}}p(\boldsymbol{x}) \right)=\mathop{\mathrm{max}}\limits_{\boldsymbol{x}}\ln p(\boldsymbol{x})
\end{equation}
分配性质仍然成立，因为
\begin{equation}
	\mathrm{max}(a+b,a+c)=a+\mathrm{max}(b,c)
\end{equation}
所以取对数的唯一效果是把最大化乘积算法中的乘积替换成了加和，因此我们得到了最大化加和算法(max-sum algorithm)。根据之前在加和-乘积算法中得到的公式给出的结果，我们可以基于信息传递写出最大化加和算法，只需把“加和”替换为“最大化”，把“乘积”替换为对数求和即可。结果为
\begin{flalign}
	\mu_{f_{s\to x}}(x)&=\mathop{\mathrm{max}}\limits_{x_1,\dots,x_M}\left[\ln f(x,x_1,\dots,x_M)+\sum_{m\in \mathrm{ne}(f)\backslash x} \mu_{x_m\to f}(x_m) \right]\\
	\mu_{x\to f}(x)&=\sum_{l\in \mathrm{ne}(x)\backslash f}\mu_{f_l\to x}(x)
\end{flalign}
最开始的由叶结点发送的信息可以通过类比公式得到，结果为
\begin{flalign}
	\mu_{f\to x}(x)&=\ln f(x)\\
	\mu_{x\to f}(x)&=0
\end{flalign}
根结点处的最大概率为
\begin{equation}
\label{zuida}
	p^{\text{最大}}=\mathop{\mathrm{max}}\limits_{\boldsymbol{x}}\left[\sum_{s\in \mathrm{ne}(x)}\mu_{f_s\to x}(x) \right]
\end{equation}
目前为止，我们已经看到了如何通过从叶结点到任意选择的根结点传递信息的方式找到联合概率分布的最大值。这个结果与根结点的选择无关。现在，我们转向第二个问题，即寻找联合概率达到最大值的变量的配置。目前，我们已经将信息从叶结点发送到了根结点。计算公式$\ref{zuida}$的过程也会得到根结点变量的概率最高的值$x^{\text{最大}}$，定义为
\begin{equation}
	x^{\text{最大}}=\mathop{\mathrm{arg\ max}}\limits_{x}\left[\sum_{s\in \mathrm{ne}(x)}\mu_{f_s\to x}(x) \right]
\end{equation}
使用动态规划的方法应用在图模型上就可以给出变量的精确最大化配置。这种方法的一个重要应用是寻找隐马尔可夫模型中隐含状态的最可能序列，这种情况下被称为Viterbi算法。
\subsection*{一般图的精确推断}
加和-乘积算法和最大化加和算法提供了树结构图中的推断问题的高效精确解法。然而，对于许多实际应用，我们必须处理带有环的图。

信息传递框架可以被推广到任意的图拓扑结构，从而得到一个精确的推断步骤，被称为联合树算法(junction tree algorithm)。联合树对于任意的图都是精确的、高效的。对于一个给定的图，通常不存在计算代价更低的算法。不幸的是，算法必须的计算代价由最大团块中的变量数量确定。在离散变量的情形中，计算代价会随着这个数量指数增长。
\subsection*{循环置信传播}
对于许多实际应用问题来说，使用精确推断是不可行的，因此我们需要研究有效的近似方法。这种近似方法中，一个重要的类别被称为变分法(variational)。作为这些确定性方法的补充，有一大类取样(sampling)方法，也被称为蒙特卡罗(Monte Carlo)方法。这些方法基于从概率分布中的随机数值取样，这两种方法将在后续章节详细讨论。

这里，我们考虑带有环的图中的近似推断的一个简单方法，它直接依赖于之前对树的精确推断的讨论。主要思想就是简单地应用加和-乘积算法，即使不保证能够产生好找结果。这种方法被称为循环置信传播(loopy belief propagation)。
\subsection*{学习图结构}
在我们关于图模型的推断的讨论中，我们假设图的结构已知且固定。然而，也有一些研究超出了推断问题的范围，关注于从数据推断图结构本身。这需要我们定义一个可能结构的空间，以及用于对每个结构评分的度量。