\section{高斯的变分混合}
我们现在回到我们对于高斯混合模型的讨论，并且使用前一节讨论的变分推断的方法。这会很好地说明变分方法的应用，也会展示出贝叶斯方法是如何优雅地解决最大似然方法中的许多困难之处的。这个例子给出了变分方法在实际应用中的许多重要的思想。许多贝叶斯模型，对应于复杂得多的概率分布，可以通过对本节中的分析进行简单的扩展进行求解。

我们的起始点是高斯混合模型的似然函数。高斯混合模型如图所示
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\tikzstyle{every node} = [color=red]
		
		\node[state] (A) {$Z_n$};
		\node[state,below of=A,fill=gray!25] (B) {$X_n$};
		
		\node[circle,inner sep=0.1cm,fill=red,left of=A,label=left:$\pi$](C){}; 
		\node[circle,inner sep=0.1cm,fill=red,left of=B,label=left:$\mu$](D){}; 
		\node[circle,inner sep=0.1cm,fill=red,right of=B,label=right:$\Sigma$](E){}; 
		\node[right of=B,xshift=-1.4cm,yshift=-0.6cm,blue](G) {$N$};
		\node[draw,color=blue,inner sep=0.3cm,rectangle,fit=(A)(B),rounded corners](F){};
		
		\path (A) edge[->] (B)
			  (C) edge[->] (A)
			  (D) edge[->] (B)
			  (E) edge[->] (B);
	\end{tikzpicture}
\end{center}
对于每个观测$\boldsymbol{x}_n$，我们有一个对应的潜在变量$\boldsymbol{z}_n$，它是一个“1-of-K”的二值向量，元素为$z_{nk}$，其中$k=1,\dots,K$。与之前一样，我们将观测数据集记作$\boldsymbol{X}=\{\boldsymbol{x}_1,\dots,\boldsymbol{x}_N\}$，类似地，我们瘵潜在变量记作$\boldsymbol{Z}=\{\boldsymbol{z}_1,\dots,\boldsymbol{z}_N \}$。给定混合系数$\boldsymbol{\pi}$，我们可以写出$\boldsymbol{Z}$的条件概率分布，形式为
\begin{equation}
\label{wei}
	p(\boldsymbol{Z}|\boldsymbol{\pi})=\prod_{n=1}^{N}\prod_{k=1}^{K}\pi_k^{z_{nk}}
\end{equation}
类似地，给定潜在变量和分量参数，我们可以写出观测数据向量的条件概率分布，形式为 
\begin{equation}
\label{jue}
	p(\boldsymbol{X}|\boldsymbol{Z},\boldsymbol{\mu},\boldsymbol{\Lambda})=\prod_{n=1}^{N}\prod_{k=1}^{K}\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k^{-1})^{z_{nk}}
\end{equation}
其中$\boldsymbol{\mu}=\{\boldsymbol{\mu}_k \}$且$\boldsymbol{\Lambda}=\{\boldsymbol{\Lambda}_k \}$。注意，我们计算时使用的是精度矩阵而不是协方差矩阵，因为这在一定程度上简化了数学计算的复杂度。

接下来，我们引入参数$\boldsymbol{\mu},\boldsymbol{\Lambda}$和$\boldsymbol{\pi}$上的先验概率分布。如果我们使用共轭先验分布，那么分析过程会得到极大的简化。于是，我们选择混合系数$\boldsymbol{\pi}$上的狄利克雷分布。
\begin{equation}
	p(\boldsymbol{\pi})=\mathrm{Dir}(\boldsymbol{\pi}|\boldsymbol{\alpha}_0)=C(\boldsymbol{\alpha}_0)\prod_{k=1}^{K}\pi_k^{\alpha_0-1}
\end{equation}
其中，根据对称性，我们为每个分量选择了同样的参数$\alpha_0$，$C(\boldsymbol{\alpha}_0)$是狄利克雷分布的归一化常数。参数$\alpha_0$可以看成与混合分布的每个分量关联的观测的有效先验数量。如果$\alpha_0$的值很小，那么后验概率分布会主要被数据集影响，而受到先验概率的影响很小。

类似地，我们引入一个独立的高斯-Wishart先验分布，控制每个高斯分布的均值和精度，形式为
\begin{equation}
	\begin{aligned}
		p(\boldsymbol{\mu},\boldsymbol{\Lambda})&=p(\boldsymbol{\mu}|\boldsymbol{\Lambda})p(\boldsymbol{\Lambda})\\
		&=\prod_{k=1}^{K}\mathcal{N}(\boldsymbol{\mu}_k|\boldsymbol{m}_0,(\beta_0\boldsymbol{\Lambda}_k)^{-1})\mathcal{W}(\boldsymbol{\Lambda}_k|\boldsymbol{W}_0,v_0)
	\end{aligned}
\end{equation}
这是由于当均值和精度均未知的时候，它表示共轭先验分布。通常根据对称性，我们选择$\boldsymbol{m}_0=0$。

生成的模型可以表示为下图
\begin{center}
	\begin{tikzpicture}[node distance=2cm,->]
	\tikzstyle{every node} = [color=red]
	
	\node[state] (A) {$Z_n$};
	\node[state,below of=A,fill=gray!25] (B) {$X_n$};
	
	\node[state,left of=A](C){$\pi$};
	\node[state,right of=A](D){$\boldsymbol{\Lambda}$};
	\node[state,right of=B](E){$\boldsymbol{\mu}$};
	
	\node[right of=B,xshift=-1.4cm,yshift=-0.6cm,blue](G) {$N$};
	\node[draw,color=blue,inner sep=0.3cm,rectangle,fit=(A)(B),rounded corners](F){};

	\path (C) edge (A)
		  (A) edge (B)
		  (D) edge (B) edge (E)
		  (E) edge (B);
	\end{tikzpicture}
\end{center}
注意，从$\boldsymbol{\Lambda}$到$\boldsymbol{\mu}$之间存在一个链接，这是由于$\boldsymbol{\mu}$上的概率分布的方差为$\boldsymbol{\boldsymbol{\Lambda}}$的函数。

这个例子很好地说明了潜在变量和参数之间的区别。像$\boldsymbol{z}_n$这样出现在方框内部的变量被看做隐含变量，因为这种变量的数量随着数据集规模的增大而增大。相反，像$\boldsymbol{\mu}$这样出现在方框外的变量的数量与数据集的规模无关，因此被当做参数。然而，从图模型的观点来看，它们之间没有本质的区别。
\subsection*{变分分布}
为了形式化地描述这个模型的变分方法，我们接下来写出所有随机变量的联合概率分布，形式为
\begin{equation}
\label{gan}
	p(\boldsymbol{X},\boldsymbol{Z},\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda})=p(\boldsymbol{X}|\boldsymbol{Z},\boldsymbol{\mu},\boldsymbol{\Lambda})p(\boldsymbol{Z}|\boldsymbol{\pi})p(\boldsymbol{\pi})p(\boldsymbol{\mu}|\boldsymbol{\Lambda})p(\boldsymbol{\Lambda})
\end{equation}
不难验证这种分解方式对应于上图给出的概率图模型。注意，只有变量$\boldsymbol{X}=\{\boldsymbol{x}_1,\dots,\boldsymbol{x}_N\}$是观测变量。

我们现在考虑一个变分分布，它可以在潜在变量与参数之间进行分解，即
\begin{equation}
\label{anwei}
	q(\boldsymbol{Z},\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda})=q(\boldsymbol{Z})q(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda})
\end{equation}
需要注意的是，为了让我们的贝叶斯混合模型能够有一个合理的可以计算的解，这是我们需要做出的唯一假设。特别地，因子$q(\boldsymbol{Z})$和$q(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda})$的函数形式会在变分分布的最优化过程中自动确定。注意，我们省略了$q$分布的下标，我们依赖参数来区分不同的分布。

通过使用一般的结果，这些因子的对应的顺序更新方程可以很容易地推导出来。让我们考虑因子$q(\boldsymbol{Z})$的更新方程的推导。最优因子的对数为
\begin{equation}
	\ln q^*(\boldsymbol{Z})=\mathbb{E}_{\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda}}[\ln p(\boldsymbol{X},\boldsymbol{Z},\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda})]+\text{常数}
\end{equation}
我们现在使用公式$\ref{anwei}$给出的分解方式。注意，我们只对等式右侧与变量$\boldsymbol{Z}$相关的函数关系感兴趣。因此，任何与变量$\boldsymbol{Z}$无关的项都可以被整合到可加的归一化系数中，从而有
\begin{equation}
	\ln q^*(\boldsymbol{Z})=\mathbb{E}_{\boldsymbol{\pi}}[\ln p(\boldsymbol{Z}|\boldsymbol{\pi})]+\mathbb{E}_{\boldsymbol{\mu},\boldsymbol{\Lambda}}[\ln p(\boldsymbol{X}|\boldsymbol{Z},\boldsymbol{\mu},\boldsymbol{\Lambda})]+\text{常数}
\end{equation}
替换右侧的两个条件分布，然后再次把与$\boldsymbol{Z}$无关的项整合到可加性常数中
\begin{equation}
\label{wencun}
	\begin{aligned}
		&\mathbb{E}_{\boldsymbol{\pi}}[\ln p(\boldsymbol{Z}|\boldsymbol{\pi})]+\mathbb{E}_{\boldsymbol{\mu},\boldsymbol{\Lambda}}[\ln p(\boldsymbol{X}|\boldsymbol{Z},\boldsymbol{\mu},\boldsymbol{\Lambda})]\\
		=&\mathbb{E}_{\boldsymbol{\pi}}\left[\ln \left(\prod_{n=1}^{N}\prod_{k=1}^{K}\pi_k^{z_{nk}}\right)\right]+\mathbb{E}_{\boldsymbol{\mu},\boldsymbol{\Lambda}}\left[\ln \left(\prod_{n=1}^{N}\prod_{k=1}^{K}\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k^{-1})^{z_{nk}}\right)\right]\\
		=&\sum_{n=1}^{N}\sum_{k=1}^{K}z_{nk}\left\{\underbrace{\mathbb{E}[\ln \pi_k]+\frac{1}{2}\mathbb{E}[\ln|\boldsymbol{\Lambda}_k|]-\frac{D}{2}\ln(2\pi)-\frac{1}{2}\mathbb{E}_{\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k}[(\boldsymbol{x}_n-\boldsymbol{\mu}_k)^T\boldsymbol{\Lambda}_k(\boldsymbol{x}_n-\boldsymbol{\mu}_k)]}_{\text{记作}\ln \rho_{nk}}  \right\}\\
		 &+\text{常数}
	\end{aligned}
\end{equation}
其中D是数据变量$\boldsymbol{x}$的维度。所以，我们有
\begin{equation}
	\ln q^*(\boldsymbol{Z})=\sum_{n=1}^{N}\sum_{k=1}^{K}z_{nk}\ln \rho_{nk}+\text{常数}
\end{equation}
公式两侧取指数，我们有
\begin{equation}
\label{qz}
	q^*(\boldsymbol{Z})\propto \prod_{n=1}^{N}\prod_{k=1}^{K}\rho_{nk}^{z_{nk}}
\end{equation}
我们要求这个概率分布是归一化的，并且我们注意到对于每个n值，$z_{nk}$都是二值的，在所有的k值上的加和等于1,因此我们有
\begin{equation}
	q^*(\boldsymbol{Z})= \prod_{n=1}^{N}\prod_{k=1}^{K}r_{nk}^{z_{nk}}
\end{equation}
其中
\begin{equation}
\label{hunue}
	r_{nk}=\frac{\rho_{nk}}{\sum_{j=1}^{K}\rho_{nk}}
\end{equation}
从中我们看到$r_{nk}$扮演着“责任”的角色。注意，$q^*(\boldsymbol{Z})$的最优解依赖于关于其他变量计算得到的矩，因此与之前一样，变分更新方程是偶合的，必须用迭代的方式求解。

现在，我们会发现定义观测数据关于“责任”的下面三个统计量会比较方便，即
\begin{flalign}
	N_k&=\sum_{n=1}^{N}r_{nk}\\
	\bar{\boldsymbol{x}}_k&=\frac{1}{N_k}\sum_{n=1}^{N}r_{nk}\boldsymbol{x}_n\\
	\boldsymbol{S}_k&=\frac{1}{N_k}\sum_{n=1}^{N}r_{nk}(\boldsymbol{x}_n-\bar{\boldsymbol{x}}_k)(\boldsymbol{x}_n-\bar{\boldsymbol{x}}_k)^T
\end{flalign}
注意，这些类似于高斯混合模型的最大似然EM算法中计算的量。

现在让我们考虑变分后验概率分布中的因子$q(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda})$。与之前一样，使用公式给出的一般结果，我们有
\begin{equation}
\label{xiaoxin}
	\begin{aligned}
		\ln q^*(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda})&=\mathbb{E}_{\boldsymbol{Z}}[\ln p(\boldsymbol{X},\boldsymbol{Z},\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda})]+\text{常数}\\
		&=\mathbb{E}_{\boldsymbol{Z}}[\ln p(\boldsymbol{X}|\boldsymbol{Z},\boldsymbol{\mu},\boldsymbol{\Lambda})+\ln p(\boldsymbol{Z}|\boldsymbol{\pi})+\ln p(\boldsymbol{\pi})+\ln p(\boldsymbol{\mu},\boldsymbol{\Lambda})]+\text{常数}\\
		&=\mathbb{E}_{\boldsymbol{Z}}[\ln p(\boldsymbol{X}|\boldsymbol{Z},\boldsymbol{\mu},\boldsymbol{\Lambda})]+\mathbb{E}_{\boldsymbol{Z}}[\ln p(\boldsymbol{Z}|\boldsymbol{\pi})]+\mathbb{E}_{\boldsymbol{Z}}[\ln p(\boldsymbol{\pi})]+\mathbb{E}_{\boldsymbol{Z}}[\ln p(\boldsymbol{\mu},\boldsymbol{\Lambda})]+\text{常数}\\
		&=\mathbb{E}_{\boldsymbol{Z}}\left[\ln\left( \prod_{n=1}^{N}\prod_{k=1}^{K}\mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k^{-1})^{z_{nk}}\right)\right]\\
		&\ \ +\mathbb{E}_{\boldsymbol{Z}}\left[\ln\left( p(\boldsymbol{Z}|\boldsymbol{\pi})\right)\right]\\
		&\ \ +\mathbb{E}_{\boldsymbol{Z}}\left[\ln\left( p(\boldsymbol{\pi})\right)\right]\\
		&\ \ +\mathbb{E}_{\boldsymbol{Z}}\left[\ln\left( \prod_{k=1}^{K}p(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)\right)\right]+\text{常数}\\	
		&=\ln p(\boldsymbol{\pi})+\sum_{k=1}^{K}\ln p(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)+\mathbb{E}_{\boldsymbol{Z}}\left[\ln\left( p(\boldsymbol{Z}|\boldsymbol{\pi})\right)\right]\\
		&\ \ +\sum_{k=1}^{K}\sum_{n=1}^{N}\mathbb{E}[z_{nk}]\ln \mathcal{N}(\boldsymbol{x}_n|\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k^{-1})+\text{常数}
	\end{aligned}
\end{equation}
我们观察到，这个表达式的右侧分解成了若干项的和，一些项只与$\boldsymbol{\pi}$相关，一些项只与$\boldsymbol{\mu}$和$\boldsymbol{\Lambda}$相关，这表明变分后验概率$q(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda})$可以分解为$q(\boldsymbol{\pi})q(\boldsymbol{\mu},\boldsymbol{\Lambda})$。此外，与$\boldsymbol{\mu}$和$\boldsymbol{\Lambda}$的项本身由k个与$\boldsymbol{\mu}_k$和$\boldsymbol{\Lambda}_k$相关的项有关，因此可以进一步分解，即
\begin{equation}
\label{bea}
	q(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda})=q(\boldsymbol{\pi})\prod_{k=1}^{K}q(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)
\end{equation}
分离出公式$\ref{xiaoxin}$右侧的与$\boldsymbol{\pi}$相关的项，我们有
\begin{equation}
\begin{aligned}
\label{1057}
	\ln q^*(\boldsymbol{\pi})&=\mathbb{E}_{\boldsymbol{Z}}\left[\ln\left( p(\boldsymbol{Z}|\boldsymbol{\pi})\right)\right]+\ln p(\boldsymbol{\pi})\\
	&=\ln \left(C(\boldsymbol{\alpha}_0)\prod_{k=1}^{K}\pi_k^{\alpha_0-1}\right)+\mathbb{E}_{\boldsymbol{Z}}\left[\ln\left( \prod_{n=1}^{N}\prod_{k=1}^{K}\pi_k^{z_{nk}}\right)\right]\\
	&=(\alpha_0-1)\sum_{k=1}^{K}\ln \pi_k +\sum_{k=1}^{K}\sum_{n=1}^{N}r_{nk}\ln \pi_k+\text{常数}
\end{aligned}
\end{equation}
最后，变分后验概率分布$q^*(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)$无法分解成边缘概率分布的乘积，但是我们总可以使用概率的乘积规则，将其写成$q^*(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)=q^*(\boldsymbol{\mu}_k|\boldsymbol{\Lambda}_k)q^*(\boldsymbol{\Lambda}_k)$。结果是一个高斯-Wishart分布，形式为
\begin{equation}
\label{1059}
	q^*(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)=\mathcal{N}(\boldsymbol{\mu}_k|\boldsymbol{m}_k,(\beta_k\boldsymbol{\Lambda}_k)^{-1})\mathcal{W}(\boldsymbol{\Lambda}_k|\boldsymbol{W}_k,v_k)
\end{equation}
其中我们已经定义了
\begin{flalign}
	\beta_k&=\beta_0+N_k\\
	\boldsymbol{m}_k&=\frac{1}{\beta_k}(\beta_0\boldsymbol{m}_0+N_k\bar{\boldsymbol{x}}_k)\\
	\boldsymbol{W}^{-1}_k&=\boldsymbol{W}_0^{-1}+N_k\boldsymbol{S}_k+\frac{\beta_0N_k}{\beta_0+N_k}(\bar{\boldsymbol{x}}_k-\boldsymbol{m}_0)(\bar{\boldsymbol{x}}_k-\boldsymbol{m}_0)^T\\
	v_k&=v_0+N_k
\end{flalign}
更新方程类似于混合高斯模型的最大似然解的EM算法的M步骤的方程。我们看到，为了更新模型参数上的变分后验概率分布，必须进行的计算涉及到的在数据集上的求和操作与最大似然方法中的求和操作相同。

为了进行这个变分M步骤，我们需要得到表示“责任”的期望$\mathbb{E}[z_{nk}]=r_{nk}$。这些可以通过对公式$\ref{wencun}$中定义的$\rho_{nk}$进行归一化的方式得到。我们看到这个表达式涉及到关于变分分布的参数求期望，这些期望很容易求出，从而可得
\begin{equation}
	\rho_{nk}=\mathbb{E}[\ln \pi_k]+\frac{1}{2}\mathbb{E}[\ln|\boldsymbol{\Lambda}_k|]-\frac{D}{2}\ln(2\pi)-\frac{1}{2}\mathbb{E}_{\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k}[(\boldsymbol{x}_n-\boldsymbol{\mu}_k)^T\boldsymbol{\Lambda}_k(\boldsymbol{x}_n-\boldsymbol{\mu}_k)]
\end{equation}
其中
\begin{flalign}
\label{mei}
	\mathbb{E}_{\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k}[(\boldsymbol{x}_n-\boldsymbol{\mu}_k)^T\boldsymbol{\Lambda}_k(\boldsymbol{x}_n-\boldsymbol{\mu}_k)]&=D\beta_k^{-1}+v_k(\boldsymbol{x}_n-\boldsymbol{m}_k)^T\boldsymbol{\Lambda}_k(\boldsymbol{x}_n-\boldsymbol{m}_k)\\
	\label{xue}
	\ln \tilde{\Lambda}_k&=\sum_{i=1}^{D}\psi\left(\frac{v_k+1-i}{2} \right)+D\ln 2+\ln |\boldsymbol{W}_k|\\
	\label{hui}
	\ln \tilde{\pi}_k&=\mathbb{E}[\ln \pi_k]=\psi(\alpha_k)-\psi(\hat{\alpha})
\end{flalign}
其中我们引入了$\tilde{\Lambda}_k$和$\tilde{\pi}_k$的定义，$\psi(\cdot)$是Digamma函数，$\hat{\alpha}=\sum_k\alpha_k$。公式$\ref{xue}$和公式$\ref{hui}$是从Wishart分布和狄利克雷分布的标准性质中得到的。

如果我们将公式$\ref{mei}$、$\ref{xue}$和$\ref{hui}$代入公式$\ref{wencun}$，然后使用公式$\ref{hunue}$，我们得到了下面的“责任”的结果
\begin{equation}
	r_{nk}\propto \tilde{\pi}_k\tilde{\Lambda}_k^{\frac{1}{2}}\exp \left\{-\frac{D}{2\beta_k}-\frac{v_k}{2}(\boldsymbol{x}_n-\boldsymbol{m}_k)^T\boldsymbol{W}_k(\boldsymbol{x}_n-\boldsymbol{m}_k) \right\}
\end{equation}
注意这个结果与最大似然EM算法得到的“责任”的对应结果的相似性。

因此变分后验概率分布的最优化涉及到在两个阶段之间进行循环，这两个阶段类似于最大似然EM算法的E步骤和M步骤。在变分推断的与E步骤等价的步骤中，我们使用当前状态下模型参数上的概率分布来计算公式$\ref{mei}$、$\ref{xue}$和$\ref{hui}$中的各阶矩，从而计算$\mathbb{E}[z_{nk}]=r_{nk}$。然后，在接下来的与M步骤等价的步骤中，我们令这些“责任”保持不变，然后使用它们通过公式$\ref{1057}$和$\ref{1059}$重新计算参数上的变分分布。在任何一种情形下，我们看到变分后验概率的形式与联合概率分布$\ref{gan}$中对应因子的函数形式相同。这是一个一般的结果，是由于选择了共轭先验所造成的。

正如我们已经看到的那样，高斯分布的贝叶斯混合的变分解与最大似然的EM算法的解很相似。事实上，如果我们考虑$N\to \infty$的极限情况，那么贝叶斯方法就收敛于最大似然方法的EM解。对于不是特别小的数据集来说，高斯混合模型的变分算法的主要的代价来自于“责任”的计算，以及加权数据协方差矩阵的计算与求逆。这些计算与最大似然EM算法中产生的计算相对应，因此使用这种贝叶斯方法几乎没有更多的计算代价。然而，这种方法有一些重要的优点。首先，在最大似然方法中，当一个高斯分量“退化”到一个具体的数据点时，会产生奇异性，而这种奇异性在贝叶斯方法中不存在。实际上，如果我们简单地引入一个先验分布，煞有然后使用MAP估计而不是最大似然估计，这种奇异性就会被消除。此外，当我们在混合分布中将混合分量的数量K选得较大时，不会出现过拟合问题。最后，变分方法使得我们可以在确定混合分布中分量的最优数量时不必借助于交互验证的技术。
\subsection*{变分下界}
下界提供了另一种推导变分重估计方程的方法。为了说明这一点，我们使用下面的事实：由于模型有共轭先验，因此变分后验分布(即$\boldsymbol{Z}$的离散分布、$\pi$的狄利克雷分布以及$(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)$的高斯-Wishart分布)的函数形式是已知的。通过使用这些分布的一般的参数形式，我们可以推导出下界的形式，将下界作为概率分布的参数的函数。关于这些参数最大化下界就会得到所需的重估计方程。
\subsection*{预测概率密度}
在高斯模型的贝叶斯混合的应用中，我们通常对观测变量的新值$\hat{\boldsymbol{x}}$的预测概率密度感兴趣。与这个观测相关联的有一个潜在变量$\hat{\boldsymbol{z}}$，从而预测概率分布为
\begin{equation}
	p(\hat{\boldsymbol{x}}|\boldsymbol{X})=\sum_{\hat{\boldsymbol{z}}}\iiint p(\hat{\boldsymbol{x}}|\hat{\boldsymbol{z}},\boldsymbol{\mu},\boldsymbol{\Lambda})p(\hat{\boldsymbol{z}}|\boldsymbol{\pi})p(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda}|\boldsymbol{X})d\boldsymbol{\pi}d\boldsymbol{\mu}d\boldsymbol{\Lambda}
\end{equation}
其中$p(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda}|\boldsymbol{X})$是参数的(未知)真实后验概率分布。使用公式$\ref{wei}$和公式$\ref{jue}$，我们可以首先完成在$\hat{\boldsymbol{z}}$上的求和，得到 
\begin{equation}
	p(\hat{\boldsymbol{x}}|\boldsymbol{X})=\sum_{k=1}^{K}\iiint \pi_k\mathcal{N}(\hat{\boldsymbol{x}}|\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k^{-1})p(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda}|\boldsymbol{X})d\boldsymbol{\pi}d\boldsymbol{\mu}d\boldsymbol{\Lambda}
\end{equation}
由于剩下的积分是无法计算的，因此我们通过将真实后验概率分布$p(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\Lambda}|\boldsymbol{X})$用它的变分近似$q(\boldsymbol{\pi})q(\boldsymbol{\mu},\boldsymbol{\Lambda})$替换的方式来近似预测概率分布，结果为
\begin{equation}
		p(\hat{\boldsymbol{x}}|\boldsymbol{X})\simeq \sum_{k=1}^{K}\iiint \pi_k\mathcal{N}(\hat{\boldsymbol{x}}|\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k^{-1})q(\boldsymbol{\pi})q(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)d\boldsymbol{\pi}d\boldsymbol{\mu}_kd\boldsymbol{\Lambda}_k
\end{equation}
其中我们使用了公式$\ref{bea}$给出的分解方式，并且在每一项中，我们已经隐式地将$j\ne k$的全部$\{\boldsymbol{\mu}_j,\boldsymbol{\Lambda}_j \}$变量积分出去。剩余的积分现在可以解析地计算，得到一个学生t分布的混合，即
\begin{equation}
	p(\hat{\boldsymbol{x}}|\boldsymbol{X})\simeq\frac{1}{\hat{\alpha}}\sum_{k=1}^{K}\alpha_k\mathrm{St}(\hat{\boldsymbol{x}}|\boldsymbol{m}_k,\boldsymbol{L}_k,v_k+1-D)
\end{equation}
其中第k个分量的均值为$\boldsymbol{m}_k$，精度为
\begin{equation}
	\boldsymbol{L}_k=\frac{(v_k+1-D)\beta_k}{1+\beta_k}\boldsymbol{W}_k
\end{equation}
当数据集的大小N很大时，预测分布就变成了高斯混合。
\subsection*{确定分量的数量}
变分下界可以用来确定具有K个分量的混合模型的后验概率分布。然而，这里有一个需要强调的比较微妙的地方。对于高斯混合模型的任意给定的参数设置(除了一些特殊的退化的设置之外)，会存在一些其他的参数设置，对于这些参数设置，观测变量上的概率密度是完全相同的。

在最大似然方法中，这种冗余性是不相关的，因为参数最优化算法(例如EM算法)会依赖于参数的初始值，找到一个具体的解，其他的等价的解不起作用。然而，在贝叶斯方法中，我们对所有可能的参数进行积分或求和。如果真实的后验概率分布是多峰的那么基于最小化$\mathrm{KL}(q\| p)$的变分推断会倾向于在某一个峰值的领域内近似这个分布，而忽视其他的峰值。由于等价的峰值具有等价的预测分布，因此只要我们考虑一个具有具体的数量K个分量组成的模型，那么这种等价性就无需担心。然而，如果我们想比较不同的K值，那么我们需要考虑这种多峰性。一个简单的近似解法是当我们进行模型比较和平均时，在下界中增加一项$\ln K!$。

值得再次强调的是，最大似然方法会使得似然函数的值随着K的值单调递增(假设奇异解已经被避开，并且不考虑局部极大值的效果)，因此不能够用于确定一个合适的模型复杂度。相反，贝叶斯推断自动地进行了模型复杂度和数据拟合之间的折中。
\subsection*{诱导分解}
在推导高斯混合模型的这些变分更新方程时，我们假定了对变分后验概率分布的一种特定的分解方式，由公式$\ref{anwei}$给定。然而，不同因子的最优解给出了额外的分解。特别地，$q^*(\boldsymbol{\mu},\boldsymbol{\Lambda})$的最优解由每个混合分量k上的独立分布$q^*(\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k)$的乘积给定，而公式$\ref{qz}$给定的潜在变量上的变分后验概率分布$q^*(\boldsymbol{Z})$可以分解为每个观测n的独立概率分布$q^*(\boldsymbol{z}_n)$(注意它不能关于k进行分解，因为对于每个n值，$z_{nk}$需要满足在k上的加和等于1的限制)。这些额外的分解的产生原因是假定的分解方式与真实分布的条件独立性质相互作用的结果。

我们会把这些额外的分解方式称为诱导分解(induced factorizations)，因为它们产生于在变分后验分布中假定的分解方式与真实联合概率分布的条件独立性质之间的相互作用。

使用一种基于d-划分的简单的图检测方法，这种诱导的分解方式可以很容易地被检测到。