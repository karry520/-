\section{主成分分析}
主成分分析，或者称为PCA，是一种被广泛使用的技术，应用的领域包括维度降低、有损数据压缩、特征抽取「数据可视化。它也被称为Karhunen-Loeve变换。

有两种经常使用的PCA的定义，它们会给出同样的算法。PCA可以被定义为数据在低维线性空间上的正交投影，这个线性空间被称为主空间(principal subspace)，使得投影数据的方差被最大化。等价地，它也可以被定义为使得平均投影代价最小的线性投影。平均投影代价是指数据点和它们的投影之间的平均平方距离。

考察一组观测数据集$\{x_n\}$，其中$n=1,\dots,N$，因此$x_n$是一个D维欧几里德空间中的变量。我们的目标是将数据投影到维度$M<D$的空间中，同时最大化投影数据的方差。\\
样本集合
\begin{equation}
X=(x_1\ x_2\ \dots x_N)^T_{N\times p}=
\begin{pmatrix}
x_1^T\\x_2^T\\\vdots\\x_N^T
\end{pmatrix}=
\begin{pmatrix}
x_{11}&x_{12}&\dots&x_{1p}\\	x_{21}&x_{22}&\dots&x_{2p}\\
\dots&\dots&\dots&\dots\\
x_{11}&x_{12}&\dots&x_{np}
\end{pmatrix}_{N\times p}
\end{equation}
样本均值
\begin{equation}
\bar{x}=\frac{1}{N}\sum_{n=1}^{N}x_n=\frac{1}{N}(x_1\ x_2\ \dots\ x_n)
\begin{pmatrix}
1\\1\\\vdots \\1
\end{pmatrix}_{N\times 1}=\frac{1}{N}X^TI_N
\end{equation}
样本方差
\begin{equation}
\begin{aligned}
S&=\frac{1}{N}\sum_{n=1}^{N}(x_n-\bar{x})(x_n-\bar{x})^T\\
&=\frac{1}{N}(x_1-\bar{x}\ x_2-\bar{x}\ \dots \ x_N-\bar{x})
\begin{pmatrix}
(x_1-\bar{x})^T\\ (x_2-\bar{x})^T\\ \vdots \\ (x_N-\bar{x})^T
\end{pmatrix}\\
&=\frac{1}{N}X^T(\overbrace{I_N-\frac{1}{N}I_NI_N^T}^{H_N-\text{中心矩阵}})\cdot (I_N-\frac{1}{N}I_NI_N^T)^T\cdot X\\
&=\frac{1}{N}X^THX
\end{aligned}	
\end{equation}
中心矩阵$H$有以下良好的性质 
\begin{flalign}
H&=I_N-\frac{1}{N}I_NI_N^T\\
H^T&=H\\
H^n&=H
\end{flalign}
\subsection*{最大方差形式}
首先，考虑在一维空间$(M=1)$上的投影。我们可心使用D维向量$u_1$定义这个空间的方向。不失一般性，我们假定选择一个单位向量，从而$u_1^Tu_1=1$(注意，我们只对$u_1$的方向感兴趣，而对$u_1$本身的大小不感兴趣)。这样，每个数据点$x_n$被投影到一个标题值$x_1^Tx_n$上。投影数据的均值是$u_1^T\bar{x}$。投影数据的方差为
\begin{equation}
\begin{aligned}
	J&=\frac{1}{N}\sum_{n=1}^{N}\{u_1^Tx_n-u_1^T\bar{x}\}^2\\
	&=u_1^T\underbrace{\sum_{n=1}^{N}\frac{1}{N}(x_n-\bar{x})\cdot(x_n-\bar{x})^T}_Su_1\\
	&=u_1^TSu_1
\end{aligned}	
\end{equation}
S是数据的协方差矩阵，优化问题变成
\begin{equation}
	\begin{aligned}
		\hat{u_1}&=arg \mathop{max} u_1^TSu_1\\
		s.t\ &\ u_1^Tu_1=1
	\end{aligned}
\end{equation}
利用拉格朗日乘子法，求偏导并令其为零，我们看到驻点满足
\begin{equation}
	Su_1=\lambda_1u_1
\end{equation}
表明$u_1$一定是S的一个特征向量，这个特征向量被称为第一主成分。

我们可以用一种增量的方式定义额外的主成分，方法为：在所有与那些已经考虑过的方向正交的所有可能的方向中，将新的方向选择为最大化投影方差的方向。

\textbf{总结一下}，主成分分析涉及到计算数据集的均值$\bar{x}$和协方差矩阵$S$，然后寻找$S$的对应于M个最大特征值的M个特征向量。
\subsection*{最小误差形式}
引入$D$维基向量的一个完整的单位正交集合$\{u_i\}$，其中$i=1,\dots,D$，且满足 
\begin{equation}
	u_i^Tu_j=\delta_{ij}
\end{equation}
由于基是完整的，因此每个数据点可以精确地表示为基向量的一个纯性组合，即
\begin{equation}
	x_n=\sum_{i=1}^{D}a_{ni}u_i
\end{equation}
其中，系数$a_{ni}$对于不同的数据点来说是不同的。这对应于将坐标系旋转到了一个由$\{u_i\}$定义的新坐标系，原始的D个分量$\{x_{n1},\dots,x_{nD}\}$被替换为一个等价的集合$\{a_{n1},\dots,a_{nD}\}$。我们有$a_{nj}=x_n^Tu_j$，因此不失一般性
\begin{equation}
	x_n=\sum_{i=1}^{D}(x_n^Tu_i)u_i
\end{equation}
然而，我们的目标是使用限定数量$M<D$个变量的一种表示方法来近似数据点，这对应于在低维子空间上的一个投影。不失一般性，M维线性子空间可以用前M个基向量表示，因此我们可以用下式来近似每个数据点$x_n$
\begin{equation}
	\widetilde{x}_n=\sum_{i=1}^{M}z_{ni}u_i+\sum_{i=M+1}^{D}b_iu_i
\end{equation}
其中$\{z_{ni}\}$依赖于特定的数据点，而$\{b_i\}$是常数，对于所有数据点都相同。我们可以任意选择$\{u_i\},\{z_{ni}\},\{b_i\}$，从而最小化由维度降低所引入的失真。作为失真的度量，我们使用原始数据点与它的近似点$\widetilde{x}_n$之间的平方距离，在数据集上取平均。因此我们的目标是最小化
\begin{equation}
	J=\frac{1}{N}\sum_{n=1}^{N}\Vert x_n -\widetilde{x}_n \Vert^2=\frac{1}{N}\sum_{n=1}^{N}\sum_{i=M+1}^{D}(x_n^Tu_i-\bar{x}^Tu_i)^2=\sum_{i=M+1}^{D}u_i^TSu_i
\end{equation}
剩下的任务是关于$\{u_i\}$对J进行最小化。同样利用拉格朗日乘子法能够得到 
\begin{equation}
	Su_i=\lambda_iu_i
\end{equation}
\subsection*{PCA的应用}
首先考虑PCA对于数据压缩的应用。我们可以写出对数据向量$\boldsymbol{x}_n$的PCA近似，形式为
\begin{equation}
\begin{aligned}
	\tilde{\boldsymbol{x}}_n&=\sum_{i=1}^{M}(\boldsymbol{x}_n^T\boldsymbol{u}_i)\boldsymbol{u}_i+\sum_{i=M+1}^{D}(\bar{\boldsymbol{x}}^T\boldsymbol{u}_i)\boldsymbol{u}_i\\
	&=\bar{\boldsymbol{x}}+\sum_{i=1}^{M}(\boldsymbol{x}_n^T\boldsymbol{u}_i-\bar{\boldsymbol{x}}^T\boldsymbol{u}_i)\boldsymbol{u}_i
\end{aligned}
\end{equation}
其中我们使用了关系
\begin{equation}
	\bar{\boldsymbol{x}}=\sum_{i=1}^{D}(\bar{\boldsymbol{x}}^T\boldsymbol{u}_i)\boldsymbol{u}_i
\end{equation}
这个关系来自于$\{\boldsymbol{u}_i\}$的完整性。这种方法表示了对数据集的一个压缩，因为对于每个数据点，我们将D维向量$\boldsymbol{x}_n$替换为M维向量，元素为$(\boldsymbol{x}_n^T\boldsymbol{u}_i-\bar{\boldsymbol{x}}^T\boldsymbol{u}_i)$。M的值越小，压缩的程度越大。

主成分分析的另一个应用是数据预处理。在这种情况下，目标不是维度降低，而是对数据集进行变换，使得数据集的某些属性得到标准化。这对于后续将模式识别算法成功应用于数据集来说很重要。

使用PCA，我们可以对数据进行更显著的归一化，得到零均值和单位方差的数据，从而不同的变量之间的相关性关系被消除。为了完成这一点，我们首先将特征向量方程写成下面的形式
\begin{equation}
	\boldsymbol{SU}=\boldsymbol{UL}
\end{equation}
其中，$\boldsymbol{L}$是一个$D\times D$的对角矩阵，元素为$\lambda_i$，$\boldsymbol{U}$是一个$D\times D$的正交矩阵，列为$\boldsymbol{u}_i$。然后对于每个数据点$\boldsymbol{x}_n$，我们定义一个变换，值为
\begin{equation}
	\boldsymbol{y}_n=\boldsymbol{L}^{-\frac{1}{2}}\boldsymbol{U}^T(\boldsymbol{x}_n-\bar{\boldsymbol{x}})
\end{equation}
其中$\bar{\boldsymbol{x}}$是样本均值。很明显，集合$\{\boldsymbol{y}_n\}$的均值为零，协方差是单位矩阵，因为
\begin{equation}
	\begin{aligned}
		\frac{1}{N}\sum_{n=1}^{N}\boldsymbol{y}_n\boldsymbol{y}_n^T&=\frac{1}{N}\sum_{n=1}^{N}\boldsymbol{L}^{-\frac{1}{2}}\boldsymbol{U}^T(\boldsymbol{x}_n-\bar{\boldsymbol{x}})(\boldsymbol{x}_n-\bar{\boldsymbol{x}})^T\boldsymbol{UL}^{-\frac{1}{2}}\\
		&=\boldsymbol{L}^{-\frac{1}{2}}\boldsymbol{U}^T\boldsymbol{SUL}^{-\frac{1}{2}}\\
		&=\boldsymbol{L}^{-\frac{1}{2}}\boldsymbol{LL}^{-\frac{1}{2}}=\boldsymbol{I}
	\end{aligned}
\end{equation}
这个操作被称为对数据的白化(whitening)或者球人形化(sphereing)。

主成分分析的另一个常见应用是数据可视化。例如，每个数据点被投影到二维(M=2)的主子空间中，从而数据点$\boldsymbol{x}_n$被画在了一个笛卡尔坐标第中，坐标系由$\boldsymbol{x}_n^T\boldsymbol{u}_1$和$\boldsymbol{x}_n^T\boldsymbol{u}_2$定义，其中$\boldsymbol{u}_1$和$\boldsymbol{u}_2$是特征向量，对应于最大的和第二大的特征值。
\subsection*{高维数据的PCA}
在主成分分析的一些应用中，数据点的数量小于数据空间的维度。在一个D维空间中，N个数据点(N<D)定义了一个线性子空间，它的维度最多为$N-1$，因此在使用PCA时，几乎没有M大于$N-1$的数据点。实际上，如果我们运行PCA，我们会发现至少$D-N+1$个特征值为零，对应于沿着数据集的方差为零的方向的特征向量。此外，通常的寻找$D\times D$矩阵的特征向量的算法的计算代价为$O(D^3)$。因此对于诸如图像这种应用来说，直接应用PCA在计算上是不可行的。

我们可以这样解这个问题。首先，我们将$\boldsymbol{X}$定义为$(N\times D)$维中心数据矩阵，它的第n行为$(\boldsymbol{x}_n-\bar{\boldsymbol{x}})^T$。这样，协方差矩阵可以写成$\boldsymbol{S}=N^{-1}\boldsymbol{X}^T\boldsymbol{X}$，对应的特征向量方程变成了
\begin{equation}
	\frac{1}{N}\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{u}_i=\lambda_i\boldsymbol{u}_i
\end{equation}
现在，将两侧左乘$\boldsymbol{X}$，可得
\begin{equation}
	\frac{1}{N}\boldsymbol{X}\boldsymbol{X}^T(\boldsymbol{X}\boldsymbol{u}_i)=\lambda_i(\boldsymbol{Xu}_i)
\end{equation}
如果我们现在定义$\boldsymbol{v}_i=\boldsymbol{Xu}_i$，那么我们有
\begin{equation}
\label{lian}
	\frac{1}{N}\boldsymbol{XX}^T\boldsymbol{v}_i=\lambda_i\boldsymbol{v}_i
\end{equation}
它是$N\times N$矩阵$N^{-1}\boldsymbol{XX}^T$的一个特征向量方程。我们看到这个矩阵与原始的协方差矩阵具有相同的$N-1$个特征值，原始的协方差矩阵本身有额外的$D-N+1$个值为零的特征值。因此我们可以在低维空间中解决特征向量问题，计算代价为$O(N^3)$而不是$O(D^3)$。为了确定特征向量，我们将公式$\ref{lian}$两侧乘以$\boldsymbol{X}^T$，可得
\begin{equation}
	\left(\frac{1}{N}\boldsymbol{X}^T\boldsymbol{X}\right)(\boldsymbol{X}^T\boldsymbol{v}_i)=\lambda_i(\boldsymbol{X}^T\boldsymbol{v}_i)
\end{equation}
从中我们可以看到$(\boldsymbol{X}^T\boldsymbol{v}_i)$是$\boldsymbol{S}$的一个特征向量，对应的特征值为$\lambda_i$。但是，需要注意，这些特征向量的长度未必等于1。为了确定合适的归一化，我们使用一个常数来对$\boldsymbol{u}_i\propto \boldsymbol{X}^T\boldsymbol{v}_i$进行重新标度，使得$\lVert \boldsymbol{u}_i\rVert=1$。假设$\boldsymbol{v}_i$的长度已经被归一化，那么我们有
\begin{equation}
\label{huaxiang}
	\boldsymbol{u}_i=\frac{1}{(N\lambda_i)^{\frac{1}{2}}}\boldsymbol{X}^T\boldsymbol{v}_i
\end{equation}
总结一下，为了应用这种方法，我们首先计算$\boldsymbol{XX}^T$，然后找到它的特征向量和特征值，之后使用公式$\ref{huaxiang}$计算原始数据空间的特征向量。