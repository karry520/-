\section{概率PCA}
前一节讨论的PCA的形式所基于的是将数据线性投影到比原始数据空间维度更低的子空间内。PCA也可以被视为概率潜在变量模型的最大似然解。PCA的这种形式，被称为概率PCA(probabilistic PCA)，与传统的PCA相比，会带来如下几个优势。
\begin{itemize}
	\item 概率PCA表示高斯分布的一个限制形式，其中自由参数的数量可以受到限制，同时仍然使得模型能够描述数据集的主要的相关关系。
	\item 我们可以为PCA推导一个EM算法，这个算法在只有几个主要的特征向量需要求出的情况下，计算效率比较高，并且避免了计算数据协方差的中间步骤。
	\item 概率模型与EM的结合使得我们能够处理数据集里缺失值的问题。
	\item 概率PCA混合模型可以用一种有理有据的方式进行形式化，并且可以使用EM算法进行训练。
	\item 概率PCA构成了PCA的贝叶斯方法的基础，其中主子空间的维度可以自动从数据中找到。
	\item 似然函数的存在使得直接与其他的概率密度模型进行对比成为可能。相反，传统的PCA会给接近主子空间的数据点分配一个较低的重建代价，即使这些数据点的位置距离训练数据任意远。
	\item 概率PCA可以被用来对类条件概率密度建模，因此可以应用于分类问题。
	\item 概率PCA模型可以用一种生成式的方式运行，从而可以按照某个概率分布生成样本。
\end{itemize}
这种概率模型形式的PCA由Tipping and Bishop和Roweis独立提出。它与因子分析(factor analysis)密切相关。

概率PCA是线性高斯框架的一个简单的例子，其中所有的边缘概率分布和条件概率分布都是高斯分布。我们可以按照下面的方式建立概率PCA模型。首先显式地引入潜在变量$\boldsymbol{z}$，对应于主成分子空间。接下来我们定义潜在变量上的一个高斯先验分布$p(z)$以及以潜在变量的值为条件，观测变量$\boldsymbol{x}$的高斯条件概率分布$p(\boldsymbol{x}|\boldsymbol{z})$。具体地说，$\boldsymbol{z}$上的先验概率分布是一个零均值单位协方差的高斯分布
\begin{equation}
	p(\boldsymbol{z})=\mathcal{N}(\boldsymbol{z}|0,\boldsymbol{I})
\end{equation}
类似地，以潜在变量$\boldsymbol{z}$的值为条件，观测变量$\boldsymbol{x}$的条件概率分布还是高斯分布，形式为
\begin{equation}
	p(\boldsymbol{x}|\boldsymbol{z})=\mathcal{N}(\boldsymbol{x}|\boldsymbol{W}\boldsymbol{z}+\boldsymbol{\mu},\sigma^2\boldsymbol{I})
\end{equation}
其中$\boldsymbol{x}$的均值是$\boldsymbol{z}$的一个一般的线性函数，由$D\times M$的矩阵$\boldsymbol{W}$和D维向量$\boldsymbol{\mu}$控制。注意，可以关于$\boldsymbol{x}$的各个元素进行分解，换句话说，这是朴素贝叶斯模型的一个例子。

我们可以从生成式的观点看待概率PCA模型，其中观测值的一个采样值通过下面的方式获得：首先为潜在变量选择一个值，然后以这个潜在变量的值为条件，对观测变量采样。具体来说，D维观测变量$\boldsymbol{x}$由M
维潜在变量$\boldsymbol{z}$的一个线性变换附加一个高斯“噪声”定义，即
\begin{equation}
\label{1233}
	\boldsymbol{x}=\boldsymbol{Wz}+\boldsymbol{\mu}+\boldsymbol{\epsilon}
\end{equation}
其中$\boldsymbol{z}$是一个M维高斯潜在变量，$\boldsymbol{\epsilon}$是一个D维零均值高斯分布的噪声变量，协方差为$\sigma^2\boldsymbol{I}$。注意，这个框架基于的是从潜在空间到数据空间的一个映射，这与之前讨论的PCA的传统观点不同。从数据空间到潜在空间的逆映射可以通过使用贝叶斯定理的方式得到。

假设我们希望使用最大似然的方式确定参数$\boldsymbol{W,\mu}$和$\sigma^2$的值。为了写出似然函数的表达式，我们需要观测变量的边缘概率分布$p(\boldsymbol{x})$的表达式。
\begin{equation}
	p(\boldsymbol{x})=\int p(\boldsymbol{x}|\boldsymbol{z})p(\boldsymbol{z})d\boldsymbol{z}
\end{equation}
由于这对应于一个线性高斯模型，因此边缘概率分布还是高斯分布，形式为
\begin{equation}
\label{1235}
	p(\boldsymbol{x})=\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu,C})
\end{equation}
其中$D\times D$协方差矩阵$C$被定义为
\begin{equation}
	\begin{aligned}
	C&=\mathrm{var}[\boldsymbol{x}]=\mathrm{var}[\boldsymbol{Wz}+\boldsymbol{\mu}+\boldsymbol{\epsilon}]\\
	&=\mathrm{var}[\boldsymbol{Wz}]+\mathrm{var}[\boldsymbol{\epsilon}]\\
	&=\boldsymbol{WIW}^T+\sigma^2\boldsymbol{I}\\
	&=\boldsymbol{WW}^T+\sigma^2\boldsymbol{I}
	\end{aligned}
\end{equation}
我们注意到预测概率分布是高斯分布，然后使用公式$\ref{1233}$计算它的均值和协方差，结果为
\begin{flalign}
	\mathbb{E}[\boldsymbol{x}]&=\mathbb{E}[\boldsymbol{Wz}+\boldsymbol{\mu}+\boldsymbol{\epsilon}]=\boldsymbol{\mu}\\
	\mathrm{var}[\boldsymbol{x}]&=\boldsymbol{WW}^T+\sigma^2\boldsymbol{I}
\end{flalign}
其中我们使用了下面的事实：$\boldsymbol{z}$和$\boldsymbol{\epsilon}$是独立的随机变量，因此非相关。

预测分布$p(\boldsymbol{x})$由参数$\boldsymbol{W,\mu}$和$\sigma^2$控制。然而，这些参数中存在冗余性，对应于潜在空间坐标的旋转。

与预测分布$p(\boldsymbol{x})$一样，我们也需要后验概率分布$p(\boldsymbol{z}|\boldsymbol{x})$，这可以直接使用公式给出的线性高斯模型的结果写出来，结果为
\begin{equation}
	p(\boldsymbol{z}|\boldsymbol{x})=\mathcal{N}(\boldsymbol{z}|\boldsymbol{M}^{-1}\boldsymbol{W}^T(\boldsymbol{x}-\boldsymbol{\mu}),\sigma^2\boldsymbol{M}^{-1})
\end{equation}
其中 
\begin{equation}
	\boldsymbol{M}=\sigma^2\boldsymbol{I}_M+\boldsymbol{W}^T\boldsymbol{W}
\end{equation}
注意，后验均值依赖于$\boldsymbol{x}$，而后验协方差与$\boldsymbol{x}$无关。
\subsection*{最大似然PCA}
我们接下来考虑使用最大似然法确定模型的参数，给定观测数据点的数据点$\boldsymbol{X}=\{\boldsymbol{x}_n\}$，概率PCA模型可以表示为一个有向图。
\begin{center}
	\begin{tikzpicture}[node distance=2cm]
		\tikzstyle{every node} = [color=red]
		
		\node[state] (A) {$Z_n$};
		\node[state,below of=A,fill=gray!25] (B) {$X_n$};
		
		\node[circle,inner sep=0.1cm,fill=red,left of=A,label=left:$\sigma^2$](C){}; 
		\node[circle,inner sep=0.1cm,fill=red,left of=B,label=left:$\boldsymbol{\mu}$](D){}; 
		\node[circle,inner sep=0.1cm,fill=red,right of=B,label=right:$\boldsymbol{W}$](E){}; 
		\node[right of=B,xshift=-1.4cm,yshift=-0.6cm,blue](G) {$N$};
		\node[draw,color=blue,inner sep=0.3cm,rectangle,fit=(A)(B),rounded corners](F){};
		
		\path (A) edge[->] (B)
		(C) edge[->] (B)
		(D) edge[->] (B)
		(E) edge[->] (B);
	\end{tikzpicture}
\end{center}
根据公式$\ref{1235}$，对应的对数似然函数为
\begin{equation}
	\begin{aligned}
		\ln p(\boldsymbol{X}|\boldsymbol{\mu},\boldsymbol{W},\sigma^2)&=\sum_{n=1}^{N}\ln p(\boldsymbol{x}_n|\boldsymbol{\mu},\boldsymbol{W},\sigma^2)\\
		&=-\frac{ND}{2}\ln (2\pi)-\frac{N}{2}\ln |C|-\frac{1}{2}\sum_{n=1}^{N}(\boldsymbol{x}_n-\boldsymbol{\mu})^TC^{-1}(\boldsymbol{x}_n-\boldsymbol{\mu})
	\end{aligned}
\end{equation}
令似然函数关于$\boldsymbol{\mu}$的导数等于零，可以得到预期的结果$\boldsymbol{\mu}=\bar{\boldsymbol{x}}$，代回到似然函数中，我们有
\begin{equation}
\begin{aligned}
	\ln p(\boldsymbol{X}|\boldsymbol{\mu},\boldsymbol{W},\sigma^2)&=-\frac{N}{2}\{D\ln (2\pi)+\ln |C|\}+\frac{N}{2}\mathrm{Tr}\left(C^{-1}\frac{1}{N}\sum_{n=1}^{N}(\boldsymbol{x}_n-\boldsymbol{\mu})^T(\boldsymbol{x}_n-\boldsymbol{\mu}) \right)\\
	&=-\frac{N}{2}\{D\ln (2\pi)+\ln |C|+\mathrm{Tr}(C^{-1}S) \}
\end{aligned}
\end{equation}
其中$S$是协方差矩阵。由于对数似然函数是$\boldsymbol{\mu}$的二次函数，因此解具有唯一的最大值，可以通过计算二阶导数的方式验证这一点。

\begin{proof}迹运算描述Frobenius范数
	
	设$A$是m行n列的矩阵，A的行向量是$\overrightarrow{b}_1^T,\dots,\overrightarrow{b}_m^T$。那么 
	\begin{equation}
		A=\begin{pmatrix}
			\overrightarrow{b}_1^T\\
			\overrightarrow{b}_2^T\\
			\vdots\\
			\overrightarrow{b}_m^T
		\end{pmatrix},\qquad 
		A^T=\begin{pmatrix}
			\overrightarrow{b}_1,
			\overrightarrow{b}_2,
			\dots
			\overrightarrow{b}_m^T
		\end{pmatrix}
	\end{equation}
	\begin{equation}
		AA^T=\begin{pmatrix}
			\overrightarrow{b}_1^T\\
			\overrightarrow{b}_2^T\\
			\vdots\\
			\overrightarrow{b}_m^T
		\end{pmatrix}\begin{pmatrix}
			\overrightarrow{b}_1,
			\overrightarrow{b}_2,
			\dots
			\overrightarrow{b}_m^T
		\end{pmatrix}=\begin{pmatrix}
			\overrightarrow{b}_1^T\overrightarrow{b}_1&\overrightarrow{b}_1^T\overrightarrow{b}_2&
			\dots&
			\overrightarrow{b}_1^T\overrightarrow{b}_m\\
			\overrightarrow{b}_2^T\overrightarrow{b}_1&\overrightarrow{b}_2^T\overrightarrow{b}_2&
			\dots&
			\overrightarrow{b}_2^T\overrightarrow{b}_m\\
			\vdots&\vdots&\ddots&\vdots\\
			\overrightarrow{b}_m^T\overrightarrow{b}_1&\overrightarrow{b}_m^T\overrightarrow{b}_2&
			\dots&
			\overrightarrow{b}_m^T\overrightarrow{b}_m			
		\end{pmatrix}
	\end{equation}
	因为迹运算返回的是矩阵对角线元素的和，所以：
	\begin{equation}
		\mathrm{Tr}(AA^T)=\sum_{i=1}^{m}\overrightarrow{b}_i^T\overrightarrow{b}_i
	\end{equation}
	$\overrightarrow{b}_i^T$是矩阵A的第i行。$\overrightarrow{b}_i^T=(A_{i,1},\dots,A_{i,n})$，$\overrightarrow{b}_i^T\overrightarrow{b}_i$是矩阵A第i行行向量的内积。那么
	\begin{equation}
		\overrightarrow{b}_i^T\overrightarrow{b}_i=\sum_{j=1}^{n}A_{ij}^2
	\end{equation}
	推出
	\begin{equation}
		\mathrm{Tr}(AA^T)=\sum_{i=1}^{m}\sum_{j=1}^{n}A_{ij}^2
	\end{equation}
	即
	\begin{equation}
		\lVert A \rVert_F=\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}A_{ij}^2}=\sqrt{\mathrm{Tr}(AA^T)}
	\end{equation}
\end{proof}

令$F=-\ln p(\boldsymbol{X}|\boldsymbol{\mu},\boldsymbol{W},\sigma^2)$(忽略掉常数项)。
\begin{equation}
	F=\ln |C|-\mathrm{Tr}(C^{-1}S)
\end{equation}
\begin{enumerate}
	\item 令 $\frac{\mathrm{d}F}{\mathrm{d}\boldsymbol{W}}=0$求$\boldsymbol{W}$
	\begin{equation}
		\begin{aligned}
			\mathrm{d}F&=\mathrm{d}\ln |\boldsymbol{C}| +\mathrm{d}\mathrm{Tr}(\boldsymbol{C}^{-1}\boldsymbol{S})\\
			&=\mathrm{Tr}(\boldsymbol{C}^{-1}\mathrm{d}C)+\mathrm{Tr}(\mathrm{d}C^{-1}\boldsymbol{S})\\
			&=\mathrm{Tr}(\boldsymbol{C}^{-1}\mathrm{d}C)-\mathrm{Tr}(\boldsymbol{C}^{-1}\mathrm{d}\boldsymbol{C}\boldsymbol{C}^{-1}\boldsymbol{S})\\
			&=\mathrm{Tr}(\boldsymbol{C}^{-1}(\mathrm{d}\boldsymbol{WW}^T+\boldsymbol{W}\mathrm{d}\boldsymbol{W}^T))-\mathrm{Tr}(\boldsymbol{C}^{-1}(\mathrm{d}\boldsymbol{WW}^T+\boldsymbol{W}\mathrm{d}\boldsymbol{W}^T)\boldsymbol{C}^{-1}\boldsymbol{S})
		\end{aligned}
	\end{equation}
	根据迹的性质，有
	\begin{equation}
		\mathrm{Tr}(\boldsymbol{C}^{-1}\mathrm{d}\boldsymbol{WW}^T)=\mathrm{Tr}(\boldsymbol{W}\mathrm{d}\boldsymbol{W}^T\boldsymbol{C}^{-1})=\mathrm{Tr}(\boldsymbol{C}^{-1}\boldsymbol{W}\mathrm{d}\boldsymbol{W}^T)
	\end{equation}
	代入上式中，有
	\begin{equation}
		\begin{aligned}
			\mathrm{d}F&=2\mathrm{Tr}(\boldsymbol{C}^{-1}\boldsymbol{W}\mathrm{d}\boldsymbol{W}^T)-2\mathrm{Tr}(\boldsymbol{C}^{-1}\boldsymbol{SC}^{-1}\boldsymbol{W}\mathrm{d}\boldsymbol{W}^T)\\
			&=2\mathrm{Tr}[(\boldsymbol{C}^{-1}\boldsymbol{W}-\boldsymbol{C}^{-1}\boldsymbol{SC}^{-1}\boldsymbol{W})\mathrm{d}\boldsymbol{W}^T]
		\end{aligned}
	\end{equation}
	因此，
	\begin{equation}
		\frac{1}{2}\frac{\mathrm{d}F}{\mathrm{d}\boldsymbol{W}}=\boldsymbol{C}^{-1}\boldsymbol{W}-\boldsymbol{C}^{-1}\boldsymbol{SC}^{-1}\boldsymbol{W}
	\end{equation}
	令$\frac{\mathrm{d}F}{\mathrm{d}\boldsymbol{W}}=0$，可求得
	\begin{equation}
	\begin{aligned}
		\boldsymbol{W}&=\boldsymbol{SC}^{-1}\boldsymbol{W}\\
		&=\boldsymbol{S}(\sigma^2\boldsymbol{I}_D+\boldsymbol{WW}^T)^{-1}\boldsymbol{W}\\
		&=\boldsymbol{S}\boldsymbol{W}(\sigma^2\boldsymbol{I}_M+\boldsymbol{W}^T\boldsymbol{W})^{-1}
	\end{aligned}
	\end{equation}
	对$\boldsymbol{W}^T\boldsymbol{W}=\boldsymbol{V}\Lambda \boldsymbol{V}^T$作分解，代入有
	\begin{equation}
		\begin{aligned}
			&\boldsymbol{S}\boldsymbol{W}(\boldsymbol{V}\sigma^2\boldsymbol{I}_M\boldsymbol{V}^T+\boldsymbol{V}\Lambda \boldsymbol{V}^T)^{-1}=\boldsymbol{W}\\
			&\Rightarrow\boldsymbol{S}\boldsymbol{W}{(\boldsymbol{V}^T)}^{-1}(\sigma^2\boldsymbol{I}_M+\Lambda )^{-1}\boldsymbol{V}^{-1}=\boldsymbol{W}\\
			&\Rightarrow\boldsymbol{S}\boldsymbol{W}\boldsymbol{V}(\sigma^2\boldsymbol{I}_M+\Lambda )^{-1}=\boldsymbol{W}\boldsymbol{V}
		\end{aligned}
	\end{equation}
	对$\boldsymbol{WV}$正交化，有
	\begin{equation}
		\Lambda^{-\frac{1}{2}}\boldsymbol{V}^T\boldsymbol{W}^T\boldsymbol{W}\boldsymbol{V}\Lambda^{-\frac{1}{2}}=\Lambda^{-\frac{1}{2}}\boldsymbol{V}^T\boldsymbol{V}\Lambda \boldsymbol{V}^T\boldsymbol{V}\Lambda^{-\frac{1}{2}}=\boldsymbol{I}
	\end{equation}
	因此，我们有
	\begin{equation}
		\boldsymbol{S}\boldsymbol{W}\boldsymbol{V}\Lambda^{-\frac{1}{2}}=\boldsymbol{W}\boldsymbol{V}\Lambda^{-\frac{1}{2}}(\sigma^2\boldsymbol{I}+\Lambda )
	\end{equation}
	令$U_M=\boldsymbol{WV\Lambda}^{-\frac{1}{2}},L_M=\sigma^2\boldsymbol{I}_M+\Lambda$，我们有
	\begin{equation}
		\begin{aligned}
			\boldsymbol{S}U_M&=U_ML_M\\
			\Rightarrow \boldsymbol{W}_{ML}&=U_M\Lambda^{\frac{1}{2}}\boldsymbol{V}^T= U_M(L_M-\sigma^2\boldsymbol{I}_M)^{\frac{1}{2}}\boldsymbol{V}^T
		\end{aligned}
	\end{equation}
	
	
	\item 令$\frac{\mathrm{d}F}{\mathrm{d}\sigma^2}=0$求$\sigma^2$
	
	\begin{equation}
		\begin{aligned}
			\mathrm{d}F&=\mathrm{Tr}(\boldsymbol{C}^{-1}\mathrm{d}\sigma^2\boldsymbol{I})-\mathrm{Tr}(\boldsymbol{C}^{-1}\mathrm{d}\sigma^2\boldsymbol{C}^{-1}\boldsymbol{S})\\
			&=[\mathrm{Tr}(\boldsymbol{C}^{-1})-\mathrm{Tr}(\boldsymbol{C}^{-1}\boldsymbol{S}\boldsymbol{C}^{-1})]\mathrm{d}\sigma^2
		\end{aligned}
	\end{equation}
	令
	\begin{equation}
		\frac{\mathrm{d}F}{\mathrm{d}\sigma^2}=\mathrm{Tr}(\boldsymbol{C}^{-1}-\boldsymbol{C}^{-1}\boldsymbol{S}\boldsymbol{C}^{-1})=0
	\end{equation}
	计算$\boldsymbol{C}^{-1}$
	\begin{equation}
		\boldsymbol{C}^{-1}=(\sigma^2\boldsymbol{I}_D+\boldsymbol{WW}^T)^{-1}=\sigma^{-2}\boldsymbol{I}_D-\sigma^{-2}\boldsymbol{W}(\sigma^2\boldsymbol{I}_M+\boldsymbol{W}^T\boldsymbol{W})^{-1}\boldsymbol{W}^T
	\end{equation}
	等式两边同乘$\boldsymbol{S}$有
	\begin{equation}
		\begin{aligned}
			\boldsymbol{S}\boldsymbol{C}^{-1}&=\sigma^{-2}\boldsymbol{S}-\sigma^{-2}\boldsymbol{S}\boldsymbol{W}(\sigma^2\boldsymbol{I}_M+\boldsymbol{W}^T\boldsymbol{W})^{-1}\boldsymbol{W}^T\\
			&=\sigma^{-2}\boldsymbol{S}-\sigma^{-2}\boldsymbol{WW}^T\quad (\text{因为}\boldsymbol{S}\boldsymbol{W}(\sigma^2\boldsymbol{I}_M+\boldsymbol{W}^T\boldsymbol{W})^{-1}=\boldsymbol{W})
		\end{aligned}
	\end{equation}
	代入上式中，有
	\begin{equation}
	\begin{aligned}
		\boldsymbol{C}^{-1}\boldsymbol{S}\boldsymbol{C}^{-1}-\boldsymbol{C}^{-1}&=\boldsymbol{C}^{-1}\sigma^{-2}\boldsymbol{S}-\boldsymbol{C}^{-1}\sigma^{-2}\boldsymbol{WW}^T-\boldsymbol{C}^{-1}\\
		&=\sigma^{-2}\boldsymbol{C}^{-1}\boldsymbol{S}-\boldsymbol{C}^{-1}\sigma^{-2}\boldsymbol{WW}^T-\boldsymbol{C}^{-1}\\
		&=\sigma^{-2}(\sigma^{-2}\boldsymbol{S}-\sigma^{-2}\boldsymbol{WW}^T)-\boldsymbol{C}^{-1}\sigma^{-2}\boldsymbol{WW}^T-\boldsymbol{C}^{-1}\\
		&=\sigma^{-2}(\sigma^{-2}\boldsymbol{S}-\sigma^{-2}\boldsymbol{WW}^T)-\sigma^{-2}\boldsymbol{C}^{-1}(\boldsymbol{C}-\sigma^2\boldsymbol{I}_D)-\boldsymbol{C}^{-1}\\
		&=\sigma^{-2}(\sigma^{-2}\boldsymbol{S}-\sigma^{-2}\boldsymbol{WW}^T)-\sigma^{-2}\boldsymbol{I_D}\\
		&=\sigma^{-4}\boldsymbol{S}-\sigma^{-4}\boldsymbol{WW}^T-\sigma^{-2}\boldsymbol{I}_D\\
	\end{aligned}
	\end{equation}
	求得
	\begin{equation}
		\begin{aligned}
			\frac{\mathrm{d}F}{\mathrm{d}\sigma^2}=\mathrm{Tr}(\boldsymbol{C}^{-1}-\boldsymbol{C}^{-1}\boldsymbol{S}\boldsymbol{C}^{-1})=\sigma^{-4}\mathrm{Tr}(\boldsymbol{S}-\boldsymbol{WW}^T-\sigma^2\boldsymbol{I}_D)=0
		\end{aligned}
	\end{equation}
	我们有
	\begin{equation}
		\begin{aligned}
			\sigma^2\mathrm{Tr}(\boldsymbol{I}_D)&=\mathrm{Tr}(\boldsymbol{S})-\mathrm{Tr}(\boldsymbol{W}^T\boldsymbol{W})\\
			D\sigma^2&=\mathrm{Tr}(\boldsymbol{S})-\mathrm{Tr}(L-\sigma^2\boldsymbol{I}_M)\\
			&=\mathrm{Tr}(\boldsymbol{S})-\mathrm{Tr}(L_M)+M\sigma^2\\
			(D-M)\sigma^2&=\mathrm{Tr}(\boldsymbol{S})-\mathrm{Tr}(L_M)\\
			\sigma^2&=\frac{\mathrm{Tr}(\boldsymbol{S})-\mathrm{Tr}(L_M)}{D-M}\\
			&=\frac{1}{D-M}\sum_{i=M+1}^{D}\lambda_i
		\end{aligned}
	\end{equation}
\end{enumerate}
总结一下，
\begin{flalign}
	\boldsymbol{W}_{ML}&=U_M(L_M-\sigma^2\boldsymbol{I}_M)^{\frac{1}{2}}\boldsymbol{V}^T\\
	\sigma_{ML}^2&=\frac{1}{D-M}\sum_{i=M+1}^{D}\lambda_i
\end{flalign}
传统的PCA通常的形式是D维空间的数据点在M维线性子空间上的投影。然而，概率PCA可以很自然地表示为从潜在空间到数据空间的映射，由公式$\ref{1233}$给出。对于数据可视化和数据压缩之类的应用，我们可以使用贝叶斯定理将这个映射取逆。这样，任何在数据空间中的点$\boldsymbol{x}$都可以使用潜在空间中的后验均值和方差进行概括。最后，我们注意到，概率PCA模型在定义多元高斯分布时具有重要的作用，其中自由度的数量(即独立参数的数量)可以进行控制，同时仍然使得模型能够描述数据中的主要的相关关系。
\subsection*{用于PCA的EM算法}
概率PCA模型可以根据连续潜在空间$\boldsymbol{z}$上的积分或求和来表示，其中对于每个数据点$\boldsymbol{x}_n$，都存在一个对应的潜在变量$\boldsymbol{z}_n$。于是，我们可以使用EM算法来找到模型参数。这看起来似乎相当没有意义，因为我们已经得到了最大似然参数值的一个精确的解析解。然而，在高维空间中，使用迭代的EM算法而不是直接计算样本协方差矩阵可能会有一些计算上的优势。这个EM的求解步骤也可以推广到因子分析模型中，那里不存在解析解。最后，它使得我们可以用一种有理有据的方式处理缺失的数据。

我们可以使用一般的EM框架来推导用于概率PCA的EM算法。因此，我们写出完整数据对数似然函数，然后关于使用旧的参数值计算的潜在变量的后验概率分布求期望。最大化完整数据对数似然函数的期望就会产生新的参数值。因为我们假定数据点是独立的，因此完整数据对数似然函数的形式为
\begin{equation}
	L_c=\ln p(\boldsymbol{X},\boldsymbol{Z}|\boldsymbol{\mu},\boldsymbol{W},\sigma^2)=\sum_{n=1}^{N}\{\ln p(\boldsymbol{x}_n|\boldsymbol{z}_n)+\ln p(\boldsymbol{z}_n) \}
\end{equation}
这里，$\ln p(\boldsymbol{x}_n,\boldsymbol{z_n})$是
\begin{equation}
	\begin{aligned}
		\ln p(\boldsymbol{x}_n,\boldsymbol{z_n})&=\ln[p(\boldsymbol{x}_n|\boldsymbol{z}_n)p(\boldsymbol{z}_n)]\\
		&=\ln \left[\frac{1}{(2\pi)^{\frac{D}{2}}\sigma^D}\exp (-\frac{1}{\sigma^2}\lVert \boldsymbol{x}_n-\boldsymbol{\mu}-\boldsymbol{Wz}_n\rVert^2)\exp(-\frac{1}{2}\lVert\boldsymbol{z}_n\rVert^2) \right]\\
		&=-\frac{D}{2}\ln \sigma^2-\frac{1}{\sigma^2}\lVert \boldsymbol{x}_n-\boldsymbol{\mu}-\boldsymbol{Wz}_n\rVert^2-\frac{1}{2}\lVert\boldsymbol{z}_n\rVert^2\quad (\text{忽略了常系数})\\
		&=-\frac{D}{2}\ln \sigma^2-\frac{1}{\sigma^2}\lVert \boldsymbol{x}_n-\boldsymbol{\mu}\rVert^2-\frac{1}{\sigma^2}\lVert\boldsymbol{Wz}_n\rVert^2+\frac{1}{\sigma^2}(\boldsymbol{x}_n-\boldsymbol{\mu})^T(\boldsymbol{Wz}_n)-\frac{1}{2}\lVert\boldsymbol{z}_n\rVert^2
	\end{aligned}
\end{equation}
\begin{enumerate}
	\item E-步：$\boldsymbol{\theta}=(\sigma^2,\boldsymbol{W})$，求$L_c$关于$p(\boldsymbol{z}_n|\boldsymbol{\theta})$的期望
	
	\begin{equation}
		\begin{aligned}
			Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)})&=\sum_{n=1}^{N}\int (\ln p(\boldsymbol{x}_n,\boldsymbol{z}_n))p(\boldsymbol{z}_n|\boldsymbol{x}_n,\boldsymbol{\theta}^{(t)})\mathrm{d}\boldsymbol{z}_n\\
			&=\sum_{n=1}^{N}\left\{-\int \frac{D}{2}\ln \sigma^2p(\boldsymbol{z}_n|\theta^{(t)})\mathrm{d}\boldsymbol{z}_n\right.\\
			&\ \ -\int \frac{1}{2\sigma^2}\lVert \boldsymbol{x}_n-\boldsymbol{\mu}\rVert^2p(\boldsymbol{z}_n|\boldsymbol{x}_n,\boldsymbol{\theta}^{(t)})\mathrm{d}\boldsymbol{z}_n\\
			&\ \ -\int \frac{1}{2\sigma^2}\lVert \boldsymbol{Wz}_n\rVert^2p(\boldsymbol{z}_n|\boldsymbol{x}_n,\boldsymbol{\theta}^{(t)})\mathrm{d}\boldsymbol{z}_n\\
			&\ \ -\int \frac{1}{\sigma^2}(\boldsymbol{x}_n-\boldsymbol{\mu})^T(\boldsymbol{Wz}_n)p(\boldsymbol{z}_n|\boldsymbol{x}_n,\boldsymbol{\theta}^{(t)})\mathrm{d}\boldsymbol{z}_n\\
			&\ \ \left.-\int \frac{1}{2}\lVert \boldsymbol{z}_n\rVert^2p(\boldsymbol{z}_n|\boldsymbol{x}_n,\boldsymbol{\theta}^{(t)})\mathrm{d}\boldsymbol{z}_n\right\}\\
			&=-\frac{ND}{2}\ln \sigma^2-\frac{N}{2\sigma^2}\mathrm{Tr}(\boldsymbol{S})\\
			&\ \ -\sum_{n=1}^{N}\int \left[\frac{1}{2\sigma^2}\lVert\boldsymbol{Wz}_n\rVert^2-\frac{1}{\sigma^2}(\boldsymbol{x}_n-\boldsymbol{\mu})^T(\boldsymbol{Wz}_n)+\frac{1}{2}\lVert\boldsymbol{z}_n\rVert^2 \right]p(\boldsymbol{z}_n|\boldsymbol{x}_n,\boldsymbol{\theta}^{(t)})\mathrm{d}\boldsymbol{z}_n
		\end{aligned}
	\end{equation}
	其中我们用到了
	\begin{equation}
		\mathrm{Tr}(\boldsymbol{S})=\frac{1}{N}\sum_{n=1}^{N}\left((\boldsymbol{x}_n-\boldsymbol{\mu})(\boldsymbol{x}_n-\boldsymbol{\mu})^T\right)
	\end{equation}
	我们定义
	\begin{equation}
		\langle\boldsymbol{z}_n\rangle=\int \boldsymbol{z}_np(\boldsymbol{z}_n|\boldsymbol{x}_n,\boldsymbol{\theta}^{(t)})\mathrm{d}\boldsymbol{z}_n=\boldsymbol{M}^{-1}_{(t)}\boldsymbol{W}_{(t)}^T(\boldsymbol{x}_n-\boldsymbol{\mu})
	\end{equation}
	因为$		(\boldsymbol{z}|\boldsymbol{x}-\boldsymbol{\mu})\sim \mathcal{N}(\boldsymbol{M}^{-1}\boldsymbol{W}^T(\boldsymbol{x}_n-\boldsymbol{\mu}),\sigma^2\boldsymbol{M}^{-1}$，这里$\boldsymbol{M}=\sigma^2\boldsymbol{I}_M+\boldsymbol{W}^T\boldsymbol{W})$
	\begin{equation}
		\langle \boldsymbol{z}_n,\boldsymbol{z}_n^T\rangle=\int \boldsymbol{z}_n\boldsymbol{z}_n^Tp(\boldsymbol{z}_n|\boldsymbol{x}_n,\boldsymbol{\theta}^{(t)})\mathrm{d}\boldsymbol{z}_n=\sigma_{(t)}^2\boldsymbol{M}_{(t)}^{-1}+\langle\boldsymbol{z}_n\rangle\langle\boldsymbol{z}_n\rangle^T
	\end{equation}
	因为
	\begin{equation}
		\mathrm{Cov}(\boldsymbol{z}_n)=\mathbb{E}(\boldsymbol{z}_n\boldsymbol{z}_n^T)-\mathbb{E}(\boldsymbol{z}_n)\mathbb{E}(\boldsymbol{z}_n^T)
	\end{equation}
	因此，$Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)})$为
	\begin{equation}
	\begin{aligned}
		Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)})&=-\frac{ND}{2}\ln \sigma^2-\frac{N}{2\sigma^2}\mathrm{Tr}(\boldsymbol{S})\\
		&-\sum_{n=1}^{N}\left\{\frac{1}{2\sigma^2}\mathrm{Tr}(\boldsymbol{W}^T\boldsymbol{W}\langle \boldsymbol{z}_n,\boldsymbol{z}_n^T\rangle)-\frac{1}{\sigma^2}(\boldsymbol{x}_n-\boldsymbol{\mu})^T\boldsymbol{W}\langle\boldsymbol{z}_n\rangle+\frac{1}{2}\mathrm{Tr}(\langle \boldsymbol{z}_n,\boldsymbol{z}_n^T\rangle) \right\}
	\end{aligned}
	\end{equation}
	\item M-步：最大化$Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)})$
	\begin{enumerate}[(a)]
		\item 
		\begin{equation}
			\begin{aligned}
				&\frac{\mathrm{d}Q}{\mathrm{d}\boldsymbol{W}}=\frac{1}{\sigma^2}\sum_{n=1}^{N}(\boldsymbol{x}_n-\boldsymbol{\mu})\langle \boldsymbol{z}_n^T\rangle -\frac{1}{\sigma^2}\sum_{n=1}^{N}\boldsymbol{W}\langle \boldsymbol{z}_n,\boldsymbol{z}_n^T\rangle =0\\
				\Rightarrow&\boldsymbol{W}\sum_{n=1}^{N}\langle \boldsymbol{z}_n,\boldsymbol{z}_n^T\rangle =\sum_{n=1}^{N}(\boldsymbol{x}_n-\boldsymbol{\mu})\langle \boldsymbol{z}_n^T\rangle\\
				\Rightarrow&\boldsymbol{W}^{(t+1)}=\left(\sum_{n=1}^{N}(\boldsymbol{x}_n-\boldsymbol{\mu})\langle \boldsymbol{z}_n^T\rangle \right)\left(\sum_{n=1}^{N}\langle \boldsymbol{z}_n,\boldsymbol{z}_n^T\rangle \right)^{-1}
			\end{aligned}
		\end{equation}
		\item 
		\begin{equation}
			\begin{aligned}
				&\frac{\partial Q}{\partial \sigma^2}=-\frac{ND}{2}\frac{1}{\sigma^2}+\frac{N}{2\sigma^4}\mathrm{Tr}(\boldsymbol{S})+\frac{1}{2\sigma^4}\sum_{n=1}^{N}\mathrm{Tr}(\boldsymbol{W}^T\boldsymbol{W}\langle \boldsymbol{z}_n,\boldsymbol{z}_n^T\rangle)\\
				&\qquad -\frac{1}{\sigma^4}\sum_{n=1}^{N}(\boldsymbol{x}_n-\boldsymbol{\mu})^T\boldsymbol{W}\langle \boldsymbol{z}_n^T\rangle=0\\
				\Rightarrow&{\sigma^2}^{(t+1)}=\frac{1}{D}\left[\mathrm{Tr}(\boldsymbol{S})+\frac{1}{N}\sum_{n=1}^{N}\mathrm{Tr}(\boldsymbol{W}^T\boldsymbol{W}\langle \boldsymbol{z}_n,\boldsymbol{z}_n^T\rangle)-\frac{2}{N}(\boldsymbol{x}_n-\boldsymbol{\mu})^T\boldsymbol{W}\langle \boldsymbol{z}_n^T\rangle \right]
			\end{aligned}
		\end{equation}
		因为
		\begin{equation}
			\boldsymbol{W}\sum_{n=1}^{N}\langle \boldsymbol{z}_n,\boldsymbol{z}_n^T\rangle =\sum_{n=1}^{N}(\boldsymbol{x}_n-\boldsymbol{\mu})\langle \boldsymbol{z}_n^T\rangle 
		\end{equation}
		我们可以简化式子，得到 
		\begin{equation}
			{\sigma^2}^{(t+1)}=\frac{1}{D}\left[\mathrm{Tr}(\boldsymbol{S})+\frac{1}{N}\sum_{n=1}^{N}(\boldsymbol{x}_n-\boldsymbol{\mu})^T\boldsymbol{W}^{(t+1)}\langle\boldsymbol{z}_n\rangle \right]
		\end{equation}
	\end{enumerate}
\end{enumerate}

用于PCA的EM算法的一个好处是对于大规模应用的计算效率。与传统的基于样本协方差矩阵的特征向量分解的PCA不同，EM算法是迭代的，因此似乎没有什么吸引力，然而，在高维空间中，EM算法的每次迭代所需的计算量都要比传统的PCA小得多。注意，EM算法可以用一种在线的形式执行，其中每个D维数据点被读入、处理，然后在处理下一个数据点之前丢弃这个数据点。EM算法的另一个特征是，我们可以取极限$\sigma^2\to 0$，对应于标准的PCA，仍然可以得到一个合法的类似EM的算法。

\subsection*{贝叶斯PCA}
\subsection*{因子分析}