\section{非参数化方法}
使用一些非参数化方法进行概率密度估计。这种方法对概率分布的形式进行了很少的假设。
\subsection*{直方图法}
标准的直方图简单地把$x$划分成不同的宽度为$\triangle _i$的箱子，然后对落在第$i$个箱子中的$x$的观测数量$n_i$进行计数。为了把这种计数转换成归一化的概率密度，我们简单地把观测数量除以观测的总数$N$，再除以箱子的宽度$\triangle _i$，得到每个箱子的概率的值
\begin{equation}
	p_i=\frac{n_i}{N\triangle_i}
\end{equation}
在实际应用中，直方图方法对于快速地将一维或者二维的数据可视化很有用，但是并不适用于大多数概率密度估计的应用。一个明显的问题是估计的概率密度具有不连续性，这种不连续性是因为箱子的边缘造成的，而不是因为生成数据的概率分布本身的性质造成。直方图的另一个主要的局限性是维数放大。但是，概率密度估计的直方图方法确实告诉了我们两个重要的事情。
\begin{enumerate}
	\item 为了估计在某个特定位置的概率密度，我们应该考虑位于那个点的某个领域内的数据点。
	\item 为了获得好的结果，平滑参数的值既不能太大也不能太小。
\end{enumerate}
\subsection*{核密度估计}
假设观测服从$D$维空间的某个未知的概率密度分布$p(x)$。把这个$D$维空间选择成欧几里德空间，并且我们想估计$p(x)$的值。根据以前对于局部性的讨论，让我们考虑包含$x$的某个小区域$\mathcal{R}$。这个区域的概率质量为
\begin{equation}
	P=\int_\mathcal{R}p(x)dx
\end{equation}
现在我们假设收集了服从$p(x)$分布的$N$次观测。由于每个数据点都有一个落在区域$\mathcal{R}$中的概率$P$，因此位于区域$\mathcal{R}$内部的数据点的总数$K$将服从二项分布
\begin{equation}
	\mathrm{Bin}(K|N,P)=\frac{N!}{K!(N-K)!}P^K(1-P)^{N-K}
\end{equation}
落在区域内部的数据点的平均比例为$\mathbb{E}[\frac{K}{N}]=P$。类似地，以此为均值的概率分布的方差为$var[\frac{K}{N}]=\frac{P(1-P)}{N}$。对于大的$N$值，这个分布将会在均值附近产生尖峰，并且
\begin{flalign}
	K&\simeq NP\\
	P&\simeq p(x)V\\
	\label{2246}
	p(x)&=\frac{K}{NV}
\end{flalign}
上式的成立依赖于两个相互矛盾的假设，即区域$\mathcal{R}$要足够小，使得这个区域内的概率密度近似为常数，但是也要足够大，使得落在这个区域内的数据点的数量K能够足够让二项分布达到尖峰。

我们有两种方式利用这个结果。
\begin{enumerate}
	\item 固定K然后从数据中确定V的值，这就是K近邻方法。
	\item 固定V然后从数据中确定K的值，这就是核方法。
\end{enumerate}

在极限$N\to \infty$的情况下，如果V随着N而合适地收缩，并且K随着N增大，那么可以证明K近邻概率密度估计和核方法概率密度估计都会收敛到真实的概率密度。

把区域$\mathcal{R}$取成以$x$为中心的小超立方体，为了统计落在这个区域内的数据点的数量$K$，定义下面的函数比较方便
\begin{equation}
\label{te}
	k(u)=\begin{cases}
	1,\quad |u_i|\leqslant \frac{1}{2},i=1,\dots,D\\
	0,\quad \text{其它情况}
	\end{cases}
\end{equation}
这表示一个以原点为中心的单位立方体。函数$k(u)$是核函数的一个例子，在这个问题中也被称为Parzen窗(parzen window)。根据公式$\ref{te}$，如果数据点$x_n$位于以$x$为中心的边长为$h$的立方体中，那么量$k(\frac{x-x_n}{h})$的值等于1,否则它的值为0.于是
\begin{equation}
	p(x)=\frac{1}{N}\sum_{n=1}^{N}\frac{1}{h^D}k\left(\frac{x-x_n}{h} \right)
\end{equation}
这个函数表述为以N个数据点$x_n$为中心的N个立方体。

核密度估计有人为带来的非连续性的问题。如果我们选择一个平滑的核函数，那么我们就可以得到一个更加光滑的模型。
\begin{equation}
	p(x)=\frac{1}{N}\sum_{n=1}^{N}\frac{1}{(2\pi h^2)^{\frac{D}{2}}}exp\left\{-\frac{\Vert x-x_n\Vert^2}{2h^2} \right\}
\end{equation}
$h$表示高斯分布的标准差。因此我们概率密度模型可以通过这种方式获得：令每个数据点都服从高斯分布，然后把数据集里的每个数据点的贡献相加，之后除以$N$，使得概率密度正确地被归一化。

这种估计方法有一个很大的优点，即不需要进行“训练”阶段的计算，因为“训练”阶段只需要存储训练集即可。然而，这也是一个巨大的缺点，因为估计概率密度的计算代价随着数据集的规模线性增长。
\subsection*{近邻方法}
核方法进行概率密度估计的一个困难之处是控制核宽度的参数$h$对于所有的核都是固定的。与之不同，考虑固定$K$的值然后使用数据来确定合适的$V$值。为了完成这一点，我们考虑一个以$x$为中心的小球体，然后我们想估计概率密度$p(x)$。并且，我们允许球体的半径可以自由增长，直到它精确地包含$K$个数据点。这样，概率密度$p(x)$的估计就由公式$\ref{te}$给出，其中$V$等于最终球体的体积。这种方法被称为$K$近邻方法。

本章的最后，我们要说明概率密度估计的K近邻方法如何推广到分类问题。为了完成这一点，我们把K近邻概率密度估计方法分别应用到每个独立的类别中然后使用贝叶斯定理。

假设我们有一个数据集，其中$N_k$个数据点属于类别$C_k$，数据点的总数为N，因此$\sum_k N_k=N$。如果我们想对一个新的数据点$x$进行分类，那么我们可以画一个以$x$为中心的球体，这个球体精确地包含K个数据点(无论属于哪个类别)。假设球体的体积为V，并且包含来自类别$C_k$的$K_k$个数据点。这样公式$\ref{2246}$提供了与每个类别关联的一个概率密度的估计 
\begin{equation}
	p(x|C_k)=\frac{K_k}{N_kV}
\end{equation}
类似地，无条件概率密度为
\begin{equation}
	p(x)=\frac{K}{NV}
\end{equation}
而类先验为
\begin{equation}
	p(C_k)=\frac{N_k}{N}
\end{equation}
由贝叶斯定理，可以得到类别的后验概率 
\begin{equation}
	p(C_k|x)=\frac{p(x|C_k)p(C_k)}{p(x)}=\frac{K_k}{K}
\end{equation}
如果我们想最小化错误分类的概率，那么我们可以把测试点$x$分配给有着最大后验概率的类别，这对应于最大的$\frac{K_k}{K}$。因此为了分类一个新的数据点，我们从训练数据中选择K个最近的数据点，然后把新的数据点分配为这个集合数量最多的点的类别。

最近邻(K=1)分类器的一个有趣的性质是在极限$N\to \infty$的情况下，错误率不会超过最优分类器(即使用真实概率分布的分布器)可以达到的最小错误率的二倍。