\section{高斯分布}
高斯分布，也被称为正态分布，广泛应用于连续型随机变量分布的模型中。对于一元变量$x$的情形，
\begin{equation}
	\mathcal{N}(x|\mu,\sigma^2)=\frac{1}{(2\pi \sigma^2)^{\frac{1}{2}}}exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2 \right\}
\end{equation}
其中$\mu$是均值，$\sigma^2$是方差。
\begin{flalign}
	\mu_{ML}&=\frac{1}{N}\sum_{i=1}^{N}x_i\qquad \text{无偏}\\
	\sigma^2_{ML}&=\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu_{ML})^2\qquad \text{有偏}\\
	\mathbb{E}[\sigma_{ML}^2]&=\frac{N-1}{N}\sigma^2\\
	\hat{\sigma}^2&=\frac{1}{N-1}\sum_{i=1}^{N}(x_i-\mu_{ML})^2\qquad \text{无偏}	
\end{flalign}

对于D维向量$\boldsymbol{x}$，多元高斯分布的形式为
\begin{equation}
	\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu},\Sigma)=\frac{1}{(2\pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}}exp\left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu}) \right\}
\end{equation}
其中，$\mu$是一个D维均值向量，$\Sigma$是一个$D\times D$的协方差矩阵，$|\Sigma|$是$\Sigma$的行列式。对$\Sigma$进行正交分解。
\begin{flalign}
	\Sigma&=U\Lambda U^T\\
	UU^T&=U^TU=I\\
	\Lambda&=diag(\lambda_i)\\ U&=(u_1,u_2,\dots,u_p)_{p\times p}
\end{flalign}

考虑高斯分布的几何形式。高斯对于$\boldsymbol{x}$的依赖是通过下面形式的二次型
\begin{equation}
	\triangle^2=(\boldsymbol{x}-\boldsymbol{\mu})^T\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})
\end{equation}
$\triangle$被叫做$\boldsymbol{\mu}$和$\boldsymbol{x}$之间的马氏距离。协方差矩阵$\Sigma$可以表示成特征向量的展开的形式
\begin{equation}
	\begin{aligned}
	\Sigma&=U\Lambda U^T\\
	&=(u_1\ u_2\ \dots \ u_p)
	\begin{pmatrix}
		\lambda_1& \dots    & 0 \\
		\vdots   & \lambda_i&\vdots\\
		0        & \dots    &\lambda_p
	\end{pmatrix}
	\begin{pmatrix}
		u_1^T\\\vdots\\u_P^T
	\end{pmatrix}\\
	&=(u_1\lambda_1\ \dots \ u_p\lambda_p)
	\begin{pmatrix}
		u_1^T\\\vdots\\u_P^T
	\end{pmatrix}\\
	&=\sum_{i=1}^{P}u_i\lambda_i u_i^T
	\end{aligned}
\end{equation}
于是
\begin{equation}
\begin{aligned}
	\Sigma^{-1}&=(U\Lambda U^T)^{-1}=U\Lambda^{-1}U^T\\
	&=\sum_{i=1}^{P}u_i\frac{1}{\lambda_i}u_i^T
\end{aligned}
\end{equation}
马氏距离就可以表示为
\begin{equation}
	\begin{aligned}
		\triangle^2&=(\boldsymbol{x}-\boldsymbol{\mu})^T\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\\
		&=\sum_{i=1}^{P}\underbrace{(\boldsymbol{x}-\boldsymbol{\mu})^Tu_i}_{y_i}\frac{1}{\lambda_i}\underbrace{u_i^T(\boldsymbol{x}-\boldsymbol{\mu})}_{y_i^T}
		&=\sum_{i=1}^{P}\frac{y_i^2}{\lambda_i}
	\end{aligned}
\end{equation}
其中
\begin{equation}
	y_i=(\boldsymbol{x}-\boldsymbol{\mu})^Tu_i
\end{equation}
可以把$\{y_i\}$表示成单位正交向量$u_i$关于原始的$x_i$坐标经过平移和旋转后形成的新的坐标系。
\begin{center}
\begin{tikzpicture}
	\draw[->] (0,0) -- (6,0) node[right]{$x_1$};
	\draw[->] (0,0) -- (0,5) node[left]{$x_2$};
	\draw (0,0) node[below left] {$0$};
	
	\draw[red,rotate around={45:(2,2)}] (2,2) ellipse [x radius=2cm, y radius=1cm];
	
	\draw[->] (0,0) -- (4,4) node[right]{$u_1$};
	\draw[->] (4,0) -- (0,4) node[right]{$u_2$};
\end{tikzpicture}
\end{center}
高斯分布的均值和方差为
\begin{flalign}
	\mathbb{E}[\boldsymbol{x}]&=\boldsymbol{\mu}\\
	var[\boldsymbol{x}]&=\Sigma
\end{flalign}

虽然高斯分布被广泛用作概率密度模型，但是它有着一些巨大的局限性.
\begin{enumerate}
	\item 自由参数的数量。可以进一步地把协方差矩阵限制成正比于单位矩阵，$\Sigma=\sigma^2I$，被称为各向同性isotropic的协方差。尽管这样的方法限制了概率分布的自由度的数量，并且使得求协方差矩阵的逆矩阵可以更快地完成，但是这样做也极大地限制了概率密度的形式，限制了它描述模型中有趣的相关性的能力。
	\item 单峰。因此不能够很好地近似多峰分布。
\end{enumerate}
高斯分布一方面相当灵活，因为它有很多参数。另一方面，它又有很大的局限性，因为它不能够近似很多分布。引入潜在变量(latent variable)也被称为隐藏变量(hidden variable)或者未观察变量(unobserved variable)，会让这两个问题都得到解决。
\subsection*{条件高斯分布}
多元高斯分布的一个重要性质是，如果两组变量是联合高斯分布，那么以一组变量为条件，另一组变量同样是高斯分布，类似地，任何一个变量的边缘分布也是高斯分布。

首先考虑条件概率的情形，假设$\boldsymbol{x}$是一个服从高斯分布$\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu},\Sigma)$的D维向量。我们把$\boldsymbol{x}$划分成两个不相交的子集$x_a,x_b$。不失一般性，我们可令$x_a$为$\boldsymbol{x}$的前$M$个分量，令$x_b$为剩余的$D-M$个分量，因此
\begin{flalign}
	\boldsymbol{x}&=
	\begin{pmatrix}
		x_a\\x_b
	\end{pmatrix}\\
	\boldsymbol{\mu}&=
	\begin{pmatrix}
		\mu_a\\\mu_b
	\end{pmatrix}\\
	\Sigma&=
	\begin{pmatrix}
		\Sigma_{aa}&\Sigma_{ab}\\
		\Sigma_{ba}&\Sigma_{bb}
	\end{pmatrix}\\
\end{flalign}
注意，协方差矩阵的对称性$\Sigma^T=\Sigma$表明$\Sigma_{aa}$和$\Sigma_{bb}$也是对称的，而$\Sigma_{ab}^T=\Sigma_{ba}$。
\begin{theorem}{}{}
	已知$x\sim \mathcal{N}(\boldsymbol{\mu},\Sigma),Y=AX+B$有如下结论：
	\begin{enumerate}
		\item $\mathbb{E}[Y]=A\mu+B$
		\item $var[Y]=A\cdot \Sigma \cdot A^T$
	\end{enumerate}
\end{theorem}
$x_a$可以表示为
\begin{equation}
	x_a=\underbrace{(I_m\ o)}_{A}
	\underbrace{
	\begin{pmatrix}
		x_a\\x_b
	\end{pmatrix}}_{x}
\end{equation}
\begin{flalign}
	\mathbb{E}[x_a]&=(I_m\ o)(\mu_a\ \mu_b)^T=\mu_a\\
	var[x_a]&=(I_m\ o)
	\begin{pmatrix}
		\Sigma_{aa} & \Sigma_{ab}\\
		\Sigma_{ba} & \Sigma_{bb}
	\end{pmatrix}
	\begin{pmatrix}
		I_m\\o
	\end{pmatrix}=\Sigma_{aa}\\
	x_a&\sim \mathcal{N}(\mu_a,\Sigma_{aa})
\end{flalign}
同理
\begin{flalign}
	\mathbb{E}[x_b]&=\mu_b\\
	var[x_b]&=\Sigma_{bb}\\
	x_b&\sim \mathcal{N}(\mu_b,\Sigma_{bb})
\end{flalign}
构造
\begin{flalign}
	x_{b|a}&=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
	\label{ta}
	\mu_{b| a}&=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\
	\label{men}
	\Sigma_{bb| a}&=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\end{flalign}
\begin{equation}
	x_{b| a}=\underbrace{(-\Sigma_{ba}\Sigma_{aa}^{-1}\ I_n)}_{A}
	\underbrace{
	\begin{pmatrix}
		x_a \\x_b
	\end{pmatrix}}_{x}
\end{equation}
$x_{b| a}$的分布为
\begin{flalign}
	\mathbb{E}[x_{b| a}]&=(-\Sigma_{ba}\Sigma_{aa}^{-1}\ I_n)| 
	\begin{pmatrix}
		\mu_a\\\mu_b
	\end{pmatrix}=\mu_{b| a}\\
	var[x_{b| a}]&=(-\Sigma_{ba}\Sigma_{aa}^{-1}\ I_n)
	\begin{pmatrix}
		\Sigma_{aa} & \Sigma_{ab}\\
		\Sigma_{ba} & \Sigma_{bb}
	\end{pmatrix}
		\begin{pmatrix}
	-\Sigma_{ba}\Sigma_{aa}^{-1}\ I_n\\
	I
	\end{pmatrix}=\Sigma_{bb| a}\\
	p(x_{b| a})&\sim \mathcal{N}(\mu_{b| a},\Sigma_{bb| a})
\end{flalign}
因为
\begin{equation}
	x_b=x_{b| a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a
\end{equation}
所以
\begin{flalign}
	\mathbb{E}[x_b|x_a]&=\mu_{b| a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
	var[x_b|x_a]&=var[x_{b| a}]=\Sigma_{bb| a}\\
	p(x_b|x_a)&\sim \mathcal{N}(\mu_{b| a},\Sigma_{bb| a})
\end{flalign}
\subsection*{高斯变量的贝叶斯定理}
令边缘概率分布和条件概率分布的形式如下
\begin{flalign}
	p(\boldsymbol{x})&=\mathcal{N}(\boldsymbol{x}|\mu,\Lambda^{-1})\\
	p(\boldsymbol{y}|\boldsymbol{x})&=\mathcal{N}(\boldsymbol{y}|\boldsymbol{Ax}+b,\boldsymbol{L}^{-1})
\end{flalign}
其中，$\mu,A$和$b$是控制均值的参数，$\Lambda$和$L$是精度矩阵。如果$x$的维度为M，$y$的维度为D，那么矩阵$A$的大小为$D\times M$。
\begin{enumerate}
	\item 求$p(y)$。
	
	\begin{flalign}
		y&=Ax+b+\epsilon,\quad \epsilon\sim \mathcal{N}(0,L^{-1})\\
		\mathbb{E}[y]&=\mathbb{E}(Ax+b)+\mathbb{E}(\epsilon)=A\mu+b\\
		var[y]&=var[Ax+b+\epsilon]=A\Lambda^{-1}A^T+L^{-1}\\
		\label{Ga}
		y&\sim\mathcal{N}(A\mu+b,A\Lambda^{-1}A^T+L^{-1})
	\end{flalign}
	\item 求$p(x|y)$。
	
	定义
	\begin{equation}
		z=\begin{pmatrix}x\\y\end{pmatrix}
	\end{equation}
	\begin{flalign}
		z\sim \mathcal{N}\left(
		\begin{bmatrix}
			\mu\\A\mu+b
		\end{bmatrix},
		\begin{bmatrix}
			\Lambda^{-1}&\triangle\\
			\triangle^T&A\Lambda^{-1}A^T+L^{-1}
		\end{bmatrix}
		\right)
	\end{flalign}
	其中
	\begin{equation}
	\begin{aligned}
		\triangle&=Cov(x,y)\\
		&=E[(x-E(x))\cdot(y-E(y))^T] \\
		&=E[(x-\mu)(y-A\mu-b)^T]\\
		&=E[(x-\mu)(Ax+b+\epsilon-A\mu-b)^T]\\
		&=E[(x-\mu)(Ax-A\mu)^T+(x-\mu)\epsilon^T]\\
		&=E[(x-\mu)(Ax-A\mu)^T]+E[(x-\mu)\epsilon^T]\\
		&=E[(x-\mu)(Ax-A\mu)^T]\\
		&=E[(x-\mu)(x-\mu)^T]\cdot A^T\\
		&=var[x]\cdot A^T\\
		&=\Lambda^{-1}A^T
	\end{aligned}
	\end{equation}
	所以
	\begin{flalign}
		\mathbb{E}[x|y]&=(\Lambda+A^TLA)^{-1}\{A^TL(y-b)+\Lambda\mu \}\\
		cov[x|y]&=(\Lambda+A^TLA)^{-1}
	\end{flalign}
\end{enumerate}

\subsection*{高斯分布的最大似然估计}
给定一个数据集$X=(x_1,x_2,\dots,x_N)^T$，其中观测$\{x_n\}$假定是独立地从多元高斯分布中抽取的。我们可以使用最大似然法估计分布的参数。
\begin{flalign}
	\mu_{ML}&=\frac{1}{N}\sum_{n=1}^{N}x_n\\
	\Sigma_{ML}&=\frac{1}{N}\sum_{n=1}^{N}(x_n-\mu_{ML})(x_n-\mu_{ML})^T\\
	\mathbb{E}[\mu_{ML}]&=\mu\\
	\mathbb{E}[\Sigma_{ML}]&=\frac{N-1}{N}\Sigma
\end{flalign}
\subsection*{顺序估计}
顺序的方法允许每次处理一个数据点，然后丢弃这个点。这对于在线应用很重要。并且当数据集相当大以至于一次处理所有数据点不可行的情况下，顺序方法也很重要。对于高斯分布
\begin{equation}
	\begin{aligned}
		\mu_{ML}^{(N)}&=\frac{1}{N}\sum_{n=1}^{N}x_n\\
		&=\frac{1}{N}x_N+\frac{1}{N}\sum_{n=1}^{N-1}x_n\\
		&=\mu_{ML}^{(N-1)}+\frac{1}{N}(x_N-\mu_{ML}^{(N-1)})
	\end{aligned}
\end{equation}
随着N的增加，后续数据点的贡献也会逐渐变小。我们不能总是能够使用这种方法推导出一个顺序的算法。因此我们要寻找一个更加通用的顺序学习的方法，这就引出了Robbins-Monro算法。考虑一对随机变量$\theta$和$z$，它们由一个联合概率分布$p(z,\theta)$所控制。已知$\theta$的条件下，z的条件期望定义了一个确定的函数$f(\theta)$，形式如下
\begin{equation}
	f(\theta)\equiv \mathbb{E}[z|\theta]=\int zp(z|\theta)dz
\end{equation}
通过这种方式定义的函数被称为回归函数(regression function)。我们的目标是寻找根$\theta^*$使得$f(\theta^*)=0$。如果我们有观测$z$和$\theta$的一个大数据集，那么我们可以直接对回归函数建模，得到根的一个估计。但是假设我们每次观测到一个$z$的值，我们想找到一个对应的顺序估计方法来找到$\theta^*$。下面的解决这种问题的通用步骤由Robbins and Monro给出。我们假设$z$的条件方差是有穷的，因此 
\begin{equation}
	\mathbb{E}[(z-f)^2|\theta]<\infty
\end{equation}
不失一般性，假设当$\theta>\theta^*$时$f(\theta)>0$，当$\theta <\theta^*$时$f(\theta^*)<0$。下式定义了一个根$\theta^*$的顺序估计的序列
\begin{equation}
	\theta^{(N)}=\theta^{(N-1)}-\alpha_{N-1}z(\theta^{(N-1)})
\end{equation}
$z(\theta^{(N)})$是当$\theta$的取值为$\theta^{(N)}$时$z$的观测值。系数$\{\alpha_{N}\}$表示一个满足下列条件的正数序列
\begin{flalign}
	\lim\limits_{N\to \infty}\alpha_{N}&=0\\
	\sum_{N=1}^{\infty}\alpha_{N}&=\infty\\
	\sum_{N=1}^{\infty}\alpha_{N}^2&<\infty
\end{flalign}
可以证明由公式给出的顺序估计确实以概率1收敛于根。第一个条件确保了后续的修正幅度会逐渐变小，从而这个过程可以收敛于一个极限值。第二个条件用来确保算法不会收敛不到根的值。第三个条件保证了累计的噪声具有一个有限的方差，因此不会导致收敛失败。
\subsection*{高斯分布的贝叶斯推断}
现在我们通过引入高斯分布中的参数的先验分布，介绍一种贝叶斯的方法。
\begin{enumerate}
	\item $\sigma^2$已知，估计$\mu$。
	\item $\mu$已知，估计$\sigma^2$。
	\item $\mu$和$\sigma_{2}$都未知。
\end{enumerate}
\subsection*{学生t分布}
\subsection*{周期变量}
\subsection*{混合高斯模型}
虽然高斯分布有一些重要的分析性质，但是当它遇到实际数据集时，也会有巨大的局限性。通过将更基本的概率分布进行线性组合的这样的叠加方法，可以被形式化为概率模型，被称为混合模型(mixture distributions)。通过使用足够多的高斯分布，并且调节它们的均值和方差以及线性组合的系数，几乎所有的连续概率密度都能够以任意的精度近似。考虑K个高斯概率密度的叠加，形式为
\begin{equation}
	p(x)=\sum_{k=1}^{K}\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)
\end{equation}
这被称为混合高斯(mixture of Gaussians)。其中
\begin{equation}
	\sum_{k=1}^{K}\pi_k=1,\ 0\leqslant \pi_k \leqslant 1
\end{equation}