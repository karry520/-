\section{二元分布}
考虑一个二元随机变量$x\in \{0,1\}$。$x=1$的概率被记作参数$\mu$，因此
\begin{equation}
	p(x=1|\mu)=\mu
\end{equation}
其中$0\leqslant \mu \leqslant 1$。$x$的概率分布因此可以写成
\begin{equation}
	\mathrm{Bern}(x|\mu)=\mu^x(1-\mu)^{1-x}
\end{equation}
这被叫做伯努利分布(Bernoulli distribution)。均值和方差为
\begin{flalign}
	\mathbb{E}[x]&=\mu\\
	var[x]&=\mu (1-\mu)
\end{flalign}
用最大似然估计方法求得
\begin{equation}
	\mu_{ML}=\frac{1}{N}\sum_{n=1}^{N}x_n=\frac{m}{N}
\end{equation}
$m$为数据集里$x=1$(正面朝上)的观测数量。

我们也可以求解给定数据集规模N的条件下，$x=1$的观测出现的数量$m$的概率分布。这被称谓二项分布(binomial distribution)。
\begin{equation}
	\mathrm{Bin}(m|N,\mu)=
	\begin{pmatrix}
		N\\m
	\end{pmatrix}
	\mu^m(1-\mu)^{N-m}
\end{equation}
其中
\begin{equation}
	\begin{pmatrix}
		N\\m
	\end{pmatrix}
	\equiv \frac{N!}{(N-m)!m!}
\end{equation}
均值和方差为
\begin{flalign}
	\mathbb{E}[m]&\equiv \sum_{m=0}^{N}m\mathrm{Bin}(m|N,\mu)=N\mu \\
	\mathrm{var}[m]&\equiv \sum_{m=0}^{N}(m-\mathbb{E}[m])^2\mathrm{Bin}(m|N,\mu)=N\mu(1-\mu)
\end{flalign}
\subsection*{Beta分布}
我们已经看到伯努利分布的参数$\mu$的最大似然解。对于小规模的数据集会给出严重的过拟合结果。为了用贝叶斯的观点看待这个问题，我们需要引入一个关于$\mu$的先验分布$p(\mu)$。

在贝叶斯统计中，如果后验分布与先验分布属于同类，则先验分布与后验分布被称为\textbf{共轭分布}，而先验分布被称为似然函数的共轭先验。具体地说，就是给定贝叶斯公式，假定似然函数$p(x|\theta)$是已知的，问题就是选取什么样的先验分布$p(\theta)$会让后验分布与先验分布具有相同的数学形式。共轭先验的好处主要在于代数上的方便性，可以直接给出后验分布的封闭形式，否则的话只能数值计算。共轭先验也有助于获得关于似然函数如何更新先验分布的直观印象。所有指数家族的分布都有共轭先验。 

因此，我们把先验分布选择为Beta分布，定义为
\begin{equation}
	\mathrm{Beta}(\mu|a,b)=\frac{\Gamma (a+b)}{\Gamma(a)+\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}
\end{equation}
其中，$\Gamma(x)$定义为
\begin{equation}
	\Gamma(x)\equiv \int_{0}^{\infty}u^{x-1}e^{-u}du
\end{equation}
有以下性质 
\begin{flalign}
	\Gamma(x+1)&=x\Gamma(x) \\
	\Gamma(x+1)&=x!
\end{flalign}
Beta分布的均值和方差为
\begin{flalign}
	\mathbb{E}[\mu]&=\frac{a}{a+b}\\
	\mathrm{var}[\mu]&=\frac{ab}{(a+b)^2(a+b+1)}
\end{flalign}
参数a和b被称为超参数(hyperparameter)，因为它们控制了参数$\mu$的概率分布。

$\mu$的后验概率分布现在可以这样得到：把Beta先验与二项似然函数相乘，然后归一化。我们看到后验概率分布的形式为
\begin{equation}
	p(\mu|m,l,a,b) =\frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)} \mu^{m+a+1}(1-\mu)^{l+b+1}
\end{equation}
其中$l=N-m$，即对应于硬币“反面朝上”的样本数量。我们看到，如果一个数据集里有$m$次观测为$x=1$，有$l$次观测为$x=0$，那么从先验概率到后验概率，$a$的值变大了$m,b$的值变大了$l$。这让我们可以简单地把先验概率中的超参数$a$和$b$分别看成$x=1$和$x=0$的有效观测数。

另外，如果接下来观测到更多的数据，那么后验概率分布可以扮演先验概率的角色。学习过程中的顺序(sequential)方法可以自然而然地得出。它与先验和似然函数的选择无关，只取决于数据独立同分布的假设。

给定数据集D的情况下，x的预测分布为
\begin{equation}
	p(x=1|D)=\int_{0}^{1}p(x=1|\mu)p(\mu|D)d\mu=\int_{0}^{1}\mu p(\mu|D)d\mu =\mathbb{E}[\mu|D]
\end{equation}