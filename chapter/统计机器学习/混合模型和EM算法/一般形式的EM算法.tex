\section{一般形式的EM算法}
EM算法是一种迭代算法，1977年由Dempster等人总结提出的，用于含有隐变量(hidden variable)的概率模型参数的极大似然估计，或极大后验概率估计。EM算法的每次迭代由两步组成：E步，求期望(expectation)；M步，求极大(maximization)。所以这一算法称为期望极大算法(expection maximization)，简称EM算法。

概率模型有时既含有观测变量(observable variable)，又含有隐变量或潜在变量(latent variable)。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。仅讨论极大似然估计，极大后验概率估计与其类似。

\subsection*{EM算法的导出}
为什么EM算法能近似实验对观测数据的极大似然估计呢？下面通过近似求解观测数据的对数似然函数的极大化问题来导出EM算法。

我们面对一个含有隐变量的概率模型，目标是极大化观测数据(不完全数据)$Y$关于参数$\theta$的对数似然函数，即极大化
\begin{equation}
\label{极大}
	\begin{aligned}
		L(\theta)&=log P(Y|\theta) =log \sum_{Z}P(Y,Z|\theta) \\
		&=log\left(\sum_{Z}P(Y|Z,\theta)P(Z|\theta)\right)
	\end{aligned}
\end{equation}
注意到这一极大化的主要困难是式$\ref{极大}$中有未观测数据并有包含和(或积分)的对数。

事实上，EM算法是通过迭代逐步极大化$L(\theta)$的。假设在第$i$次迭代后$\theta$的估计值是$\theta^{(i)}$。我们希望新估计值$\theta$能使$L(\theta)$增加，即$L(\theta)>L(\theta^{(i)})$，并逐步达到极大值。为此，考虑两者的差：
\begin{equation}
	L(\theta)-L(\theta^{(i)})=log \left(\sum_{Z}P(Y|Z,\theta)P(Z|\theta) \right)-log P(Y|\theta^{(i)})
\end{equation}
利用Jensen不等式\footnote{这里用到的是$log\sum_{j}\lambda_jy_j\geqslant \sum_{j}\lambda_jlogy_j$}(Jensen inequality)得到其下界：
\begin{equation}
	\begin{aligned}
		L(\theta)-L(\theta^{(i)})&=log \left(\sum_{Z}{\color{red}{P(Y|Z,\theta^{(i)})}} \frac{P(Y|Z,\theta)P(Z|\theta)}{{\color{red}{P(Y|Z,\theta^{(i)})}}}  \right)-log P(Y|\theta^{(i)})\\
		&\geqslant \sum_{Z} P(Y|Z,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z,\theta^{(i)})}-log P(Y|\theta^{(i)})\\
		&=\sum_{Z}P(Y|Z,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z,\theta^{(i)})P(Y|\theta^{(i)})}
	\end{aligned}
\end{equation}
令
\begin{equation}
	B(\theta,\theta^{(i)})=L(\theta^{(i)})+\sum_{Z}P(Y|Z,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z,\theta^{(i)})P(Y|\theta^{(i)})}
\end{equation}
则
\begin{equation}
	L(\theta)\geqslant B(\theta,\theta^{(i)})
\end{equation}
即函数$B(\theta,\theta^{(i)})$是$L(\theta)$的一个下界，因此\textbf{任何可以使$B(\theta,\theta^{(i)})$增大的$\theta$，也可以使$L(\theta)$增大}。为了使$L(\theta)$尽可能大的增大，选择$\theta^{(i+1)}$使$B(\theta,\theta^{(i)})$达到极大，即
\begin{equation}
	\theta^{(i+1)}=arg \mathop
	{max}\limits_{\theta}B(\theta,\theta^{(i)})
\end{equation}
现在求$\theta^{(i+1)}$的表达式。省去对$\theta$的极大化而言是常数的项，有
\begin{equation}
\label{目标}
	\begin{aligned}
		\theta^{(i+1)}&=arg \mathop
		{max}\limits_{\theta}\left( L(\theta^{(i)})+\sum_{Z}P(Y|Z,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z,\theta^{(i)})P(Y|\theta^{(i)})} \right)\\
		&=arg \mathop
		{max}\limits_{\theta}\left( \sum_{Z}P(Y|Z,\theta^{(i)})log( P(Y|Z,\theta)P(Z|\theta)) \right)\\
		&=arg \mathop
		{max}\limits_{\theta}\left( {\color{red}{\sum_{Z}P(Y|Z,\theta^{(i)})log( P(Y,Z|\theta))}} \right)\\
		&=arg \mathop
		{max}\limits_{\theta}\underline{Q(\theta,\theta^{(i)})}
	\end{aligned}
\end{equation}
式$\ref{目标}$等价于EM算法的一次迭代，即求$Q$函数及其极大化。\textbf{EM算法是通过不断求解下界的极大化逼近求解对数似然函数极大化的算法}。
\begin{definition}{Q函数}{Q函数}
	完全数据的对数似然函数$logP(Y,Z|\theta)$关于在给定观测数据$Y$和当前参数$\theta^{(i)}$下对未观测数据$Z$的条件概率分布$P(Z|Y,\theta^{(i)})$的期望称为Q函数。即
	\begin{equation}
		Q(\theta,\theta^{(i)})=E_Z[log P(Y,Z|\theta)|Y,\theta^{(i)}]
	\end{equation}
\end{definition}
\subsection*{EM算法}
输入：观测变量数据$Y$，隐变量数据$Z$，联合分布$P(Y,Z|\theta)$，条件分布$P(Z|Y,\theta)$；

输出：模型参数$\theta$

\begin{enumerate}
	\item 选择参数的初值$\theta^{(0)}$，开始迭代；
	
	\item E步：记$\theta^{(i)}$为第i次迭代参数$\theta$的估计值，在第i+1次迭代的E步，计算
	\begin{equation}
		\begin{aligned}
			Q(\theta,\theta^{(i)}) &= E_Z[log P(Y,Z|\theta) |Y,\theta^{(i)}] \\
			&= \sum_Zlog P(Y,Z|\theta)P(Z|Y,\theta^{(i)})
		\end{aligned}	
	\end{equation}
	这里，$P(Z|Y,\theta^{(i)})$是在给定观测数据Y和当前的参数估计$\theta^{(i)}$下隐变量数据Z的条件概率分布；
	
	\item M步：求使$Q(\theta,\theta^{(i)})$极大化的$\theta$，确定第i+1次迭代的参数的估计值$\theta^{(i+1)}$
	\begin{equation}
		\theta^{(i+1)} = arg \mathop{max}\limits_\theta Q(\theta, \theta^{(i)})
	\end{equation}
	
	\item 重复第(2)、(3)步，直到收敛。
\end{enumerate}
\subsection*{EM算法的收敛性}
EM算法提供一种近似计算含有隐变量概率模型的极大似然估计的方法。EM算法的最大优点是简单性和普适性。我们自然地要问：EM算法得到的估计序列是否收敛？如果收敛，是否收敛到全局最大值或局部极大值？
\begin{theorem}{}{}
	设$P(Y|\theta)$为观测数据的似然函数，$\theta^{(i)}(i=1,2,\dots)$为EM算法得到的参数估计序列，$P(Y|\theta^{(i)})(i=1,2,\dots)$为对应的似然函数序列，则$P(Y|\theta^{(i)})$是单调递增的，即
	\begin{equation}
		P(Y|\theta^{(i+1)})\geqslant P(Y|\theta^{(i)})
	\end{equation}
	设$L(\theta)=logP(Y|\theta)$为观测数据的对数似然函数，$L(\theta^{(i)})(i=1,2,\dots)$为对应的对数似然函数序列。
	\begin{enumerate}
		\item 如果$P(Y|\theta)$有上界，则$L(\theta^{(i)})=logP(Y|\theta^{(i)})$收敛到某一值$L^*$
		\item 在函数$Q(\theta,\theta^{'})$与$L(\theta)$满足一定条件下，由EM算法得到的参数估计序列$\theta^{(i)}$的收敛值$\theta^*$是$L(\theta)$的稳定点
	\end{enumerate}
\end{theorem}
