\section{核函数}
SVM处理线性可分的情况，而对于非线性的情况，SVM的处理方法是选择一个核函数$K<\cdot,\cdot>$，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。

此外，用对偶形式表示学习器的优势在于该表示中可调参数的个数不依赖输入属性的个数，通过使用恰当的核函数来替代内积，可以隐式得将非线性的训练数据映射到高维空间，而不增加可调参数的个数。

在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维牲空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。

而在我们遇到核函数之前，如果用原始的方法，那么在用线性学习器学习一个非线性关系，需要选择一个非线性特征集，并且将数据写成新的表达形式，这等价于应用一个固定的非线性映射，将数据映射到特征空间，在特征空间中使用线性学习器，因此考虑的假设集是这种类型的函数：
\begin{equation}
	f(x) = \sum_{i = 1}^Nw_i\phi_i(x) + b
\end{equation}
这里$\phi:X\to F$是从输入空间到某个特征空间的映射，这意味着建立非线性学习器分为两步：
\begin{enumerate}
	\item 首先使用一个非线性映射将数据变换到一个特征空间F
	\item 然后在特征空间使用线性学习器分类 
\end{enumerate}
而由于对偶形式就是线性学习器的一个重要性质，这意味着假设可以表达为训练点的线性组合，因此决策规则可以用测试点和训练点的内积来表示：
\begin{equation}
	f(x) = \sum_{i = 1}^la_iy_i<\phi(x_i),\phi(x)> + b	
\end{equation}
如果有一种方式可以在特征空间中直接计算内积$<\phi(x_i),\phi(x)>$，就像在原始输入点的函数中一样，就有可能将两个步骤融合到一起建立一个非线性的学习器，这样直接计算法的方法称为\textbf{核函数方法}
\begin{definition}{Kernel}{}
	核是一个函数K，对所有$x,z\in X$，满足$K(x,z) = <\phi(x),\phi(z)>$，这里$\phi$是从X到内积特征空间F的映射。
\end{definition}
下面举一个核函数把低维空间映射到高维空间的例子：

我们考虑核函数$K(v_1,v_2) = <v_i,v_2>^2$，即“内积平方”，这里$v_1 = (x_1,y_1),v_2 = (x_2,y_2)$是二维空间中的两个点。
这个核函数对应着一个二维空间到三维空间的映射，它的表达式是：
$$P(x,y) = (x^2,\sqrt{2}xy, y^2)$$
可以验证，
\begin{equation*}
	\begin{aligned}
		<P(v_1), p(v_2)> &= <(x_1^2,\sqrt{2}x_1y_1, y_1^2),(x_2^2,\sqrt{2}x_2y_2, y_2^2)> \\
		&= x_1^2x_2^2 + 2x_1x_2y_1y_2 + y_1^2y_2^2 \\
		&= (x_1x_2 + y_1y_2)^2 \\
		&= <v_1,v_2>^2 \\
		&= K(v_1,v_2)
	\end{aligned}
\end{equation*}
上面的例子所说，核函数的作用就是隐含着一个从低维空间到高维空间的映射，而这个映射可以把低维空间中线性不可分的两类点变成线性可分的。

\textbf{核函数的本质}
\begin{enumerate}
	\item 实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把特征映射到高维空间中去
	\item 但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小会高到可怕的。那咋办呢？
	\item 此时，核函数就隆重登场了，核函数的价值在于它虽然也是讲特征进行从低维到高维的转换，但核函数绝就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就避免了直接在高维空间中的复杂计算。
\end{enumerate}

几个核函数：
\begin{itemize}
	\item 多项式核$K(x_1,x_2) = (<x_1, x_2> + R)^d $
	\item 高斯核$K(x_1,x_2) = exp(-\lVert x_1 - x_2 \rVert ^2 / 2\sigma ^2)$
	\item 线性核$K(x_1,x_2) = <x_1, x_2> $
\end{itemize}

