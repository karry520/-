\section{生成模型与判别模型}
监督学习方法又可以分为生成方法(generative approach)和判别方法(discriminative approach)。所学到的模型分别称为生成模型和判别模型。

\textbf{生成方法}由数据学习联合分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型，即生成模型：
\begin{equation}
	P(Y|X) = \frac{P(X,Y)}{P(X)}
\end{equation}
这样的方法之所以称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。

\textbf{判别方法}由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型，即判模型。判别方法关心的是对给定的输入X，应该预测什么样的输出Y。

生成方法的特点：生成方法可以还原出联合概率分布P(X|Y)，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。

判别方法的特点：判别方法直接学习的是条件概率P(Y|X)或决策函数$f(X)$，直接面对预测，往往学习的准确率更高；由于直接学习P(Y|X)或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。

\section{贝叶斯概率}
\subsection*{频率学派与贝叶斯学派的区别}
简单地说，频率学派与贝叶斯学派探讨\textbf{不确定性}这件事的出发点与立足点不同。频率学派从"自然"角度出发，试图直接为"事件"本身建模，即事件A在独立重复试验中发生的频率趋于极限p，那么这个极限就是该事件发生的概率。贝叶斯学派并不从试图刻画"事件"本身，而从"观察者"角度出发。贝叶斯学派并不试图说"事件本身是随机的"，或者"世界的本体带有某种随机性"，而只是从"观察者知识不完备"这一出发点开始，构造一套在贝叶斯概率论的框架下可以对不确定知识做出推断的方法。体现在参数估计中，频率学派认为参数是客观存在，不会改变，虽然未知，但却是固定值；贝叶斯学派则认为参数是随机值，因此参数也可以有分布。
\subsection*{频率学派的参数估计}
极大似然估计(Maximum Likelihood Estimate,MLE)，也叫最大似然估计。若总体$X$属离散型(连续型与此类似)，其分布律$P\{X=x\}=p(x;\theta),\theta\in \Theta$的形式为已知，$\theta$为待估参数，$\Theta$是$\theta$的取值范围，设$X_1,X_2,\dots,X_n$是来自$X$的样本，则$X_1,X_2,\dots,X_n$的联合概率分布为
\begin{equation}
	\prod_{i=1}^{n}p(x;\theta)
\end{equation}
设$x_1,x_2,\dots,x_n$是相应的样本值，则
\begin{equation}
	L(\theta)=L(x_1,x_2,\dots,x_n;\hat{\theta})=\mathop{\mathrm{argmax}}\limits_{\theta\in\Theta}\prod_{i=1}^{n}p(x;\theta)
\end{equation}

然而，在作出合理的推断时，如果我们想要尊重常识，那么使用概率论来表达不确定性还是可选的，而是不可避免的。最大似然估计对应于选择使观察到的数据集出现概率最大的$\boldsymbol{w}$的值。在机器学习的文献中，似然函数的负对数被叫做误差函数。由于负对数是单调递减的函数，最大化似然函数等价于最小化误差函数。
\subsection*{贝叶斯学派的参数估计}
从贝叶斯的观点来看，我们能够使用概率论来描述模型参数(例如$w$)的不确定性，或者模型本身的选择。最大后验估计(Maximum a Posteriori estimation,MAP)，它与极大似然估计最大的区别就是，它考虑了参数本身的分布，也就是先验分布。最大后验估计是根据经验数据获得对难以观察的量的点估计。可以看作规则化的最大似然估计。贝叶斯定理通过将观察到的数据融合，来把先验概率转化为后验概率。在观察到数据之前，我们有一些关于参数$\boldsymbol{w}$的假设，这以先验概率$p(\boldsymbol{w})$的形式给出。观测数据$\mathcal{D}=\{t_1,t_2,\dots,t_N \}$的效果可以通过条件概率$p(\mathcal{D}|\boldsymbol{w})$表达，贝叶斯定理的形式为
\begin{equation}
\label{1.43}
	p(\boldsymbol{w}|\mathcal{D})=\frac{p(\mathcal{D}|\boldsymbol{w})p(\boldsymbol{w})}{p(\mathcal{D})}
\end{equation}
它让我们能够通过后验概率$p(\boldsymbol{w}|\mathcal{D})$。在观测到$\mathcal{D}$之后估计$\boldsymbol{w}$的不确定性。

贝叶斯定理右侧的量可以被看成参数向量$\boldsymbol{w}$的函数，被称为似然函数(likelihood function)。它表达了在不同的参数向量$\boldsymbol{w}$下，观测数据出现的可能性的大小。注意，似然函数不是$\boldsymbol{w}$的概率分布，并且它关于$\boldsymbol{w}$的积分并不(一定)等于1。

其中所有的量都可以看成$\boldsymbol{w}$的函数。公式$\ref{1.43}$的分母是一个归一化常数，确保了左侧的后验概率分布是一个合理的概率函数，积分为1。
\begin{equation}
	\int p(\boldsymbol{w}|\mathcal{D})p(\mathcal{D})\mathrm{d}\boldsymbol{w}=p(\mathcal{D})=\int p(\mathcal{D}|\boldsymbol{w})p(\boldsymbol{w})\mathrm{d}\boldsymbol{w}
\end{equation}

在贝叶斯观点和频率学家观点中，似然函数$p(\mathcal{D}|\boldsymbol{w})$都起着重要的作用。然而，在两种观点中，使用的方式有着本质的不同。在频率学家的观点中，$\boldsymbol{w}$被认为是一个固定的参数，它的值由某种形式的“估计”来确定，这个估计的误差通过考察可能的数据集$\mathcal{D}$的概率分布来得到。相反，从贝叶斯的观点来看，只有一个数据集$\mathcal{D}$(即实际观测到的数据集)，参数的不确定性通过$\boldsymbol{w}$的概率分布来表达。

关于频率学家的观点的贝叶斯的观点的相对优势有很多争论。事实上并没有纯粹的频率学家观点或者贝叶斯的观点。例如，针贝叶斯方法的一种广泛的批评就是先验概率的选择通常是为了计算的方便而不是为了反映出任何先验的知识。某些人甚至把贝叶斯观点中结论对于先验的选择的信赖性的本质看成困难的来源。

虽然贝叶斯的框架起源于18世纪，但是贝叶斯方法的实际应用在很长时间内都被执行完整的贝叶斯步骤的困难性所限制，尤其是需要在整个参数空间求和或者求积分，这在做预测或者比较不同的模型时必须进行。采样方法的发展，以及计算机速度和存储容量的巨大提升，打开了在相当多的问题中使用贝叶斯技术的大门。最近，许多高效的判别式方法被提出来，例如变种贝叶斯和期望传播。这些提供了一种可选的补充的取样方法，让贝叶斯方法能够应用于大规模的应用中。