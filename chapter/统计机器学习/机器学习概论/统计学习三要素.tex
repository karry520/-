\section{统计学习三要素}
统计学习方法都是由模型、策略和算法构成的，即统计学习方法由三要素构成，可以简单地表示为
\begin{equation*}
\text{方法＝模型＋策略＋算法}
\end{equation*}
\subsection*{实现统计学习方法的步骤}
\begin{enumerate}[(1)]
	\item 得到一个有限的训练数据集合
	\item 确定包含所有可能的模型的假设空间，即学习模型的集合
	\item 确定模型选择的准则，即学习的策略
	\item 实现求解最优化模型的算法，即学习的算法 
	\item 通过学习方法选择最优模型
	\item 利用学习的最优模型对新数据进行预测或分析
\end{enumerate}

\subsection*{模型}
统计学习首先要考虑的问题是学习什么样的模型。模型的假设空间(hypothesis space)包含所有可能的条件概率分页或决策函数。例如，假设决策函数是输入变量的线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合。假设空间中的模型一般有无穷多个。

假设空间用$\mathcal{F}$表示，假设空间可以定义为决策函数的集合
\begin{equation}
	\mathcal{F}=\{f|Y=f(X)\}
\end{equation}
其中，$X$和$Y$是定义在输入空间$\mathcal{X}$和$\mathcal{Y}$上的变量。这时$\mathcal{F}$通常是由一个参数向量决定的函数族：
\begin{equation}
	\mathcal{F}=\{f|Y=f_\theta(X),\theta\in \mathbb{R}^n\}
\end{equation}
参数向量$\theta$取值于$n$维欧氏空间$\mathbb{R}^n$，称为参数空间(parameter space)。

假设空间也可以定义为条件概率的集合
\begin{equation}
	\mathcal{F}=\{P|P(Y|X)\}
\end{equation}
这时$\mathcal{F}$通常是由一个参数向量决定的条件概率分布族：
\begin{equation}
\mathcal{F}=\{P|P_\theta(Y|X),\theta\in \mathbb{R}^n\}
\end{equation}
称由决策函数表示的模型为非概率模型，由条件概率表示的模型为概率模型。
\subsection*{策略}
有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。统计学习的目标在于从假设空间中选取最优模型。首先引入损失函数与风险函数的概念。损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。

\textbf{损失函数}

监督学习问题是在假设空间$\mathcal{F}$中选取模型$f$作为决策函数，对于给定的输入$X$，由$f(X)$给出相应的输出$Y$，这个输出的预测值$f(X)$与真实值$Y$可能一致也可能不一致，用一个损失函数(loss function)或代价函数(cost function)来度量预测错误的程度。损失函数是$f(X)$和$Y$的非负实值函数，记作$L(Y,f(X))$。损失函数值越小，模型就越好。由于模型的输入、输出$(X,Y)$是随机变量，遵循联合分布$P(X,Y)$，所以损失函数的期望是
\begin{equation}
	R_{exp}(f)=E_p[L(Y,f(X))]=\int_{\mathcal{X\times Y}}L(y,f(x))P(x,y)\mathrm{d}x\mathrm{d}y
\end{equation}
这是理论上模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数(risk function)或期望损失(expected loss)。

学习的目标就是选择期望风险最小的模型。由于联合分布$P(X,Y)$是未知的，$R_{exp}(f)$不能直接计算。

给定一个训练数据集
\begin{equation}
	T=\{(x_1,y_1),\dots,(x_N,y_N)\}
\end{equation}
模型$f(X)$关于训练数据集的平均损失称为经验风险(empirical risk)或经验损失(empirical loss)，记作$R_{emp}$：
\begin{equation}
	R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))
\end{equation}
期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本集的平均损失。根据大数定律，当样本容量N趋于无穷时，经验风险趋于期望风险。所以一个很自然的想法是用经验风险估计期望风险。但是，由于现实中训练样本数目有限，甚至很小，所以用经验风险估计期望常常并不理想，要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略：经验风险最小化和结构风险最小化.

\textbf{经验风险最小化与结构风险最小化}

在假设空间、损失函数以及训练数据集确定的情况下，经验风险函数式就可以确定。经验风险最小化(empirical risk minimization,ERM)的策略认为，经验风险最小的模型是最优的模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：
\begin{equation}
	\min\limits_{f\in \mathcal{F}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))
\end{equation}
其中，$\mathcal{F}$将假设空间。当样本容量足够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛采用，比如，极大似然估计(maximum likelihood estimation)就是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。

但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“过拟合”现象。

结构风险最小化(structural risk minimization,SRM)是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化(regularization)。结构风险在经验风险上加上表示模型复杂度的正则化项或罚项。在假设空间、损失函数以及训练数据集确定的情况下，结构风险的定义是
\begin{equation}
	R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)
\end{equation}
其中$J(f)$是模型的复杂度，是定义在假设空间$\mathcal{F}$上的泛函。复杂度表示了对复杂模型的惩罚。$\lambda\geq 0$是系数，用以权衡经验风险和模型复杂度，结构风险小需要经验风险与模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。

比如，贝叶斯估计中的最大后验概率估计(maximum posterior probability estimation,MAP)就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。

这样，监督学习问题就变成了经验风险或结构风险函数的最优化问题。这时经验或结构风险函数是最优化的目标函数。
\subsection*{算法}
算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。

这时，统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。如果最优化问题有显式的解析解，这个最优化问题就比较简单。但通常解析解不存在，这就需要用数值计算的方法求解。如何保证找到全局最优解，并使求解的过程非常高效，就成为一个重要问题。统计学习可以利用已有的最优化算法，有时也需要开发独自的算法。

统计学习方法之间的不同，主要来自其模型、策略、算法的不同。确定了模型、策略、算法，统计学习的方法也就确定了，这也就是将其称为统计学习三要素的原因。
