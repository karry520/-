\section{贝叶斯神经网络}
目前为止，我们对于神经网络的讨论集中于使用最大似然方法来确定网络的参数(权值和偏置)。正则化的最大似然方法可以看成MAP(maximum posterior)方法，其中正则化项可以被看成先验参数分布的对数。然而，在贝叶斯方法中，为了进行预测，我们需要对参数的概率分布进行积分或求和。

在多层神经网络的情况下，网络函数对于参数值的高度非线性的性质意味着精确的贝叶斯方法不再可行，事实上，后验概率分布的对数是非凸的，对应于误差函数中的多个局部极小值。

变分推断方法已经被用在了贝叶斯神经网络中，这种方法使用了对后验概率的分解的高斯近似，也使用了一个具有完成协方差矩阵的高斯分布。但是，最完整的贝叶斯方法是基于拉普拉斯的方法，这种方法构成了本节讨论的基础。我们会使用一个以真实后验概率的众数为中心的高斯分布来近似后验概率分布。此外，我们会假设这个高斯分布的协方差很小，从而网络函数关于参数空间的区域中的参数近似是线性关系。在参数空间中，后验概率距离概率为零的状态相当远。使用这两个近似，我们会得到与之前讨论的线性回归和线性分布的模型相类似的模型，从而我们就可以利用之前得到了结果了。这样，我们可以使用模型证据的框架来对参数进行点估计，并且比较不同的模型。
\subsection*{后验参数分布}
考虑从输入向量$\boldsymbol{x}$预测单一连续目标变量$t$的问题。我们假设条件概率分布$p(t|\boldsymbol{x})$是一个高斯分布，均值与$\boldsymbol{x}$有关，由神经网络模型的输出$y(\boldsymbol{x},\boldsymbol{w})$确定，精度$\beta$为
\begin{equation}
	p(t|\boldsymbol{x},\boldsymbol{w},\beta)=\mathcal{N}(t|y(\boldsymbol{x},\boldsymbol{w}),\beta^{-1})
\end{equation}
类似地，我们将权值$\boldsymbol{w}$的先验概率分布选为高斯分布，形式为
\begin{equation}
	p(\boldsymbol{w}|\alpha)=\mathcal{N}(\boldsymbol{w}|0,\alpha^{-1}\boldsymbol{I})
\end{equation}
对于N次独立同分布的观测$\boldsymbol{x}_1,\dots,\boldsymbol{x}_N$，对应的目标值集合$\mathcal{D}=\{t_1,\dots,t_N \}$，似然函数为
\begin{equation}
	p(\mathcal{D}|\boldsymbol{w},\beta)=\prod_{n=1}^{N}\mathcal{N}(t_n|y(\boldsymbol{x}_n,\boldsymbol{w}),\beta^{-1})
\end{equation}
因此最终的后验概率为
\begin{equation}
	p(\boldsymbol{w}|\mathcal{D},\alpha,\beta)\propto p(\boldsymbol{w}|\alpha)p(\mathcal{D}|\boldsymbol{w},\beta)
\end{equation}
由于$y(\boldsymbol{w},\boldsymbol{x})$与$\boldsymbol{w}$的关系是非线性的，因此后验概率不是高斯分布。

使用拉普拉斯近似，我们可以找到对于后验概率分布的一个高斯近似。为了完成这一点，我们必须首先找到后验概率分布的一个(局部)最大值，这必须使用迭代的数值最优化算法才能找到。比较方便的做法是最大化后验概率分布的对数，它可以写成下面的形式
\begin{equation}
	\ln p(\boldsymbol{w}|\mathcal{D})=-\frac{\alpha}{2}\boldsymbol{w}^T\boldsymbol{w}-\frac{\beta}{2}\sum_{n=1}^{N}\{y(\boldsymbol{x}_n,\boldsymbol{w})-t_n \}^2+\text{常数}
\end{equation}
这对应于一个正则化的平方和误差函数。假设$\alpha$和$\beta$都是定值，那么我们可以通过标准的非线性最优化算法，使用误差反向传播计算所需的导数，找到后验概率的最大值。我们将最大值的位置记作$\boldsymbol{w}_{MAP}$

找到了众数，我们就可以通过计算后验概率分布的负对数的二阶导数，建立一个局部的高斯近似。负对数后验概率的二阶导数为
\begin{equation}
	\boldsymbol{A}=-\triangledown\triangledown \ln p(\boldsymbol{w}|\mathcal{D},\alpha,\beta)=\alpha\boldsymbol{I}+\beta\boldsymbol{H}
\end{equation}
这里，$\boldsymbol{H}$是一个Hessian矩阵，由平方和误差函数关于$\boldsymbol{w}$的分量组成。这样，后验概率对应的高斯近似形式为
\begin{equation}
\label{5167}
	q(\boldsymbol{w}|\mathcal{D})=\mathcal{N}(\boldsymbol{w}_{MAP},\boldsymbol{A}^{-1})
\end{equation}
类似地，预测分布可以通过将后验概率分布求积分的方式获得
\begin{equation}
\label{5168}
	p(t|\boldsymbol{x},\mathcal{D})=\int p(t|\boldsymbol{x},\boldsymbol{w})q(\boldsymbol{w}|\mathcal{D})d\boldsymbol{w}
\end{equation}
然而，即使对于后验分布的高斯近似，这个积分仍然无法得到解析解，因为网络函数$y(\boldsymbol{x},\boldsymbol{w})$与$\boldsymbol{w}$的关系是非线性的。为了将计算过程进行下去，我们现在假设，与$y(\boldsymbol{x},\boldsymbol{w})$发生变化造成的$\boldsymbol{w}$幅度相比，后验概率分布的方差较小。这使得我们可以在$\boldsymbol{w}_{MAP}$附近对网络函数进行泰勒展开。只保留展开式的现行项，可得
\begin{equation}
	y(\boldsymbol{x},\boldsymbol{w})\simeq y(\boldsymbol{x},\boldsymbol{w}_{MAP})+\boldsymbol{g}^T(\boldsymbol{w}-\boldsymbol{w}_{MAP})
\end{equation}
其中，我们定义了
\begin{equation}
	\boldsymbol{g}=\triangledown_{\boldsymbol{w}}y(\boldsymbol{w},\boldsymbol{x})|_{\boldsymbol{w}=\boldsymbol{w}_{MAP}}
\end{equation}
使用这个近似，我们现在得到了一个线性高斯模型，$p(\boldsymbol{w})$为高斯分布。并且，$p(t|\boldsymbol{w})$也是高斯分布，它的均值是$\boldsymbol{w}$的线性函数，分布的形式为
\begin{equation}
	p(t|\boldsymbol{x},\boldsymbol{w},\beta)\simeq \mathcal{N}(t|y(\boldsymbol{x},\boldsymbol{w}_{MAP})+\boldsymbol{g}^T\underbrace{(\boldsymbol{w}-\boldsymbol{w}_{MAP})}_{\text{只有这里含有$\boldsymbol{w}$}},\beta^{-1} )
\end{equation}
于是，我们我们可以求出边缘分布$p(t)$
\begin{equation}
	p(t|\boldsymbol{x},\mathcal{D},\alpha,\beta)=\mathcal{N}(t|y(\boldsymbol{x},\boldsymbol{w}_{MAP}),\sigma^2(\boldsymbol{x})
\end{equation}
其中，与输入相关的方差为
\begin{equation}
	\sigma^2(\boldsymbol{x})=\beta^{-1}+\boldsymbol{g}^T\boldsymbol{A}^{-1}\boldsymbol{g}
\end{equation}
我们看到预测分布$p(t|\boldsymbol{x},\mathcal{D})$是一个高斯分布，它的均值由网络函数$y(\boldsymbol{x},\boldsymbol{w}_{MAP})$给出，参数设置为了MAP值。方差由两项组成。第一项来自目标变量的固有噪声，第二项是一个与$\boldsymbol{x}$相关的项，表示由于模型参数$\boldsymbol{w}$的不确定性造成的内插的不确定性。
\subsection*{超参数最优化}
目前为止，我们已经假定了超参数$\alpha$和$\beta$是固定的、已知的。我们可以使用模型证据框架，结合使用拉普拉斯近似得到的后验概率的高斯近似，得到确定这些超参数的值的步骤。

超参数的边缘似然函数，或者模型证据，可以通过对网络权值进行积分的方法得到，即
\begin{equation}
	p(D|\alpha,\beta)=\int p(D|\boldsymbol{w},\beta)p(\boldsymbol{w}|\alpha)\mathrm{d}\boldsymbol{w}
\end{equation}
通过使用拉普拉斯近似的结果$\ref{4135}$，这个积分很容易计算，取对数，可得
\begin{equation}
\label{5175}
	\ln p(D|\alpha,\beta)\simeq -E(\boldsymbol{w}_{MAP})-\frac{1}{2}\ln |\boldsymbol{A}|+\frac{W}{2}\ln \alpha +\frac{N}{2}\ln \beta -\frac{N}{2}\ln (2\pi)
\end{equation}
其中$W$是$\boldsymbol{w}$中参数的总数。正则化误差函数的定义为
\begin{equation}
	E(\boldsymbol{w}_{MAP})=\frac{\beta}{2}\sum_{n=1}^{N}\{y(\boldsymbol{x}_n,\boldsymbol{w}_{MAP})-t_n \}^2 +\frac{\alpha}{2}\boldsymbol{w}_{MAP}^T\boldsymbol{w}_{MAP}
\end{equation}
我们看到这与线性回归模型的对应的结果的函数形式相同。

在模型证据框架中，我们通过最大化$\ln p(D|\alpha,\beta)$对$\alpha$和$\beta$进行点估计。
\begin{enumerate}
	\item 关于$\alpha$进行最大化。首先，我们定义特征值方程
	\begin{equation}
		\beta \boldsymbol{Hu}_{i}=\lambda_i\boldsymbol{u}_i
	\end{equation}
	其中$\boldsymbol{H}$是在$\boldsymbol{w}=\boldsymbol{w}_{MAP}$处计算的Hessian矩阵，由平方和误差函数的二阶导数组成。通过类比公式$\ref{392}$，我们有
	\begin{equation}
		\alpha=\frac{\gamma}{\boldsymbol{w}_{MAP}^T\boldsymbol{w}_{MAP} }
	\end{equation}
	其中$\gamma$表示参数的有效数量，定义为
	\begin{equation}
	\label{5178}
		\gamma =\sum_{i=1}^{W}\frac{\lambda_i}{\alpha+\lambda_i}
	\end{equation}
	注意，这个结果与线性回归的情形完全相同。然而，对于非线性神经网络，它忽略了下面的事实：$\alpha$的改变会引起Hessian矩阵$\boldsymbol{H}$的改变，进而改变特征值。于是，我们隐式地忽略了涉及到$\lambda_i$关于$\alpha$的导数的项。
	
	\item 关于$\beta$最大化模型证据。可以得到下面的重估计公式 
	\begin{equation}
		\frac{1}{\beta}=\frac{1}{N-\gamma}\sum_{n=1}^{N}\{y(\boldsymbol{x}_n,\boldsymbol{w}_{MAP}-t_n) \}^2
	\end{equation}
	与线性模型一样，我们需要交替地进行超参数$\alpha$和$\beta$的重新估计以及后验概率分布的更新。然而，对于神经网络来说，由于后验概率分布的多峰性质，情况更复杂。
	
	为了比较不同的模型，例如具有不同隐含单元数量的神经网络，我们需要计算模型证据$p(D)$。将使用迭代最优化过程得到的超参数$\alpha$和$\beta$代入公式$\ref{5175}$，我们可以得到模型证据的近似。
\end{enumerate}
\subsection*{用于分类的贝叶斯神经网络}
目前，我们已经使用了拉普拉斯近似，推导出了神经网络回归模型的贝叶斯方法。我们现在要讨论的是，当应用于分类问题时，这个框架应该如何修改。这里，我们要考虑的网络有一个logistic sigmoid输出，对应于一个二分类问题。将网络扩展到多类softmax输出是很直接的。

模型的对数似然函数为
\begin{equation}
	\ln p(D|\boldsymbol{w})=\sum_{n=1}^{N}\{t_n\ln y_n +(1+t_n)\ln (1-y_n) \}
\end{equation}
其中$t_n\in \{0,1 \}$是目标值，且$y_n\equiv y(\boldsymbol{x}_n,\boldsymbol{w})$。注意，这里没有超参数$\beta$，因为我们假定数据点被正确标记了。

将拉普拉斯框架用在这个模型中的第一个阶段是初始化超参数$\alpha$，然后通过最大化对数后验概率分布的方法确定参数向量$\boldsymbol{w}$。这等价于最小化正则化误差函数。
\begin{equation}
	E(\boldsymbol{w})=-\ln p(D|\boldsymbol{w})+\frac{\alpha}{2}\boldsymbol{w}^T\boldsymbol{w}
\end{equation}
最小化的过程可以通过使用误差反向传播方法结合标准的最优化算法得到。

找到权向量的解$\boldsymbol{w}_{MAP}$之后，下一步是计算由负对数似然函数的二阶导数组成的Hessian矩阵$\boldsymbol{H}$。这样，后验概率的高斯近似就由公式$\ref{5167}$给出。

为了最优化超参数$\alpha$，我们再次最大化边缘似然函数。边缘似然函数的形式为
\begin{equation}
	\ln p(D|\alpha)=\simeq -E(\boldsymbol{w}_{MAP})-\frac{1}{2}\ln |\boldsymbol{A}|+\frac{W}{2}\ln \alpha
\end{equation}
其中，正则化的误差函数为
\begin{equation}
	E(\boldsymbol{w}_{MAP})=-\sum_{n=1}^{N}\{t_n\ln y_n+(1-t_n)\ln (1-y_n) \}+\frac{\alpha}{2}\boldsymbol{w}_{MAP}^T\boldsymbol{w}_{MAP}
\end{equation}
其中$y_n\equiv y(\boldsymbol{x}_n,\boldsymbol{w}_{MAP})$。关于$\alpha$，最大化这个模型证据函数，可以得到公式$\ref{5178}$给出的重估计方程。

最后，我们需要找到公式$\ref{5168}$定义的预测分布。与之前一样，由于网络函数的非线性的性质，积分是无法直接计算的。最简单的近似方法是假设后验概率非常窄，因此可以进行下面的近似
\begin{equation}
	p(t|\boldsymbol{x},D)\simeq p(t|\boldsymbol{x},\boldsymbol{w}_{MAP})
\end{equation}
然而，我们可以放宽这个假设，通过考虑后验概率分布的方差。在这种情况下，与回归问题的情形相同，对网络输出进行线性近似是不合适的，因为输出激活函数是logistic sigmoid函数，将输出限制在了区间$(0,1)$。相反，我们对输出激活函数进行线性近似，形式为
\begin{equation}
	a(\boldsymbol{x},\boldsymbol{w})\simeq a_{MAP}(\boldsymbol{x})+\boldsymbol{b}^T(\boldsymbol{w}-\boldsymbol{w_{MAP}})
\end{equation}
其中，$a_{MAP}(\boldsymbol{x})=a(\boldsymbol{x},\boldsymbol{w}_{MAP})$以及向量$\boldsymbol{b}\equiv \triangledown a(\boldsymbol{w}-\boldsymbol{w}_{MAP})$都可以通过反向传播方法求出。

由神经网络的权值的分布引出的输出单元激活的值的分布为
\begin{equation}
\label{5187}
	p(a|\boldsymbol{x},D)=\int \delta (a-a_{MAP}(\boldsymbol{x})-\boldsymbol{b}^T(\boldsymbol{x})(\boldsymbol{w}-\boldsymbol{w}_{MAP}))q(\boldsymbol{w}|D)\mathrm{d}\boldsymbol{w}
\end{equation}
其中$q(\boldsymbol{w}|D)$是公式$\ref{5187}$给出的对后验概率分布的高斯近似。由前面章节，我们看到这个分布是一个高斯分布，均值为$a_{MAP}\equiv a(\boldsymbol{x},\boldsymbol{w}_{MAP})$，方差为 
\begin{equation}
	\sigma_a^2(\boldsymbol{x})=\boldsymbol{b}^T(\boldsymbol{x})\boldsymbol{A}^{-1}\boldsymbol{b}(\boldsymbol{x})
\end{equation}
最后，为了得到预测分布，我们必须对$a$进行积分
\begin{equation}
	p(t=1|\boldsymbol{x},D)=\int \sigma(a)p(a|\boldsymbol{x},D)\mathrm{d}a
\end{equation}
高斯分布与logistic sigmoid函数的卷积是无法计算的。于是我们将公式$\ref{4153}$，可得
\begin{equation}
	p(t=1|\boldsymbol{x},D)=\sigma(k(\sigma_a^2)a_{MAP})
\end{equation}
其中，$k(\cdot)$由公式$\ref{4154}$定义。$\sigma_a^2$和$\boldsymbol{b}$都是$\boldsymbol{x}$的函数。