\section{前馈神经网络}
回归的线性模型和分类的线性模型分别在第5章和第6章讨论过了。它们基于固定非线性基函数$\phi_j(\boldsymbol{x})$的线性组合，形式为
\begin{equation}	y(\boldsymbol{x},\boldsymbol{w})=f\left(\sum_{j=1}^{M}w_j\phi_j(\boldsymbol{x}) \right)
\end{equation}
其中$f(\cdot)$在分类问题中是一个非线性激活函数，在回归问题中为恒等函数。我们的目标是推广这个模型，使得基函数$\phi_j(\boldsymbol{x})$依赖于参数，从而能够让这些参数以及系数$\{w_j\}$能够在训练阶段调节。

这就引出了基本的神经网络，它可以被描述为一系列的函数变换。
深度前馈网络(deep feedforward network)也叫做前馈神经网络(feedforward neural network)或者多层感知机(multilayer perceptron,MLP)，是典型的深度学习模型。与感知器相比，一个重要的区别是神经网络在隐含单元中使用连续的sigmoid非线性函数，而感知器使用阶梯函数这一非线性函数。这意味着神经网络函数关于神经网络是可微的，这个性质在神经网络的训练过程中起着重要的作用。前馈神经网络的目标是近似某个函数$f^*$。前馈网络定义了一个映射$y=f(x;\theta)$，并且学习参数$\theta$的值，使它能够得到最佳的函数近似。

前馈神经网络之所以被称作网络，是因为它们通常用许多不同函数复合在一起来表示。该模型与一个有向无环图相关联，而图描述了函数是如何复合在一起的。例如，我们有三个函数$f^{(1)},f^{(2)},f^{(3)}$连接在一个链上以形成$f^{(3)}(f^{(2)}(f^{(1)}(x)))$。

在神经网络训练过程中，我们让$f(x)$去匹配$f^*(x)$的值。训练数据为我们提供了在不同训练点上取值的、含有噪声的$f^*(x)$近似实例。每个样本$x$都伴随着一个标签$y\approx f^*(x)$。训练样本直接指明了输出层在每一点$x$上必须做什么；它必须产生一个接近$y$的值。但是训练数据并没有直接指明其他层应该怎么做。学习算法必须决定如何使用这些层来诞生想要的输出，但是训练数据并没有说每个单独的层应该做什么。相反，学习算法必须决定如何使用这些层来最好地实现$f^*$的近似。因为训练数据并没有给出这些层中的每一层所需的输出，所以这些层被称为隐藏层(hidden layer)。

