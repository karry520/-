\section{前馈神经网络}
回归的线性模型和分类的线性模型分别在第5章和第6章讨论过了。它们基于固定非线性基函数$\phi_j(\boldsymbol{x})$的线性组合，形式为
\begin{equation}	y(\boldsymbol{x},\boldsymbol{w})=f\left(\sum_{j=1}^{M}w_j\phi_j(\boldsymbol{x}) \right)
\end{equation}
其中$f(\cdot)$在分类问题中是一个非线性激活函数，在回归问题中为恒等函数。我们的目标是推广这个模型，使得基函数$\phi_j(\boldsymbol{x})$依赖于参数，从而能够让这些参数以及系数$\{w_j\}$能够在训练阶段调节。

这就引出了基本的神经网络，它可以被描述为一系列的函数变换。
深度前馈网络(deep feedforward network)也叫做前馈神经网络(feedforward neural network)或者多层感知机(multilayer perceptron,MLP)，是典型的深度学习模型。与感知器相比，一个重要的区别是神经网络在隐含单元中使用连续的sigmoid非线性函数，而感知器使用阶梯函数这一非线性函数。这意味着神经网络函数关于神经网络是可微的，这个性质在神经网络的训练过程中起着重要的作用。前馈神经网络的目标是近似某个函数$f^*$。前馈网络定义了一个映射$y=f(x;\theta)$，并且学习参数$\theta$的值，使它能够得到最佳的函数近似。

前馈神经网络之所以被称作网络，是因为它们通常用许多不同函数复合在一起来表示。该模型与一个有向无环图相关联，而图描述了函数是如何复合在一起的。例如，我们有三个函数$f^{(1)},f^{(2)},f^{(3)}$连接在一个链上以形成$f^{(3)}(f^{(2)}(f^{(1)}(x)))$。

在神经网络训练过程中，我们让$f(x)$去匹配$f^*(x)$的值。训练数据为我们提供了在不同训练点上取值的、含有噪声的$f^*(x)$近似实例。每个样本$x$都伴随着一个标签$y\approx f^*(x)$。训练样本直接指明了输出层在每一点$x$上必须做什么；它必须产生一个接近$y$的值。但是训练数据并没有直接指明其他层应该怎么做。学习算法必须决定如何使用这些层来诞生想要的输出，但是训练数据并没有说每个单独的层应该做什么。相反，学习算法必须决定如何使用这些层来最好地实现$f^*$的近似。因为训练数据并没有给出这些层中的每一层所需的输出，所以这些层被称为隐藏层(hidden layer)。

我们可以将各个阶段结合，得到整体的网络函数。对于sigmoid输出单元激活函数，整体的网络函数为
\begin{equation}
	y_k(\boldsymbol{x},\boldsymbol{w})=\sigma\left( \sum_{j=1}^{M}w_{kj}^{(2)}h\left( \sum_{i=1}^{D}w_{ji}^{(1)}x_i+w_{j0}^{(1)}  \right)+w_{k0}^{(2)} \right)
\end{equation}

如果网络中的所有隐含单元的激活函数都限线性函数，那么对于任何这种网络，我们总可以找到一个等价的无隐含单元的网络。这是由于连续的线性变换的组合本身是一个线性变换。线性单元的网络可以引出主成分分析。但是通常情况下，我们对线性单元的多层神经网络几乎不感兴趣。

神经网络很容易扩展，例如，可以增加额外的处理层；引入跨层(skip-layer)链接，此外 网络可以是稀疏的。

由于在网络图和它的数学函数表达式之间有一个直接的对应关系，因此我们可以通过考虑更复杂的网络图来构造更一般的网络映射。然而，这些网络必须被限制为前馈(feed-forward)结构，换句话说，网络中不能存在有向圈，从而确保了输出是输入的确定函数。
\subsection*{权空间对称性}
前馈神经网络的一个性质是，对于多个不同的权向量$\boldsymbol{w}$的选择，网络可能产生同样的从输入到输出的映射函数。这个性质在我们考虑贝叶斯模型比较的问题时会很有帮助。
