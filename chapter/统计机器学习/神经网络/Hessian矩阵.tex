\section{Hessian矩阵}
反向传播也可以用来计算误差函数的二阶导数，形式为
\begin{equation}
	\frac{\partial^2 E}{\partial w_{ji}\partial w_{lk}}
\end{equation}
有时将所有的权值和偏置参数看成一个向量(记作$\boldsymbol{w}$)的元素$w_i$更方便，此时二阶导数组成了Hessian矩阵$\boldsymbol{H}$的元素$H_{ij}$，其中$i,j\in \{1,\dots,W\}$，且W是权值和偏置的总数。Hessian矩阵在神经网络计算的许多方面都有着重要的作用，包括
\begin{enumerate}
	\item 一些用来训练神经网络的非线性最优化算法是基于误差曲面的二阶性质的，这些性质由Hessian矩阵控制。
	\item 对于训练数据的微小改变，Hessian矩阵构成了快速重新训练前馈网络的算法的基础。
	\item Hessian矩阵的逆矩阵用来鉴别神经网络中最不重要的权值，这是网络“剪枝”算法的一部分。
	\item Hessian矩阵是贝叶斯神经网络的拉普拉斯近似的核心。它的逆矩阵用来确定训练过的神经网络的预测分布，它的特征值确定了超参数的值，它的行列式用来计算模型证据。
\end{enumerate}

计算神经网络的Hessian矩阵有很多近似方法，然而，使用反向传播方法的一个扩展，Hessian矩阵可以精确地被计算出来。
\subsection*{对角近似}
Hessian矩阵的一些应用需要求出Hessian矩阵的逆矩阵，而不是Hessian矩阵本身。因此，我们对Hessian矩阵的对角化近似比较感兴趣。换句话说，就是把非对角线上的元素置为零，因此这样做之后，矩阵的逆矩阵很容易计算。Hessian矩阵的对角线元素可以写成
\begin{equation}
\begin{aligned}
	\frac{\partial^2 E_n}{\partial w_{ji}^2}&=\frac{\partial^2 E_n}{\partial a_j^2}\frac{\partial a_j}{\partial w_{ji}}z_i+0\\
	&=\frac{\partial^2 E_n}{\partial a_j^2}z_i^2
\end{aligned}
\end{equation}
公式右侧的二阶导数可以通过递归地使用微分的链式法则的方式求出。这样，可以得到反向传播方程的形式为
\begin{equation}
\begin{aligned}
	\frac{\partial^2 E_n}{\partial a_j^2}&=\frac{\partial}{\partial a_j}\left[\sum_{k}\frac{\partial E_n}{\partial a_k}\frac{\partial a_k}{\partial a_j} \right]\\
	&=\frac{\partial}{\partial a_j}\left[h^{'}(a_j)\sum_{k}w_{kj}\frac{\partial E_n}{\partial a_k} \right]\\
	&=h^{'}(a_j)^2\sum_{k}\sum_{k^{'}}w_{kj}w_{k^{'}j}\frac{\partial^2 E_n}{\partial a_k\partial a_{k^{'}}}+h^{''}(a_j)\sum_{k}w_{kj}\frac{\partial E_n}{\partial a_k}
\end{aligned}
\end{equation}
如果忽略二阶导数中非对角线无素，那么我们有
\begin{equation}
	\frac{\partial^2 E_n}{\partial a_j^2}=h^{'}(a_j)^2\sum_{k}w_{kj}^2\frac{\partial^2 E_n}{\partial a_k^2}+h^{''}(a_j)\sum_{k}w_{kj}\frac{\partial E_n}{\partial a_k}
\end{equation}
对角近似的主要问题是，在实际应用中Hessian矩阵通常是强烈非对角化的，因此为了计算方便而采取的这些近似手段必须谨慎使用。
\subsection*{外积近似}
当神经网络应用于回归问题时，通常使用平方和误差函数。
我们可以把Hessian矩阵写成下面的形式
\begin{equation}
	\begin{aligned}
		\boldsymbol{H}&=\triangledown\triangledown E=\frac{\partial^2}{\partial y_n^2}\left[\frac{1}{2}\sum_{n=1}^{N}(y_n-t_n)^2 \right]\\
		&=\frac{\partial}{\partial y_n}\left[\triangledown y_n\sum_{n=1}^{N}(y_n-t_n) \right]\\
		&=\sum_{n=1}^{N}\triangledown y_n(\triangledown y_n)^T+\sum_{n=1}^{N}(y_n-t_n)\triangledown\triangledown y_n
	\end{aligned}
\end{equation}
如果网络已经在数据集上训练过，输出$y_n$恰好非常接近$t_n$，那么公式的第二项会很小，可以被忽略。我们就得到了Levenberg-Marquardt近似，或者称为外积近似(outer product approximation)。形式为
\begin{equation}
	\boldsymbol{H}\simeq \sum_{n=1}^{N}\boldsymbol{b}_n\boldsymbol{b}_n^T
\end{equation}
其中$\boldsymbol{b}_n\equiv \triangledown a_n=\triangledown y_n$，因为输出单元的激活函数就是恒等函数。Hessian矩阵近似的计算是很容易的，因为它只涉及到误差函数的一阶导数，这可能通过使用标准的反向传播算法在$O(W)$个步骤内高效地求出。需要强调的是，这种近似只在网络被恰当地训练时才成立，对于一个一般的网络映射，公式右侧的二阶导数项通常不能忽略。

在误差函数为交叉熵误差函数，输出单元激活函数为logistic sigmoid函数的神经网络中，对应的近似为
\begin{equation}
	\boldsymbol{H}\simeq \sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{b}_n\boldsymbol{b}_n^T
\end{equation}
对于输出函数为softmax函数的多类神经网络，可以得到类似的结果。
\subsection*{Hessian矩阵的逆矩阵}
使用外积近似可以提出一个计算Hessian矩阵的逆矩阵的高效方法。首先，我们用矩阵的记号写出外积近似，即
\begin{equation}
\boldsymbol{H}= \sum_{n=1}^{N}\boldsymbol{b}_n\boldsymbol{b}_n^T
\end{equation}
其中，$\boldsymbol{b}_n\equiv \triangledown_{\boldsymbol{w}}a_n$是数据点n产生的输出单元激活对梯度的贡献。

现上推导一个建立Hessian矩阵的顺序步骤，每次处理一个数据点。假设我们已经使用前L个数据点得到了Hessian矩阵的逆矩阵。通过将第L+1个数据点的贡献单独写出来，我们有
\begin{equation}
	\boldsymbol{H}_{L+1}=\boldsymbol{H}_L+\boldsymbol{b}_{L+1}\boldsymbol{b}_{L+1}^T
\end{equation}
为了计算Hessian矩阵的逆矩阵，我们考虑Sherman-Morrison-Woodbury公式
\begin{equation}
\label{booy}
	(A+UV^T)^{-1}=A^{-1}-A^{-1}U(I_k+V^TA^{-1}U)^{-1}V^TA^{-1}
\end{equation}
其中，$A\in \mathbb{R}^{n\times n}$非奇异，即$A^{-1}$存在，$U,V\in \mathbb{R}^{n\times k}$。当$k=1$时的特殊形式
\begin{equation}
\begin{aligned}\label{pa}
		(A+uv^T)^{-1}&=A^{-1}-A^{-1}u(1+v^TA^{-1}u)^{-1}v^TA^{-1}\\
		&=A^{-1}-\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}
\end{aligned}
\end{equation}
SM公式看似复杂，但可以通过求解以下线性方程组来推导出来：
\begin{equation}
\label{1}
	(A+uv^T)x=b
\end{equation}
式$\ref{1}$两边同乘以$A^{-1}$，令$A^{-1}u=z,A^{-1}b=y$，则有
\begin{equation}
\label{2}
	x+zv^Tx=y
\end{equation}
注意到$V^Tx$是标量，令$a=v^Tx$。式$\ref{2}$两边同时乘以$v^T$，得
\begin{equation}
\label{3}
	a+v^Tza=v^Ty
\end{equation}
由于式$\ref{3}$中$v^Tz$和$v^Ty$都是标量，从而由式$\ref{3}$可解得
\begin{equation}
	a=\frac{v^Ty}{1+v^Tz}
\end{equation}
由式$\ref{2}$和$y,z,a$的定义可得
\begin{equation}
\label{4}
	\begin{aligned}
	x&=y-az\\
	&=A^{-1}b-A^{-1}u(1+v^TA^{-1}u)^{-1}v^TA^{-1}b\\
	&=\left[A^{-1}-A^{-1}u(1+v^TA^{-1}u)^{-1}v^TA^{-1} \right]b
	\end{aligned}
\end{equation}
由$\ref{1}$和$\ref{4}$即可得Sherman-Morrison公式，即$\ref{pa}$。
\begin{proof}
	公式$\ref{booy}$两边同乘$(A+UV^T)$
	\begin{equation}
		\begin{aligned}
		I_n&=(A+UV^T)\left[A^{-1}-A^{-1}U(I_k+V^TA^{-1}U)^{-1}V^TA^{-1}\right]\\
		&=I_n+UV^TA^{-1}-U(I_k+V^TA^{-1}U)^{-1}V^TA^{-1}-UV^TA^{-1}U(I_k+V^TA^{-1}U)^{-1}V^TA^{-1}\\
		&=I_n+UV^TA^{-1}-U(I_k+V^TA^{-1}U)(I_k+V^TA^{-1}U)^{-1}V^TA^{-1}\\
		&=I_n
		\end{aligned}
	\end{equation}
\end{proof}
Woodbury恒等式的主要用途是当$n>k$时把一个相对较大的n阶矩阵求逆的问题转化为求一个相对较小的k阶矩阵的逆矩阵。我们可以把公式看成在已知$A^{-1}$的情况下对$A+\triangle A$求逆的工具。其中$\triangle A$是一个低秩扰动。

如果我们令$\boldsymbol{H}_L=A$，且$\boldsymbol{b}_{L+1}=\boldsymbol{v}$，我们有
\begin{equation}
	\boldsymbol{H}_{L+1}^{-1}=\boldsymbol{H}_{L}^{-1}-\frac{\boldsymbol{H}_{L}^{-1}\boldsymbol{b}_{L+1}\boldsymbol{b}_{L+1}^T\boldsymbol{H}_L^{-1} }{1+\boldsymbol{b}_{L+1}^T\boldsymbol{H}_L^{-1}\boldsymbol{b}_{L+1} }
\end{equation}
使用这种方式，数据点可以依次使用，直到$L+1=N$，整个数据集被处理完毕。于是，这个结果表示一个计算Hessian矩阵的逆矩阵的算法，这个算法只需对数据集扫描一次。最开始的矩阵$\boldsymbol{H}_0$被选为$\alpha \boldsymbol{I}$，其中$\alpha$是一个较小的量，从而算法实际找的是$\boldsymbol{H}+\aleph \boldsymbol{I}$的逆矩阵。结果对于$\alpha$的精确值不是特别敏感。
\subsection*{有限差}
与误差函数的一阶导数的形式相同，我们可以使用有限差的方法求二阶导数，精度受数值计算的精度限制。
\subsection*{Hessian矩阵的精确计算}
对于一个任意的前馈拓扑结构的网络，Hessian矩阵也可以精确地计算。计算的方法是使用反向传播算法计算一阶导数的推广，同时也保留了计算一阶导数的方法的许多良好的性质，包括计算效率。这种方法可以应用于任何可微的可以表示成网络输出的函数形式的误差函数，以及任何具有可微的激活函数的神经网络。

这里我们考虑一个具体的情况，即具有两层权值的网络。这种网络中待求的方程很容易推导。我们将使用下标$i$和$i^{'}$表示输入，用下标$j$和$j^{'}$表示隐含单元，用下标$k$和$k^{'}$表示输出。首先我们定义 
\begin{equation}
	\delta_k=\frac{\partial E_n}{\partial a_k},\quad M_{kk^{'}}\equiv \frac{\partial^2 E_n}{\partial a_k \partial a_{k^{'}}}
\end{equation}
其中$E_n$是数据点n对误差函数的贡献。于是，这个网络的Hessian矩阵可以被看成三个独立的模块，即
\begin{enumerate}
	\item 两个权值都在第二层
	\begin{equation}
		\frac{\partial ^2E_n}{\partial w_{kj}^{(2)}\partial w_{k^{'}j^{'}}^{(2)}}=z_jz_{j^{'}}M_{kk^{'}}
	\end{equation}
	\item 两个权值都在第一层
	\begin{equation}
		\begin{aligned}
			\frac{\partial ^2E_n}{\partial w_{ji}^{(1)}\partial w_{j^{'}i^{'}}^{(1)}}&=x_ix_{i^{'}}h^{''}(a_{j^{'}})I_{jj^{'}}\sum_{k}w_{kj^{'}}^{(2)}\delta_k\\
			&\ +x_ix_{i^{'}}h^{'}(a_{j^{'}})\sum_{k}\sum_{k^{'}}w_{k^{'}j^{'}}^{(2)}w_{kj}^{(2)}M_{kk^{'}}
		\end{aligned}
	\end{equation}
	\item 每一层有一个权值
	\begin{equation}
		\frac{\partial ^2E_n}{\partial w_{ji}^{(1)}\partial w_{kj^{'}}^{(2)}}=x_ih^{'}(a_j)\left\{\delta_kI_{j^{'}j}+z^{j^{'}}\sum_{k^{'}}w_{k^{'}j}^{(2)}M_{kk^{'}} \right\}
	\end{equation}
\end{enumerate}
这里$I_{jj^{'}}$是单位矩阵的第$j,j^{'}$个元素。如果权值中的一个或者两个偏置项，那么只需将激活设为1即可得到对应的表达式。很容易将这个结果推广到允许网络包含跨层链接的情形。
\subsection*{Hessian矩阵的快速乘法}
对于Hessian矩阵的许多应用来说，我们感兴趣的不是Hessian本身，而是$\boldsymbol{H}$与某些向量$\boldsymbol{v}$的乘积。我们已经看到Hessian矩阵的计算需要$O(W^2)$次操作，所需的存储空间也是$O(W^2)$。但是，我们想要计算的向量$\boldsymbol{v}^T\boldsymbol{H}$只有W个元素。因此，我们可以不把计算Hessian矩阵当成一个中间的步骤，而是可以尝试寻找一种只需$O(W)$次操作的直接计算$\boldsymbol{v}^T\boldsymbol{H}$的高效方法。

为了完成这一点，我们首先注意到
\begin{equation}
	\boldsymbol{v}^T\boldsymbol{H}=\boldsymbol{v}^T\triangledown(\triangledown E)
\end{equation}
其中$\triangledown$表示权空间的梯度算符。然后，我们可以写下计算$\triangledown E$的标准正向传播和反向传播的方程，然后将公式应用于这些方程，得到一组计算$\boldsymbol{v}^T\boldsymbol{H}$的正向传播和反向传播的方程。这对应于将微分算符$\boldsymbol{v}^T\triangledown$作用于原始的正向传播和反向传播的方程。使用记号$\mathcal{R}\{\cdot \}$表示算符$\boldsymbol{v}^T\triangledown$，我们将遵从这个惯例。与之前一样，我们使用两层网络，以及线性的输出单元和平方和误差函数。我们考虑数据集里的一个模式对于误差函数的贡献。这样，我们所要求解的向量可以通过求出每个模式各自的贡献然后求和的方式得到。对于两层神经网络，正向传播方程为
\begin{flalign}
	a_j&=\sum_iw_{ji}x_i\\
	z_j&=h(a_j)\\
	y_k&=\sum_{j}w_{kj}z_j
\end{flalign}
我们现在使用$\mathcal{R}\{\cdot \}$作用于这些方程上，得到一组正向传播方程，形式为
\begin{flalign}
	\mathcal{R}\{a_j \}&=\sum_iv_{ji}x_i\\
	\mathcal{R}\{z_j \}&=h^{'}(a_j)\mathcal{R}\{a_j \}\\
	\mathcal{R}\{y_k \}&=\sum_{j}w_{kj}\mathcal{R}\{z_j \}+\sum_{j}v_{kj}z_j
\end{flalign}
其中，$v_{ji}$是向量$\boldsymbol{v}$中对应于权值$w_{ji}$的元素。

考虑的平方和误差函数，我们有下面的标准的反向传播表达式
\begin{flalign}
	\delta_k &= y_k - t_k\\
	\delta_j &= h^{'}(a_j)\sum_{k}w_{kj}\delta_k
\end{flalign}
与之前一样，我们将$\mathcal{R}\{\cdot \}$作用于这些方程上，得到一组反向传播方程，形式为
\begin{flalign}
	\mathcal{R}\{\delta_k \}&=\mathcal{R}\{y_k \}\\\mathcal{R}\{\delta_j \}&=h^{''}(a_j)\mathcal{R}\{a_j \}\sum_{k}w_{kj}\delta_k\nonumber \\
	&\ +h^{'}(a_j)\sum_{k}v_{kj}\delta_k+h^{'}(a_j)\sum_{k}w_{kj}\mathcal{R}\{\delta\}
\end{flalign}
然后，我们有误差函数的一阶导数的方程
\begin{flalign}
	\frac{\partial E}{\partial w_{kj}}&=\delta_kz_j\\
	\frac{\partial E}{\partial w_{ji}}&=\delta_jx_j
\end{flalign}
使用$\mathcal{R}\{\cdot \}$作用在这些方程上，我们得到了下面的关于$\boldsymbol{v}^T\boldsymbol{H}$的表达式
\begin{flalign}
\label{rq}
	\mathcal{R}\left\{\frac{\partial E}{\partial w_{kj}} \right\}&=\mathcal{R}\{\delta_k \}z_j+\delta_k\mathcal{R}\{z_j \}\\
\label{ew}
	\mathcal{R}\left\{\frac{\partial E}{\partial w_{ji}} \right\}&=x_i\mathcal{R}\{\delta_j \}
\end{flalign}
算法的执行涉及到将新的变量$\mathcal{R}\{a_j \},\mathcal{R}\{z_j \},\mathcal{R}\{\delta_j \}$引入到隐含单元，将$\mathcal{R}\{\delta_k \},\mathcal{R}\{y_k \}$引入到输出单元。对于每个输入模式，这些量的值可以使用上面的结果求出，$\boldsymbol{v}^T\boldsymbol{H}$的值由公式$\ref{rq}$和公式$\ref{ew}$给出。这种方法的一个好处是，计算$\boldsymbol{v}^T\boldsymbol{H}$的方程与标准的正向传播和反向传播的方程相同，因此将现有的神经网络计算程序扩展到能够计算这个乘积通常很容易。