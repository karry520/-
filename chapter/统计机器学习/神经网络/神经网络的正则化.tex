\section{神经网络的正则化}
神经网络的输入单元和输出单元的数量通常由数据集的维度确定，而隐含单元的数量M是一个自由的参数，可以通过调节来给出最好的预测性能。M控制了网络中参数(权值和偏置)的数量。然后，泛化误差与M的关系不是一个简单的函数关系，因为误差函数中存在局部极小值。有其他的方式控制神经网络的模型复杂度来避免过拟合。一种方法是选择一个相对大的M值，然后通过给误差函数增加一个正则化项，来控制模型的复杂度。最简单的正则化项是二次的，给出了正则化的误差函数，形式为
\begin{equation}
\label{na}
	\tilde{E}(\boldsymbol{w})=E(\boldsymbol{w})+\frac{\lambda}{2}\boldsymbol{w}^T\boldsymbol{w}
\end{equation}
这个正则化项也被称为权值衰减(weight decay)。这样模型复杂度可以通过选择正则化系数$\lambda$来确定。正则化项可以表示为权值$\boldsymbol{w}$上的零均值高斯先验分布的负对数。
\subsection*{相容的高斯先验}
公式$\ref{na}$给出的简单权值衰减的一个局限性是，它与网络映射的确定缩放性质不相容。为了说明这一点，考虑一个多层感知器网络，这个网络有两层权值和线性输出单元，它给出了从输入变量集合$\{x_i\}$到输出变量集合$\{y_k\}$的映射。第一个隐含层的隐含单元的激活的形式为
\begin{equation}
	z_j=h\left(\sum_iw_{ji}x_i+w_{j0} \right)
\end{equation}
输出单元的激活为
\begin{equation}
	y_k=\sum_jw_{kj}z_j+w_{k0}
\end{equation}
假设我们对输入变量进行一个线性变换，形式为
\begin{equation}
	x_i\rightarrow \tilde{x}_i=ax_i+b
\end{equation}
然后我们可以根据这个映射对网络进行调整，使得网络给出的映射不变。调整的方法为，对从输入单元到隐含层单元的权值和偏置也进行一个对应的线性变换，形式为
\begin{flalign}
	w_{ji}\rightarrow \tilde{w}_{ji}&=\frac{1}{a}w_{ji}\\
	w_{j0}\rightarrow \tilde{w}_{j0}&=w_{j0}-\frac{b}{a}\sum_iw_{ji}
\end{flalign}
类似地，网络的输出变量的线性变换
\begin{equation}
	y_k\rightarrow \tilde{y}_k = cy_k + d
\end{equation}
可以通过对第二层的权值和偏置进行线性变换的方式实现。变换的形式为
\begin{flalign}
	w_{kj}\rightarrow \tilde{w}_{kj}&=cw_{kj}\\
	w_{k0}\rightarrow \tilde{w}_{k0}&=cw_{k0}+d
\end{flalign}
如果我们使用原始数据训练一个网络，还使用输入和(或)目标变换进行了上面的线性变换的数据训练一个网络，那么相容性要求这两个网络应该是等价的，差别仅在于上面给出的权值的线性变换。任何正则化项都应该与这个性质相容，否则模型就会倾向于选择某个解，而忽视某个等价的解。显然，简单的权值衰减由于把所有的权值和偏置同等对待，因此不满足这个性质。

于是我们要寻找一个正则化项，它在线性变换下具有不变性。这需要正则化项应该对于权值的重新缩放不变，对于偏置的平移不变，这样的正则化项为
\begin{equation}
\label{zz}
	\frac{\lambda_1}{2}\sum_{w\in \mathcal{W}_1}w^2+\frac{\lambda_2}{2}\sum_{w\in \mathcal{W}_2}w^2
\end{equation}
其中$\mathcal{W}_1$表示第一层的权值集合，$\mathcal{W}_2$表示第二层的权值集合，偏置未出现在求和式中。这个正则化项在权值的变换下不会发生变化，只要正则化参数进行下面的重新放缩即可：$\lambda_1 \rightarrow a^{\frac{1}{2}}\lambda_1,\lambda_2\to c^{-\frac{1}{2}}\lambda_2$

正则化项$\ref{zz}$对应于下面形式的先验概率分布
\begin{equation}
	p(\boldsymbol{w}|\alpha_1,\alpha_2)\propto \exp\left(-\frac{\alpha_1}{2}\sum_{w\in \mathcal{W}_1}w^2-\frac{\alpha_2}{2}\sum_{w\in \mathcal{W}_2}w^2 \right)
\end{equation}
注意，这种形式的先验是反常的(improper)(不能被归一化)，因为偏置参数没有限制。使用反常先验会给正则化系数的选择造成很大的困难，也会给贝叶斯框架下的模型选择造成很大的困难，因为对应的模型证据等于零。因此，通常的做法是单独包含一个有着自己单独的一套超参数的偏置的先验(这就破坏了平移不变性)。

更一般地，我们可以考虑权值被分为任意数量的组$W_k$的情况下的先验，即
\begin{equation}
	p(\boldsymbol{w})\propto \exp \left(-\frac{1}{2}\sum_k \alpha_k\lVert \boldsymbol{w}\rVert_k^2 \right)
\end{equation}
其中
\begin{equation}
	\lVert \boldsymbol{w}\rVert_k^2=\sum_{j\in W_k}w_j^2
\end{equation}
作为这种形式的先验的一个特殊情况，如果我们将每个输入单元关联的权值设为一个分组，并且关于对应的参数$\alpha_k$最优化边缘似然函数，那么我们就得到了自动相关性确定(automatic relevance determination)的方法。
\subsection*{早停止}
另一种控制网络的复杂度的正则化方法是早停止(early stopping)。非线性网络模型的训练对应于误差函数的迭代减小，其中误差函数是关于训练数据集定义的。对于许多用于网络训练的最优化算法，误差函数是一个关于迭代次数的不增函数。然而，在独立数据(通常被称为验证集)上测量的误差，通常首先减小，接下来由于模型开始过拟合而逐渐增大。于是训练过程可以在关于验证集误差最小的点停止，这样可以得到一个有着较好泛化性能的网络。
\subsection*{不变性}
在许多模型识别的应用中，在对于输入变量进行了一个或者多个变换之后，预测不应该发生变化，或者说应用具有不变性(invariant)。如果可以得到足够多的训练模式，那么可调节的模型(例如神经网络)可以学习到不变性，至少可以近似地学习到。这涉及到训练集里包含足够多的表示各种变换的效果的样本。如果训练样本数受限，或者有多个不变性(变换的组合的数量随着变换的数量指数增长)，那么这种方法就很不实用。于是，我们要寻找另外的方法来让可调节的模型能够表述所需的不变性。这此方法大致可以分为四类。
\begin{enumerate}
	\item 通过复制训练模式，同时根据要求的不变性进行变换，对训练集进行扩展。
	\item 为误差函数加上一个正则化项，用来惩罚当输入进行变换时，输出发生的改变。这引出了切线传播方法。
	\item 通过抽取在要求的变换下不发生改变的特征，不变性被整合到预处理过程中。任何后续的使用这些特征作为输入的回归或者分类系统就会具有不变性。
	\item 把不变性的性质整合到神经网络的构建过程中，或者对于相关向量机的方法，整合到核函数中。一种方法是通过使用局部接收场和共享权值。如卷积神经网络。
\end{enumerate}
\subsection*{切线传播}
通过切线传播(tangent propagation)的方法，我们可以使用正则化来让模型对于输入的变换具有不变性。对于一个特定的输入向量$\boldsymbol{x}_n$，考虑变换产生的效果。假设变换是连续的(例如平移或者旋转，而不是镜像翻转)，那么变换的模式会扫过D维输入空间的一个流形$M$。考虑$D=2$的情形，假设变换由单一参数$\xi$控制(例如，$\xi$可能是旋转的一个角度)。那么被$\boldsymbol{x}_n$扫过的子空间$M$是一维的，并且以$\xi$为参数。令这个变换作用于$\boldsymbol{x}_n$上产生的向量为$\boldsymbol{s}(\boldsymbol{x}_n,\xi)$，且$\boldsymbol{s}(\boldsymbol{x},0)=\boldsymbol{x}$。这样曲线$M$的切线就由方向导数$\boldsymbol{\tau}=\frac{\partial \boldsymbol{x}}{\partial \xi}$给出，且点$\boldsymbol{x}_n$处的切线向量为
\begin{equation}
	\boldsymbol{\tau}_n=\frac{\partial \boldsymbol{s}(\boldsymbol{x}_n,\xi)}{\partial \xi}\Bigg|_{\xi=0}
\end{equation}
对于输入向量进行变换之后，网络的输出通常会发生变化。输出$k$关于$\xi$的导数为
\begin{equation}
\label{5126}
	\frac{\partial y_k}{\partial \xi}\Bigg|_{\xi=0}=\sum_{i=1}^{D}\frac{\partial y_k}{\partial x_i}\frac{\partial x_i}{\partial \xi}\Bigg|_{\xi=0}=\sum_{i=1}^{D}J_{ki}\tau_i
\end{equation}
其中$J_{ki}$为Jacobian矩阵$J$的第$(k,i)$个元素，公式$\ref{5126}$给出的结果可以用于修改标准的误差函数，使得在数据点的邻域之内具有不变性。修改的方法为：给原始的误差函数$E$增加一个正则化函数$\Omega$，得到下面形式的误差函数
\begin{equation}
	\tilde{E}=E+\lambda\Omega
\end{equation}
其中$\lambda$是正则化系数，且
\begin{equation}
\label{5128}
	\Omega=\frac{1}{2}\sum_n\sum_k\left(\frac{\partial y_{nk}}{\partial \xi}\Bigg|_{\xi=0} \right)^2=\frac{1}{2}\sum_n\sum_k\left(\sum_{i=1}^{D}J_{nk}\tau_{ni}\right)^2
\end{equation}
当网络映射函数在每个模式向量的邻域内具有变换不变性时，正则化函数等于零。$\lambda$的值确定了训练数据和学习不变性之间的平衡。

如果变换由L个参数控制(例如，对于二维图像的平移变换与面内旋转变换项结合)，那么流形$M$的维度为L，对应的正则化项由形如$\ref{5128}$的项求和得到，每个变换都对应求和式中的一项。如果同时考虑若干个变换，并且让网络映射对于每个变换分别具有不变性，那么对于变换的组合来说就会具有(局部)不变性。

一个相关的技术，被称为切线距离(tangent distance)，可以用来构造基于距离的方法(例如最近邻分类器)的不变性。
\subsection*{用变换后的数据训练}
我们已经看到，让模型对于一组变换具有不变性的一种方法是使用原始输入模式的变换后的模式来扩展训练集。这里，我们会说明，这种方法与切线传播的方法密切相关。

与上一节一样，我们要考虑由单一参数$\xi$控制的变换，且这个变换由函数$\boldsymbol{s}(\boldsymbol{x}_n,\xi)$描述，基中$\boldsymbol{s}(\boldsymbol{x}_n,0)=\boldsymbol{x}$。我们也会考虑平方和误差函数。对于未经过变换的输入，误差函数可以写成(在无限数据集的极限情况下)
\begin{equation}
	E=\frac{1}{2}\iint \{y(\boldsymbol{x})-t \}^2p(t|\boldsymbol{x})p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}\mathrm{d}t
\end{equation}
这里，为了保持记号的简洁，我们考虑一个输出单元的网络。如果我们现在考虑每个数据点的无穷多个副本，每个副本都由一个变换施加了扰动，这个变换的参数为$\xi$，且$\xi$服从概率分布$p(\xi)$，那么在这个扩展的误差函数上定义的误差函数可以写成
\begin{equation}
\label{5130}
	\tilde{E}=\frac{1}{2}\iiint \{y(\boldsymbol{s}(\boldsymbol{x},\xi))-t \}^2p(t|\boldsymbol{x})p(\boldsymbol{x})p(\xi)\mathrm{d}\boldsymbol{x}\mathrm{d}t\mathrm{d}\xi
\end{equation}
我们现在假设分布$p(\xi)$的均值为零，方差很小，即我们只考虑对原始输入向量的小的变换。我们可以对变换函数进行关于$\xi$的展开，可得
\begin{equation}
	\begin{aligned}
		\boldsymbol{s}(\boldsymbol{x},\xi)&=\boldsymbol{s}(\boldsymbol{x},0)+\xi \frac{\partial }{\partial \xi}\boldsymbol{s}(\boldsymbol{x},\xi)\Bigg|_{\xi=0}+\frac{\xi^2}{2}\frac{\partial^2}{\partial \xi^2}\Bigg|_{\xi=0}+O(\xi^3)\\
		&=\boldsymbol{x}+\xi\boldsymbol{\tau}+\frac{1}{2}\xi^2\boldsymbol{\tau}^{'}+O(\xi^3)
	\end{aligned}
\end{equation}
其中$\boldsymbol{\tau}^{'}$表示$\boldsymbol{s}(\boldsymbol{x},\xi)$关于$\xi$的二阶导数在$\xi=0$处的值。这使得我们可以展开模型函数，可得 
\begin{equation}
	y(\boldsymbol{s}(\boldsymbol{x},\xi))=y(\boldsymbol{x})+\xi\boldsymbol{\tau}^T\triangledown y(\boldsymbol{x})+\frac{\xi^2}{2}\left[(\boldsymbol{\tau}^{'})^T\triangledown y(\boldsymbol{x})+\boldsymbol{\tau}^T\triangledown\triangledown y(\boldsymbol{x})\boldsymbol{\tau} \right]+O(\xi^3)
\end{equation}
代入平均误差函数$\ref{5130}$，我们有
\begin{equation}
	\begin{aligned}
		\tilde{E}&=\frac{1}{2}\iint \{y(\boldsymbol{x})-t \}^2p(t|\boldsymbol{x})p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}\mathrm{d}t\\
		&+\mathbb{E}[\xi]\iint \{y(\boldsymbol{x})-t \}\boldsymbol{\tau}^T\triangledown y(\boldsymbol{x})p(t|\boldsymbol{x})p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}\mathrm{d}t\\
		&+\mathbb{E}[\xi^2]\frac{1}{2}\iint \left[\{y(\boldsymbol{x})-t \}\{(\boldsymbol{\tau}^{'})^T\triangledown y(\boldsymbol{x})+\boldsymbol{\tau}^T\triangledown\triangledown y(\boldsymbol{x})\boldsymbol{\tau} \} \right.\\
		&\left.+(\boldsymbol{\tau}^T\triangledown y(\boldsymbol{x}))^2\right]p(t|\boldsymbol{x})p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}\mathrm{d}t +O(\xi^3)
	\end{aligned}
\end{equation}
由于变换的分布的均值为零，因此我们有$\mathbb{E}[\xi]=0$。并且，我们把$\mathbb{E}[\xi^2]$记作$\lambda$。省略$O(\xi^3)$项，这样平均误差函数就变成了
\begin{equation}
\label{5131}
	\tilde{E}=E+\lambda\Omega
\end{equation}
其中$E$是原始的平方和误差，正则化项$\Omega$的形式为
\begin{equation}
	\begin{aligned}
		\Omega&=\frac{1}{2}\int \left[\{y(\boldsymbol{x})-\mathbb{E}[t|\boldsymbol{x}] \}\{(\boldsymbol{\tau}^{'})^T\triangledown y(\boldsymbol{x})+\boldsymbol{\tau}^T\triangledown\triangledown y(\boldsymbol{x})\boldsymbol{\tau} \}+(\boldsymbol{\tau}^T\triangledown y(\boldsymbol{x}))^2 \right]p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}
	\end{aligned}
\end{equation}

我们可以进一步简化这个正则化项，如下所述，我们已经看到，使平方和误差函数达到最小值的函数为目标值$t$的条件均值$\mathbb{E}[t|\boldsymbol{x}]$。根据公式$\ref{5131}$，我们看到正则化的误差函数等于非正则化的误差函数加上一个$O(\xi^2)$的项，因此最小化总误差函数的网络函数的形式为
\begin{equation}
	y(\boldsymbol{x})=\mathbb{E}[t|\boldsymbol{x}]+O(\xi^2)
\end{equation}
从而，正则化项中的第一项消失，剩下的项为
\begin{equation}
	\Omega=\frac{1}{2}\int (\boldsymbol{\tau}^T\triangledown y(\boldsymbol{x}))^2p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}
\end{equation}
这等价于切线传播的正则化项。

如果我们考虑一个特殊情况，即输入变量的变换只是简单地添加随机噪声，从而$\boldsymbol{x}\to \boldsymbol{x}+\boldsymbol{\xi}$，那么正则化项的形式为
\begin{equation}
	\Omega=\frac{1}{2}\int \lVert \triangledown y(\boldsymbol{x})\rVert^2 p(\boldsymbol{x})\mathrm{d} \boldsymbol{x}
\end{equation}
这被称为Tikhonov正则化。这个正则化项关于网络的权值的导数可以使用反向传播算法求出。我们看到，对于小的噪声，Tikhonov正则化与输入添加随机噪声有关系。可以证明，在恰当的情况下，这种做法会提升模型的泛化能力。
\subsection*{卷积神经网络}
另一种构造对输入变量的变换具有不变性的模型的方法是将不变性的性质融入到神经网络结构的构建中。这就是卷积神经网络(convolutional neural network)的基础，它被广泛地应用于图像处理领域。

考虑手写数字识别这个具体的任务。我们知道，数字的各类对于平移、缩放以及旋转具有不变性。一种简单的方法是把图像作为一个完全链接的神经网络的输入。假如数据集充分大，那么这样的网络原则上可以产生这个问题的一个较好的解，从而可以从样本中学习到恰当的不变性。然而，这种方法忽略了图像的一个关键性质，即距离较近的像素的相关性要远大于距离较远的像素的相关性。这些想法被整合到了卷积神经网络中，通过下面的三种方式：
\begin{enumerate}[(1)]
	\item 局部接收声场
	\item 权值共享
	\item 下采样
\end{enumerate}

实际构造中，可能有若干对卷积层和下采样层。在每个阶段，与前一层相比，都会有一个更高层次的关于输入变换的不变性。

整个网络可以使用误差函数最小化的方法计算。误差函数梯度的计算可以使用反向传播算法。这需要对通常的反向传播算法进行微小的修改，确保共享权值的限制能够满足。由于使用局部接收场，网络中权值的数量要小于完全连接的网络的权值数量。此外，由于权值的本质数量的限制，需要从训练数据中学习到的独立参数的数量仍然相当小。
\subsection*{软权值共享}
降低具有大量权值参数的网络复杂度的一种方法是将权值分组，然后令分组内的权值相等。这里，我们考虑软权值共享(soft weight sharing)。这种方法中，权值相等的硬限制被替换为一种形式的正则化，其中权值的分组倾向于取近似的值。此外，权值的分组、每组权值的均值，以及分组内的取值范围全都作为学习过程的一部分被确定。

简单的权值衰减正则化项可以被看成权值上的高斯分布的负对数。我们可以将权值分成若干组，而不是将所有权值分为一个组。分组的方法是使用高斯混合概率分布。混合分布中，每个高斯分量的均值、方差，以及混合系数，都会作为可调节的参数在学习过程中被确定。于是，我们有下面形式的概率密度
\begin{equation}
	p(\boldsymbol{w})=\prod_ip(w_i)
\end{equation}
其中
\begin{equation}
	p(w_i)=\sum_{j=1}^{M}\pi_j\mathcal{N}(w_i|\mu_j,\sigma_j^2)
\end{equation}
$\pi_j$为混合系数。取负对数，即可得到正则化函数，形式为
\begin{equation}
	\Omega(\boldsymbol{w})=-\sum_i\ln\left(\sum_{j=1}^{M}\pi_j\mathcal{N}(w_i|\mu_j,\sigma_j^2) \right)
\end{equation}
从而，总的误差函数为
\begin{equation}
	\tilde{E}(\boldsymbol{w})=E(\boldsymbol{w})+\lambda \Omega(\boldsymbol{w})
\end{equation}
其中，$\lambda$是正则化系数。这个误差函数同时关于权值$w_i$和混合模型参数$\{\pi_j,\mu_j,\sigma_j \}$进行最小化。如果权值是常数，那么混合模型的参数可以由EM算法确定。然而，权值分布本身在学习过程中是不断变化的，因此为了避免数值的不稳定性，我们同时关于权值和混合模型参数进行最优化。可以使用标准的最优化算法来完成这件事情。

为了最小化总的误差函数，难免计算出它关于各个可调节参数的参数是很有必要的。为了完成这一点，比较方便的做法是把$\{\pi_j \}$当成先验概率，然后引入对应的后验概率。后验概率由贝叶斯定理给出，形式为
\begin{equation}
\begin{aligned}
	\gamma_j(w)&=p(\pi_j|w)\\
	&=\frac{p(\pi_j)p(w|\pi_j)}{\sum_k p(k)p(w|k)}\\
	&=\frac{\pi_j\mathcal{N}(w_i|\mu_j,\sigma_j^2)}{\sum_k\pi_k\mathcal{N}(w_i|\mu_k,\sigma_k^2)}
\end{aligned}
\end{equation}
这样，总的误差函数关于权值的导数为
\begin{equation}
	\begin{aligned}
	\frac{\partial \tilde{E}}{\partial w_i}&=\frac{\partial E}{\partial w_i}+\lambda\frac{\partial \Omega}{\partial w_i}\\
	&=\frac{\partial E}{\partial w_i}+\lambda\sum_{j}\frac{\pi_j\mathcal{N}(w_i|\mu_j,\sigma_j^2)}{\sum_k\pi_k\mathcal{N}(w_i|\mu_k,\sigma_k^2)}\frac{(w_i-\mu_j)}{\sigma_j^2}\\
	&=\frac{\partial E}{\partial w_i}+\lambda\sum_{j}\gamma_j(w_i)\frac{(w_i-\mu_j)}{\sigma_j^2}
	\end{aligned}
\end{equation}
正则化项的效果是把每个权值拉向第j个高斯分布的中心，拉力正比于给定权值的高斯分布的后验概率。

误差函数关于高斯分布的中心的导数
\begin{equation}
	\frac{\partial \tilde{E}}{\partial \mu_j}=\lambda\sum_i\gamma_j(w_i)\frac{(\mu_j-w_i)}{\sigma_j^2}
\end{equation}
效果是把$\mu_j$拉向了权值的平均值，拉力为第j个高斯分量产生的权值参数的后验概率。

关于方差的导数为
\begin{equation}
	\frac{\partial \tilde{E}}{\partial \sigma_j}=\lambda\sum_i\gamma_j(w_i)\left(\frac{1}{\sigma_j}-\frac{(w_i-\mu_j)^2}{\sigma_j^3} \right)
\end{equation}
效果是将$\sigma_j$拉向权值在对应的中心$\mu_j$附近的偏差的平方的加权平均，加权平均的权系数与之前一样，等于由第j个高斯分量产生的权值参数的后验概率。

关于混合系数$\pi_j$的导数，我们需要考虑下面的限制条件 
\begin{equation}
	\sum_{j}\pi_j=1,\quad 0\leqslant \pi_i \leqslant 1
\end{equation}
这个限制的产生，是因为我们把$\pi_i$看成了先验概率。可以这样做：将混合系数通过一组辅助变量$\{\eta_j \}$用softmax函数表示，即
\begin{equation}
	\pi_j=\frac{exp(\eta_j)}{\sum_{k=1}^{M}exp(\eta_k)}
\end{equation}
这样，正则化的误差函数关于$\{\eta_j \}$的导数的形式为
\begin{equation}
	\frac{\partial \tilde{E}}{\partial \eta_j}=\sum_i\{\pi_j-\gamma_j(w_i) \}
\end{equation}
效果是$\pi_j$被拉向第j个高斯分量的平均后验概率。