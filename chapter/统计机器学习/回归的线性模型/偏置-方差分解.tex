\section{偏置-方差分解}
目前为止，我们对于回归的线性模型的讨论中，我们假定了基函数的形式和数量都是固定的。如果使用有限规模的数据集来训练复杂的模型，那么使用最大似然法，或者等价地使用最小平方法，会导致严重的过拟合问题。正如前面所说，过拟合现象确实是最大似然方法的一个不好的性质。但是当我们在使用贝叶斯方法对参数进行求和或者积分时，过拟合现象不会出现。从贝叶斯观点讨论模型的复杂度之前，从频率学家的观点考虑一下模型的复杂度的问题——偏置-方差折中(bias-variance trade-off)。

\subsection*{回归问题的损失函数}
例如之前讨论过的曲线拟合问题。决策阶段包括对于每个输入$\boldsymbol{x}$，选择一个对于$t$值的具体的估计$y(\boldsymbol{x})$。最优解是使损失函数最小的解。但是，损失函数信赖于真实的$t$，这是未知的。对于一个给定的输入向量$\boldsymbol{x}$，我们对于真实类别的不确定性通过联合概率分布$p(\boldsymbol{x},t)$表示。假设这样做之后，我们造成了一个损失$L(t,y(\boldsymbol{x}))$。平均损失就是
\begin{equation}
	\mathbb{E}[L]=\iint L(t,y(\boldsymbol{x}))p(\boldsymbol{x},t)\mathrm{d}\boldsymbol{x}\mathrm{d}t
\end{equation}
回归问题中，损失函数的一个通常的选择是平方损失。这种情况下，期望损失函数可以写成
\begin{equation}
	\mathbb{E}[L]=\iint \{y(\boldsymbol{x}-t)\}^2p(\boldsymbol{x},t)\mathrm{d}\boldsymbol{x}\mathrm{d}t
\end{equation}
我们的目标是选择$y(\boldsymbol{x})$来最小化$\mathbb{E}[L]$。如果我们假设一个完全任意的函数$y(\boldsymbol{x})$，我们能够形式化地使用变分法求解：
\begin{equation}
	\frac{\delta \mathbb{E}[L]}{\delta y(\boldsymbol{x})}=2\int \{y(\boldsymbol{x}-t)\}p(\boldsymbol{x},t)\mathrm{d}t=0
\end{equation}
求解$y(\boldsymbol{x})$，使用概率的加和规则和乘积规则，我们得到
\begin{equation}
	y(\boldsymbol{x})=\frac{\int tp(\boldsymbol{x},t)\mathrm{d}t}{p(\boldsymbol{x})}=\int tp(t|\boldsymbol{d})\mathrm{d}t=\mathbb{E}_t[t|\boldsymbol{x}]
\end{equation}
这是在$\boldsymbol{x}$的条件下$t$的条件均值，被称为回归函数(regression function)。记作$h(x)$)，即
\begin{equation}
	h(\boldsymbol{x})=\mathbb{E}[t|\boldsymbol{x}]=\int tp(t|\boldsymbol{x})\mathrm{d}t
\end{equation}
已经知道了最优解是条件期望，我们可以把平方项按照下面的方式展开
\begin{equation}
	\begin{aligned}
		\{y(\boldsymbol{x})-t \}^2&=\{y(\boldsymbol{x})-\mathbb{E}[t|\boldsymbol{x}]+\mathbb{E}[t|\boldsymbol{x}]-t\}^2\\
		&=\{y(\boldsymbol{x}-\mathbb{E}[t|\boldsymbol{x}]) \}^2+2\{y(\boldsymbol{x})-\mathbb{E}[t|\boldsymbol{x}]\}\{\mathbb{E}[t|\boldsymbol{x}]-t\}\\
		&\ +\{\mathbb{E}[t|\boldsymbol{x}]-t\}^2
	\end{aligned}
\end{equation}
此处第2项等于0。因此，平方损失函数的期望可以写成
\begin{equation}
\label{llk}
\begin{aligned}
	\mathbb{E}[L]&=\iint \{y(\boldsymbol{x})-t\}^2p(x,t)\mathrm{d}x\mathrm{d}t\\
	&=\int \{y(\boldsymbol{x})-\mathbb{E}[t|\boldsymbol{x}]\}^2p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}+\int \mathrm{var}[t|\boldsymbol{x}]p(\boldsymbol{x})\mathrm{d}\boldsymbol{x}\\
	&=\int \{y(\boldsymbol{x})-h(\boldsymbol{x}) \}^2p(\boldsymbol{x})d\boldsymbol{x} + \underbrace{\iint \{h(\boldsymbol{x})-t\}^2p(\boldsymbol{x},t)\mathrm{d}\boldsymbol{x}\mathrm{d}t}_{\text{噪声}}
\end{aligned}
\end{equation}
与$y(\boldsymbol{x})$无关的第二项，是由数据本身的噪声造成的，表明期望损失能够达到的最小值。第一项与我们对函数$y(\boldsymbol{x})$的选择有关，我们要找一个$y(\boldsymbol{x})$的解，使得这一项最小。在实际应用中，我们的数据集D只有有限的N个数据点，从而我们不能够精确地知道回归函数$h(\boldsymbol{x})$。

如果我们使用由参数向量$\boldsymbol{w}$控制的函数$y(\boldsymbol{x},\boldsymbol{w})$对$h(\boldsymbol{x})$建模，那么从贝叶斯的观点来看，我们模型的不确定性是通过$\boldsymbol{w}$的概率分布来表示的。但是，频率学家的方法涉及到根据数据集$D$对$\boldsymbol{w}$进行点估计，然后试着通过下面的思想实验来表示估计的不确定性。

考虑第一项的被积函数，对于一个特定的数据集$D$，它的形式为
\begin{equation}
	\{y(x;D)-h(x)\}^2
\end{equation}
由于这个量与特定的数据集$D$相关，因此我们对所有的数据集取平均。如果我们在括号内加上然后减去$\mathbb{E}_D[y(x;D)]$，然后展开，我们有
\begin{equation}
	\begin{aligned}
		\{y(x;D)&-\mathbb{E}_D[y(x;D)]+\mathbb{E}_D[y(x;D)] -h(x) \}^2\\
		&=\{y(x;D)-\mathbb{E}_D[y(x;D)]\}^2\\
		&+\{\mathbb{E}_D[y(x;D)]-h(x) \}^2\\
		&+ 2\{y(x;D)-\mathbb{E}_D[y(x;D)]\}\{\mathbb{E}_D[y(x;D)]-h(x) \}
	\end{aligned}
\end{equation}
现在关于$D$求期望，然后注意到\textbf{最后一项等于零}，可得
\begin{equation}
\label{kk}
\begin{aligned}
	\mathbb{E}_D&[\{y(x;D)-h(x) \}^2]\\
	&=\underbrace{\mathbb{E}_D[y(x;D)]-h(x)\}^2}_{(\text{偏置})^2}+\underbrace{\mathbb{E}_D[\{y(x;D)-\mathbb{E}_D[y(x;D)] \}^2]}_{\text{方差}}
\end{aligned}
\end{equation}
我们看到，$y(\boldsymbol{x};D)$与回归函数$h(\boldsymbol{x})$的差的平方的期望可以表示为两项的和。第一项，被称为平方偏置(bias)，表示所有数据集的平均预测与预期的回归函数之间的差异。第二项，被称为方差(variance)，度量了对于单独的数据集，模型所给出的解在平均值附近波动的情况，因此也就度量了函数$y(\boldsymbol{x};D)$对于特定的数据集的选择的敏感程度。

将式$\ref{kk}$代入式$\ref{llk}$中，就得到了对于期望平方损失的分解
\begin{equation}
	\text{期望损失}=\text{偏置}^2+\text{方差}+\text{噪声}
\end{equation}
其中
\begin{flalign}
	\text{偏置}^2&=\int \{\mathbb{E}_D[y(x;D)]-h(x)\}^2p(x)dx\\
	\text{方差}&=\int \mathbb{E}_D[\{y(x;D)-\mathbb{E}_D[y(x;D)] \}^2]p(x)dx\\
	\text{噪声}&=\iint \{h(x)-t\}^2p(x,t)dxdt
\end{flalign}

我们的目标是最小化期望损失，它可以分解为(平方)偏置、方差和一个常数噪声项的和。对于非常灵活的模型来说，偏置较小，方差较大。对于相对固定的模型来说，偏置较大，方差较小。有着最优预测能力的模型是在偏置和方差之间取得最优的平衡的模型。