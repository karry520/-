\section{基于树的模型}
有许多简单但广泛使用的模型，它们将输入空间划分为超立方体区域，超立方体的边与坐标轴对齐，然后为每个区域分配一个简单的模型(例如，一个常数)。这些模型可以被看成一种模型组合方法，其中只有一个模型对于输入空间中任意给定点的预测起作用。给定一个新的输入$\boldsymbol{x}$，选择一个具体的模型的过程可以由一个顺序决策的过程描述，这个过程对应于一个二叉树的遍历。决策树(decision tree)是一种基本的分类与回归方法。本节主要讨论用于分类的决策树。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。这些决策树学习的思想主要源于由Quinlan在1986年提出的ID3算法和1993年提出的C4.5算法，以及由Breiman等人在1984年提出的CART算法。

本节首先介绍决策树的基本概念，然后通过ID3和C4.5介绍特征的选择、决策树的生成以及决策树的修剪，最后介绍CART算法。

\subsection*{决策树模型与学习}
分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。
\begin{enumerate}
	\item 可以将决策树看成一个it-then规则的集合。
	\item 决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分(partition)上。
	\item 决策树学习本质上是从训练数据集中归纳出一组分类规则；从另一个角度看，决策树学习是由训练数据集估计条件概率模型。
\end{enumerate}

\subsection*{特征选择}
直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益（information gain）就能够很好地表示这一直观的准则。

\textbf{信息增益}
特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即
$$g(D,A) = H(D) - H(D|A)$$
一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息(mutual information)。

\textbf{信息增益的算法}

输入：训练数据集D和特征A；

输出：特征A对训练数据集D的信息增益$g(D,A)$
\begin{enumerate}[(1)]
	\item 计算数据集D的经验熵$H(D)$
	\begin{equation}
		H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}
	\end{equation}
	\item 计算特征A对数据集D的经验条件熵$H(D|A)$
	\begin{equation}
		H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(|D_i|) = -\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{D_{ik}}{D_i}log_2\frac{|D_{ik}|}{|D_i|}
	\end{equation}
	\item 计算信息增益
	\begin{equation}
		g(D,A) = H(D) - H(D|A)
	\end{equation}
\end{enumerate}
以信息增益作为划分训练数据集的特征，存在偏向选择取值较多的特征的问题。使用信息增益比(information gain ratio)可以对这一问题进行校正。这是特征选择的另一准则。

\textbf{信息增益比}
\begin{equation}
	g_R(D,A) = \frac{g(D,A)}{H_A(D)}
\end{equation}
其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{D}log_2\frac{|D_i|}{|D|}$，n是特征A取值的个数。

\subsection*{决策树的生成}
\begin{enumerate}
	\item ID3算法
	
	输入：训练数据集D，特征集A，阈值$\epsilon$；
	
	输出：决策树T
	\begin{enumerate}[(1)]
		\item 若D中所有实例属于同一类$C_k$，则T为单结点树，并将类$C_k$作为该结点的类标记，返回T
		\item 若$A=\emptyset $，则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T
		\item 否则按信息增益算法计算A中各特征对D的信息增益，选择信息增益最大的特征$A_g$
		\item 如果$A_g$的信息增益小于阈值$\epsilon$，则置T为单结点树，并将D中实例数最大的类作为标记，返回T
		\item 否则，对$A_g$的每一可能值 $a_i$，依$A_g=a_i$将D分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T
		\item 对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用(1)~(5)，得到子树$T_i$，返回T
	\end{enumerate}
	\item C4.5的生成算法
	
	C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进。C4.5在生成的过程中，用信息增益比来选择特征。
\end{enumerate}
\subsection*{决策树的剪枝}
决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。
\subsection*{CART算法}
分类回归树(classification and regression tree, CART)，是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。
\begin{enumerate}
	\item 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数(Gini index)最小化准则，进行特征选择，生成二叉树。
	\item 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准
	\begin{enumerate}[(1)]
		\item 剪枝，形成一个子树序列
		\item 在剪枝得到的子树序列$T_0,T_1,\dots, T_n$中通过交叉验证选取最优子树$T_a$
	\end{enumerate}
\end{enumerate}

