我们考虑了回归问题和分类问题的线性参数模型，其中从输入$\boldsymbol{x}$到输出$y$的映射$y(\boldsymbol{x},\boldsymbol{w})$的形式由可调节参数构成的向量$\boldsymbol{w}$控制。在学习阶段，一组训练数据用来得到参数向量的点估计，或者用来确定这个向量的后验概率分布。然后，训练数据之后被丢弃，对于新输入的预测纯粹依靠学习到的参数向量$\boldsymbol{w}$。这个方法也被用于非线性参数模型，例如神经网络。

然而，有这样一类模式识别的技术：训练数据点或者它的一个子集在预测阶段仍然保留并且被使用。例如最近邻方法。基于存储的方法把整个训练数据存储起来，用来对未来的数据点进行预测。通常这种方法需要一个度量，来定义输入空间任意两个向量之间的相似度。这种方法通常“训练”速度很快，但是对测试数据点的预测速度很慢。

许多线性参数模型可以被转化为一个等价的“对偶表示”。对偶表示中，预测的基础也是在训练数据点处计算的核函数(kernel function)的线性组合。对于基于固定非线性特征空间(feature space)映射$\phi(\boldsymbol{x})$的模型来说，核函数由下面的关系给出。
\begin{equation}
\label{kernel}
	k(\boldsymbol{x},\boldsymbol{x}^{'})=\phi(\boldsymbol{x})^T\phi(\boldsymbol{x}^{'})
\end{equation}
核的概念由Aizenman引入模型识别领域。那篇文章介绍了势函数的方法。之所以被称为势函数，是因为它类似于静电学中的概念。虽然被忽视了很多年，但是Boser在边缘分类器的问题中把它重新引入到了机器学习领域。那篇文章提出了支持向量机的方法。从那里起，这个话题在理论上和实用上都吸引了大家的兴趣。一个最重要的发展是把核方法进行了扩展，使其能处理符号化的物体，从而极大地扩展了这种方法能处理的问题的范围。