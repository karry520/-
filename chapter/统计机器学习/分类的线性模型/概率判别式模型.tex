\section{概率判别式模型}
对于二分类问题，我们已经看到，对于一大类的类条件概率密度$p(\boldsymbol{x}|C_k)$的选择，类别$C_1$后验概率分布可以写成作用于$\boldsymbol{x}$的线性函数上的logistic sigmoid函数的形式。类似地，对于多分类的情形，类别$C_k$的后验概率由$\boldsymbol{x}$的线性函数的softmax变换给出。对于类条件概率密度$p(\boldsymbol{x}|C_k)$的具体的选择，我们已经使用了最大似然方法估计了概率密度的参数以及类别先验$p(C_k)$，然后使用贝叶斯定理就可以求出后验类概率。

另一种方法是显示地使用一般的线性模型的函数形式，然后使用最大似然法直接确定它的参数。

寻找一般的线性模型参数的间接方法是，分别寻找类条件概率密度和类别先验，然后使用贝叶斯定理。这是生成式建模的一个例子。在直接方法中，我们最大化由条件概率分布$p(C_k|\boldsymbol{x})$定义的似然函数。这种方法代表了判别式训练的一种形式。判别式方法的一个优点是通常有更少的可调节的参数需要确定，并且预测表现会提升，尤其是当类条件概率密度的假设没有很好地近似真实的分布的时候更是如此。
\subsection*{固定基函数}
如果首先使用一个基函数向量$\phi(\boldsymbol{x})$对输入变量进行一个固定的非线性变换，所有的这些算法仍然同样适用。
\subsection*{logistic回归}
首先通过二分类问题开始对于一般线性模型的讨论。类别$C_1$的后验概率可以写成作用在特征向量$\phi$的线性函数上的logistic sigmoid函数的形式，即
\begin{flalign}
	p(C_1|\phi)&=y(\phi)=\sigma(\boldsymbol{w}^T\phi)\\
	p(C_2|\phi)&=1-p(C_1|\phi)
\end{flalign}
这个模型被称为logistic回归，这是一个分类模型而不是回归模型。

对于一个M维特征空间$\phi$，这个模型有M个可调节参数。相反，如果我们使用最大似然方法调节高斯类条件概率密度，那么我们有2M个参数来描述均值，以及$\frac{M(M+1)}{2}$个参数来描述协方差矩阵。算上类先验$p(C_1)$，参数的总数为$\frac{M(M+5)}{2}+1$，这随着M的增长而以二次的方式增长。这和logstic回归方法中对于参数数据M的线性依赖不同。对于大的M值，直接使用logistic回归模型有着很明显的优势。

现在使用最大似然方法来确定logistic回归模型的参数。对于一个数据集$\phi_n,t_n$，其中$t_n\in \{0,1\},\phi_n=\phi(\boldsymbol{x}_n),n=1,\dots,N$，似然函数可以写成
\begin{equation}
	p(\boldsymbol{t}|\boldsymbol{w})=\prod_{n=1}^{N}y_n^{t_n}\{1-y_n \}^{1-t_n}
\end{equation}
其中$\boldsymbol{t}=(t_1,\dots,t_N)^T$且$y_n=p(C_1|\phi_n)$。通过取似然函数的负对数的方式，定义一个误差函数。这种方式产生了交叉熵(cross-entropy)误差函数，形式为
\begin{equation}
	\begin{aligned}
	E(\boldsymbol{w})&=-\ln p(\boldsymbol{t}|\boldsymbol{w})\\
	&=-\sum_{n=1}^{N}\{t_n\ln y_n+(1-t_n)\ln (1-y_n) \}
	\end{aligned}
\end{equation}
其中$y_n=\sigma(a_n)$且$a_n=\boldsymbol{w}^T\phi_n$。两侧关于$\boldsymbol{w}$取误差函数的梯度，我们有
\begin{equation}
\begin{aligned}
	\triangledown E(\boldsymbol{w})&=-\sum_{n=1}^{N}\{\frac{t_n}{y_n}y_n^{'}+\frac{(1-t_n)}{1-y_n}(1-y_n)^{'} \}\\
	&=-\sum_{n=1}^{N}\frac{t_n}{\sigma}\sigma(1-\sigma)\phi_n - \frac{1-t_n}{1-\sigma}\sigma(1-\sigma)\phi_n\\
	&=\sum_{n=1}^{N}(y_n-t_n)\phi_n
\end{aligned}
\end{equation}
推导时用到了
\begin{equation}
	\frac{d\sigma}{da}=\sigma(1-\sigma)
\end{equation}
我们看到，涉及到logistic sigmoid的导数的因子已经被消去，使得对数似然函数的梯度的形式十分简单。特别地，数据点$n$对梯度的贡献为目标值和模型预测值之间的“误差”与基函数向量$\phi_n$相乘。此外它的函数形式与线性回归模型中的平方和误差函数的梯度的函数形式完全相同。

问题变成了以对数似然函数为目标函数的最优化问题。logistic回归学习中通常采用的方法是梯度下降法及拟牛顿法。

值得注意的一点是，最大似然方法对于线性可分的数据集会产生严重的过拟合现象。最大似然方法无法区分某个解优于另一个解，并且在实际应用中哪个解被找到将会依赖于优化算法的选择和参数的初始化。只要数据是线性可分的，这个问题就会出现。通过引入先验概率，然后寻找$\boldsymbol{w}$的MAP解，或者等价地，通过给误差函数增加一个正则化项，这种奇异性就可以被避免。
\subsection*{迭代重加权最小平方}
对于logistic回归来说，不再有解析解，因为logistic sigmoid函数是一个非线性函数。然而，函数形式不是二次函数并不是本质的原因。精确地说，误差函数是凸函数，因此有一个唯一的最小值。此外，误差函数可以通过一种高效的迭代方法求出最小值，这种迭代方法基于Newton-Raphson迭代最优化框架，使用了对数似然函数的局部二次近似。为了最小化函数$E(\boldsymbol{w})$，Newton-Raphson对权值的更新的形式为
\begin{equation}
	\boldsymbol{w}^{\text{新}}=\boldsymbol{w}^{\text{旧}}-H^{-1}\triangledown E(\boldsymbol{w})
\end{equation}
其中$H$是一个Hessian矩阵。把Newton-Raphson方法应用到现行回归模型上，误差函数为平方和误差函数。这个误差函数的梯度和Hessian矩阵为
\begin{flalign}
	\triangledown E[\boldsymbol{w}]&=\sum_{n=1}^{N}(\boldsymbol{w}^T\phi_n-t_n)\phi_n=\Phi^T\Phi \boldsymbol{w}-\Phi^T\boldsymbol{t}\\
	H&=\triangledown\triangledown E(\boldsymbol{w})=\sum_{n=1}^{N}\phi_n\phi_n^T=\Phi^T\Phi
\end{flalign}
其中$\Phi$是$N\times M$设计矩阵，第n行为$\phi_n^T$。于是，Newton-Raphson更新的形式为
\begin{equation}
	\begin{aligned}
	\boldsymbol{w}^{\text{新}}&=\boldsymbol{w}^{\text{旧}}-(\Phi^T\Phi)^{-1}\{\Phi^T\Phi \boldsymbol{w}^{\text{旧}}-\Phi^T\boldsymbol{t} \}\\
	&=(\Phi^T\Phi)^{-1}\Phi^T\boldsymbol{t}
	\end{aligned}
\end{equation}
现在把Newton-Raphson更新应用到logistic回归模型的交叉熵误差函数上。
\begin{flalign}
	\triangledown E(\boldsymbol{w})&=\sum_{n=1}^{N}(y_n-t_n)\phi_n = \Phi^T(\boldsymbol{y}-\boldsymbol{t})\\
	H=\triangledown \triangledown  E(\boldsymbol{w})&=\sum_{n=1}^{N}y_n(1-y_n)\phi_n\phi_n^T=\Phi^T\mathcal{R}\Phi
\end{flalign}
推导过程中，引入了一个$N\times N$的对角矩阵$\mathcal{R}$，元素为
\begin{equation}
	\mathcal{R}_{nn}=y_n(1-y_n)
\end{equation}
我们看到，Hessian矩阵不再是常量，而是通过权矩阵$\mathcal{R}$依赖于$\boldsymbol{w}$。这对应于误差函数不是二次函数的事实。使用性质$0<y_n<1$，我们看到对于任意向量$\boldsymbol{u}$都有$\boldsymbol{u}^TH\boldsymbol{u}>0$，因此Hessian矩阵H是正定的。因此误差函数是$\boldsymbol{w}$的一个凸函数，从而有唯一的最小值。

这样，logistic回归模型的Newton-Raphson更新公式就变成了
\begin{equation}
	\begin{aligned}
	\boldsymbol{w}^{\text{新}}&=\boldsymbol{w}^{\text{旧}}-(\Phi^T\mathcal{R}\Phi)^{-1}\Phi^T(\boldsymbol{y}-\boldsymbol{t})\\
	&=(\Phi^T\mathcal{R}\Phi)^{-1}\{\Phi^T\mathcal{R}\Phi\boldsymbol{w}^{\text{旧}}- \Phi^T(\boldsymbol{y}-\boldsymbol{t})\}\\
	&=(\Phi^T\mathcal{R}\Phi)^{-1}\Phi^T\mathcal{R}\boldsymbol{z}
	\end{aligned}
\end{equation}
其中$\boldsymbol{z}$是一个$N$维向量，元素为
\begin{equation}
	\boldsymbol{z}=\Phi\boldsymbol{w}^{\text{旧}}-\mathcal{R}^{-1}(\boldsymbol{y}-\boldsymbol{t})
\end{equation}
更新公式的形式为一组加权最小平方问题的规范方程。由于权矩阵$\mathcal{R}$不是常量，而是依赖于参数向量$\boldsymbol{w}$，因此我们必须迭代地应用规范方程，每次使用新的权向量$\boldsymbol{w}$计算一个修正的权矩阵$\mathcal{R}$，由于这个原因，这个算法被称为迭代重加权最小平方(iterative reweighted least squares)，或者简称为IRLS。对角$\mathcal{R}$可以看成方差。
\subsection*{多类logistic回归}
对于多分类的生成式模型，后验概率由特征变量的线性函数的softmax变换给出，即
\begin{equation}
	p(C_k|\phi)=y_k(\phi)=\frac{exp(\boldsymbol{w}_k^T\phi)}{\sum_jexp(\boldsymbol{w}_k^T\phi)}
\end{equation}
\subsection*{probit回归}
对于由指数族分布描述的一大类的类条件概率分布，最终求出的后验类概率为作用在特征变量的线性函数上的logistic(或者softmax)变换。然而，不是所有的类条件概率密度都有这样简单的后验概率函数形式(例如，如果类条件概率密度由高斯混合模型建模)。这表明研究其他类型的判别式概率模型可能会很有价值。回到二分类的情形，再次使用一般的线性模型的框架，即
\begin{equation}
	p(t=1|a)=f(a)
\end{equation}
其中$a=\boldsymbol{w}^T\phi$，且$f(\cdot)$为激活函数。
对于每个输入$\phi_n$，我们计算$\boldsymbol{w}^T\phi_n$，然后按照下面的方式设置目标值
\begin{equation}
	\begin{cases}
	t_n=1,\quad \text{如果}a_n\geqslant \theta\\
	t_n=0,\quad \text{其他情况}
	\end{cases}
\end{equation}
如果$\theta$的值从概率密度$p(\theta)$中抽取，那么对应的激活函数由累积分布函数给出
\begin{equation}
	f(a)=\int_{-\infty}^ap(\theta)d\theta
\end{equation}
这被称为逆probit(inverse probit)函数。它的形状为sigmoid形。许多用于计算这个函数的数值计算包都与下面的这个函数紧密相关
\begin{equation}
	erf(a)=\frac{2}{\sqrt{\pi}}\int_0^aexp(-\theta^2)d\theta
\end{equation}
它被称为erf函数或者被称为error函数。它与逆probit函数的关系为
\begin{equation}
	\Phi(a)=\frac{1}{2}\left\{1+erf\left(\frac{a}{\sqrt{2}} \right) \right\}
\end{equation}
基于probit激活函数的一般的线性模型被称为probit回归。
\subsection*{标准链接函数}
如果假设目标变量的条件分布来自于指数族分布，对应的激活函数选为标准链接函数(canonical link function)，那么这个结果是一个一般的结果。

