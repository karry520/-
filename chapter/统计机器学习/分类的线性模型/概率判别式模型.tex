\section{概率判别式模型}
对于二分类问题，我们已经看到，对于一大类的类条件概率密度$p(\boldsymbol{x}|C_k)$的选择，类别$C_1$后验概率分布可以写成作用于$\boldsymbol{x}$的线性函数上的logistic sigmoid函数的形式。类似地，对于多分类的情形，类别$C_k$的后验概率由$\boldsymbol{x}$的线性函数的softmax变换给出。对于类条件概率密度$p(\boldsymbol{x}|C_k)$的具体的选择，我们已经使用了最大似然方法估计了概率密度的参数以及类别先验$p(C_k)$，然后使用贝叶斯定理就可以求出后验类概率。

另一种方法是显示地使用一般的线性模型的函数形式，然后使用最大似然法直接确定它的参数。

寻找一般的线性模型参数的间接方法是，分别寻找类条件概率密度和类别先验，然后使用贝叶斯定理。这是生成式建模的一个例子。在直接方法中，我们最大化由条件概率分布$p(C_k|\boldsymbol{x})$定义的似然函数。这种方法代表了判别式训练的一种形式。判别式方法的一个优点是通常有更少的可调节的参数需要确定，并且预测表现会提升，尤其是当类条件概率密度的假设没有很好地近似真实的分布的时候更是如此。
\subsection*{固定基函数}
如果首先使用一个基函数向量$\phi(\boldsymbol{x})$对输入变量进行一个固定的非线性变换，所有的这些算法仍然同样适用。最终的决策边界在特征空间$\phi$中是线性的，因此对应于原始$\boldsymbol{x}$空间中的非线性决策边界。

对于许多实际问题来说，类条件概率密度$p(\boldsymbol{x}|C_k)$之间有着相当大的重叠。这表明至少对于某些$\boldsymbol{x}$的值，后验概率$p(C_k|\boldsymbol{x})$不等于0或1。在这种情况下，最优解可以通过下面的方式获得：对后验概率精确建模，然后使用前面章节讨论的标准的决策论。需要注意的是，非线性变换$\phi(\boldsymbol{x})$不会消除这些重叠。实际上，这些变换会增加重叠的程度，或者在原始观测空间中不存在重叠的地方产生出新的重叠。然而，恰当的选择非线性变换能够让后验概率的建模过程更简单。

这样的固定基函数模型有着重要的局限性，这些局限性在后续的章节中会被解决，解决方法为允许基函数自身根据数据进行调节。
\subsection*{logistic回归}
首先通过二分类问题开始对于一般线性模型的讨论。类别$C_1$的后验概率可以写成作用在特征向量$\phi$的线性函数上的logistic sigmoid函数的形式，即
\begin{flalign}
	p(C_1|\phi)&=y(\phi)=\sigma(\boldsymbol{w}^T\phi)\\
	p(C_2|\phi)&=1-p(C_1|\phi)
\end{flalign}
这个模型被称为logistic回归，这是一个分类模型而不是回归模型。

对于一个M维特征空间$\phi$，这个模型有M个可调节参数。相反，如果我们使用最大似然方法调节高斯类条件概率密度，那么我们有2M个参数来描述均值，以及$\frac{M(M+1)}{2}$个参数来描述协方差矩阵。算上类先验$p(C_1)$，参数的总数为$\frac{M(M+5)}{2}+1$，这随着M的增长而以二次的方式增长。这和logstic回归方法中对于参数数据M的线性依赖不同。对于大的M值，直接使用logistic回归模型有着很明显的优势。

现在使用最大似然方法来确定logistic回归模型的参数。对于一个数据集$\phi_n,t_n$，其中$t_n\in \{0,1\},\phi_n=\phi(\boldsymbol{x}_n),n=1,\dots,N$，似然函数可以写成
\begin{equation}
	p(\boldsymbol{t}|\boldsymbol{w})=\prod_{n=1}^{N}y_n^{t_n}\{1-y_n \}^{1-t_n}
\end{equation}
其中$\boldsymbol{t}=(t_1,\dots,t_N)^T$且$y_n=p(C_1|\phi_n)$。通过取似然函数的负对数的方式，定义一个误差函数。这种方式产生了交叉熵(cross-entropy)误差函数，形式为
\begin{equation}
	\begin{aligned}
	E(\boldsymbol{w})&=-\ln p(\boldsymbol{t}|\boldsymbol{w})\\
	&=-\sum_{n=1}^{N}\{t_n\ln y_n+(1-t_n)\ln (1-y_n) \}
	\end{aligned}
\end{equation}
其中$y_n=\sigma(a_n)$且$a_n=\boldsymbol{w}^T\phi_n$。两侧关于$\boldsymbol{w}$取误差函数的梯度，我们有
\begin{equation}
\begin{aligned}
	\triangledown E(\boldsymbol{w})&=-\sum_{n=1}^{N}\{\frac{t_n}{y_n}y_n^{'}+\frac{(1-t_n)}{1-y_n}(1-y_n)^{'} \}\\
	&=-\sum_{n=1}^{N}\frac{t_n}{\sigma}\sigma(1-\sigma)\phi_n - \frac{1-t_n}{1-\sigma}\sigma(1-\sigma)\phi_n\\
	&=\sum_{n=1}^{N}(y_n-t_n)\phi_n
\end{aligned}
\end{equation}
推导时用到了
\begin{equation}
	\frac{\mathrm{d}\sigma}{\mathrm{d}a}=\sigma(1-\sigma)
\end{equation}
我们看到，涉及到logistic sigmoid的导数的因子已经被消去，使得对数似然函数的梯度的形式十分简单。特别地，数据点$n$对梯度的贡献为目标值和模型预测值之间的“误差”与基函数向量$\phi_n$相乘。此外它的函数形式与线性回归模型中的平方和误差函数的梯度的函数形式完全相同。

问题变成了以对数似然函数为目标函数的最优化问题。logistic回归学习中通常采用的方法是梯度下降法及拟牛顿法。

值得注意的一点是，最大似然方法对于线性可分的数据集会产生严重的过拟合现象。最大似然方法无法区分某个解优于另一个解，并且在实际应用中哪个解被找到将会依赖于优化算法的选择和参数的初始化。只要数据是线性可分的，这个问题就会出现。通过引入先验概率，然后寻找$\boldsymbol{w}$的MAP解，或者等价地，通过给误差函数增加一个正则化项，这种奇异性就可以被避免。
\subsection*{迭代重加权最小平方}
对于logistic回归来说，不再有解析解，因为logistic sigmoid函数是一个非线性函数。然而，函数形式不是二次函数并不是本质的原因。精确地说，误差函数是凸函数，因此有一个唯一的最小值。此外，误差函数可以通过一种高效的迭代方法求出最小值，这种迭代方法基于Newton-Raphson迭代最优化框架，使用了对数似然函数的局部二次近似。为了最小化函数$E(\boldsymbol{w})$，Newton-Raphson对权值的更新的形式为
\begin{equation}
	\boldsymbol{w}^{\text{新}}=\boldsymbol{w}^{\text{旧}}-H^{-1}\triangledown E(\boldsymbol{w})
\end{equation}
其中$H$是一个Hessian矩阵。把Newton-Raphson方法应用到现行回归模型上，误差函数为平方和误差函数。这个误差函数的梯度和Hessian矩阵为
\begin{flalign}
	\triangledown E[\boldsymbol{w}]&=\sum_{n=1}^{N}(\boldsymbol{w}^T\phi_n-t_n)\phi_n=\Phi^T\Phi \boldsymbol{w}-\Phi^T\boldsymbol{t}\\
	H&=\triangledown\triangledown E(\boldsymbol{w})=\sum_{n=1}^{N}\phi_n\phi_n^T=\Phi^T\Phi
\end{flalign}
其中$\Phi$是$N\times M$设计矩阵，第n行为$\phi_n^T$。于是，Newton-Raphson更新的形式为
\begin{equation}
	\begin{aligned}
	\boldsymbol{w}^{\text{新}}&=\boldsymbol{w}^{\text{旧}}-(\Phi^T\Phi)^{-1}\{\Phi^T\Phi \boldsymbol{w}^{\text{旧}}-\Phi^T\boldsymbol{t} \}\\
	&=(\Phi^T\Phi)^{-1}\Phi^T\boldsymbol{t}
	\end{aligned}
\end{equation}
我们看到这是标准的最小平方解。注意，这种情况下误差函数是二次的，因此Newton-Raphson公式用1步就给出了精确解。

现在把Newton-Raphson更新应用到logistic回归模型的交叉熵误差函数上。
\begin{flalign}
	\triangledown E(\boldsymbol{w})&=\sum_{n=1}^{N}(y_n-t_n)\phi_n = \Phi^T(\boldsymbol{y}-\boldsymbol{t})\\
	H=\triangledown \triangledown  E(\boldsymbol{w})&=\sum_{n=1}^{N}y_n(1-y_n)\phi_n\phi_n^T=\Phi^T\mathcal{R}\Phi
\end{flalign}
推导过程中，引入了一个$N\times N$的对角矩阵$\mathcal{R}$，元素为
\begin{equation}
	\mathcal{R}_{nn}=y_n(1-y_n)
\end{equation}
我们看到，Hessian矩阵不再是常量，而是通过权矩阵$\mathcal{R}$依赖于$\boldsymbol{w}$。这对应于误差函数不是二次函数的事实。使用性质$0<y_n<1$，我们看到对于任意向量$\boldsymbol{u}$都有$\boldsymbol{u}^TH\boldsymbol{u}>0$，因此Hessian矩阵H是正定的。因此误差函数是$\boldsymbol{w}$的一个凸函数，从而有唯一的最小值。

这样，logistic回归模型的Newton-Raphson更新公式就变成了
\begin{equation}
	\begin{aligned}
	\boldsymbol{w}^{\text{新}}&=\boldsymbol{w}^{\text{旧}}-(\Phi^T\mathcal{R}\Phi)^{-1}\Phi^T(\boldsymbol{y}-\boldsymbol{t})\\
	&=(\Phi^T\mathcal{R}\Phi)^{-1}\{\Phi^T\mathcal{R}\Phi\boldsymbol{w}^{\text{旧}}- \Phi^T(\boldsymbol{y}-\boldsymbol{t})\}\\
	&=(\Phi^T\mathcal{R}\Phi)^{-1}\Phi^T\mathcal{R}\boldsymbol{z}
	\end{aligned}
\end{equation}
其中$\boldsymbol{z}$是一个$N$维向量，元素为
\begin{equation}
	\boldsymbol{z}=\Phi\boldsymbol{w}^{\text{旧}}-\mathcal{R}^{-1}(\boldsymbol{y}-\boldsymbol{t})
\end{equation}
更新公式的形式为一组加权最小平方问题的规范方程。由于权矩阵$\mathcal{R}$不是常量，而是依赖于参数向量$\boldsymbol{w}$，因此我们必须迭代地应用规范方程，每次使用新的权向量$\boldsymbol{w}$计算一个修正的权矩阵$\mathcal{R}$，由于这个原因，这个算法被称为迭代重加权最小平方(iterative reweighted least squares)，或者简称为IRLS。对角$\mathcal{R}$可以看成方差。

事实上，我们可以把IRLS看成变量空间$a=\boldsymbol{w}^T\phi$的线性问题的解。这样，$\boldsymbol{z}$的第$n$个元素$z_n$就可以简单地看成这个空间中的有效的目标值。$z_n$可以通过对当前操作点$w^{\text{旧}}$附近的logistic sigmoid函数的局部线性近似的方式得到。
\begin{equation}
\begin{aligned}
	a_n(\boldsymbol{w})&\simeq a_n(\boldsymbol{w}^{\text{旧}})+\frac{\mathrm{d}a_n}{\mathrm{d}y_n}\Bigg|_{\boldsymbol{w}^{\text{旧}}}(t_n-y_n)\\
		&=\phi_n^T\boldsymbol{w}^{\text{旧}}-\frac{y_n-t_n}{y_n(1-y_n)}=z_n
\end{aligned}
\end{equation}
\subsection*{多类logistic回归}
对于多分类的生成式模型，后验概率由特征变量的线性函数的softmax变换给出，即
\begin{equation}
	p(C_k|\phi)=y_k(\phi)=\frac{\exp(\boldsymbol{w}_k^T\phi)}{\sum_j\exp(\boldsymbol{w}_k^T\phi)}
\end{equation}
\subsection*{probit回归}
对于由指数族分布描述的一大类的类条件概率分布，最终求出的后验类概率为作用在特征变量的线性函数上的logistic(或者softmax)变换。然而，不是所有的类条件概率密度都有这样简单的后验概率函数形式(例如，如果类条件概率密度由高斯混合模型建模)。这表明研究其他类型的判别式概率模型可能会很有价值。回到二分类的情形，再次使用一般的线性模型的框架，即
\begin{equation}
	p(t=1|a)=f(a)
\end{equation}
其中$a=\boldsymbol{w}^T\phi$，且$f(\cdot)$为激活函数。
对于每个输入$\phi_n$，我们计算$\boldsymbol{w}^T\phi_n$，然后按照下面的方式设置目标值
\begin{equation}
	\begin{cases}
	t_n=1,\quad \text{如果}a_n\geqslant \theta\\
	t_n=0,\quad \text{其他情况}
	\end{cases}
\end{equation}
如果$\theta$的值从概率密度$p(\theta)$中抽取，那么对应的激活函数由累积分布函数给出
\begin{equation}
	f(a)=\int_{-\infty}^ap(\theta)d\theta
\end{equation}
在随机阈值模型中，如果$a=\boldsymbol{w}^T\phi$的值超过某个阈值，则类别标签的取值为$t=1$，否则它的取值为$t=0$。这等价于由累积密度函数$f(a)$给出的激活函数。

作为一个具体的例子，假设概率密度$p(\theta)$是零均值、单位方差的高斯概率密度。对应的累积分布函数为
\begin{equation}
	\Phi(a)=\int_{-\infty}^{a}\mathcal{N}(\theta |0,1)\mathrm{d}\theta 
\end{equation}
这被称为逆probit(inverse probit)函数。它的形状为sigmoid形。许多用于计算这个函数的数值计算包都与下面的这个函数紧密相关
\begin{equation}
	\mathrm{erf}(a)=\frac{2}{\sqrt{\pi}}\int_0^a\exp(-\theta^2)d\theta
\end{equation}
它被称为erf函数或者被称为error函数。它与逆probit函数的关系为
\begin{equation}
	\Phi(a)=\frac{1}{2}\left\{1+\mathrm{erf}\left(\frac{a}{\sqrt{2}} \right) \right\}
\end{equation}
基于probit激活函数的一般的线性模型被称为probit回归。

在实际应用中经常出现的一个问题是离群点，它可能由输入向量$\boldsymbol{x}$的测量误差产生，或者由目标值$t$的错误标记产生。他们会严重地干扰分类器，在这一点上probit模型对于离群点会更加敏感。对于错误标记的影响可以合并到概率模型中。我们引入一个概率$\epsilon$，它是目标值$t$被翻转到错误值的概率。$\epsilon$可以事先设定，也可以被当成超参数，然后从数据中推断它的值。
\subsection*{标准链接函数}
对于高斯噪声分布的线性回归模型，如果我们对数据点$n$对误差函数的贡献关于参数向量$\boldsymbol{w}$求导，那么导数的形式为“误差”$y_n-t_n$与特征向量$\phi_n$的乘积，其中$y_n=\boldsymbol{w}^T\phi_n$。类似地，对于logistic sigmoid激活函数与交叉熵误差函数的组合，以及多类交叉熵误差函数的softmax激活函数，我们再次得到了同样的简单形式。现在我们证明，如果假设目标变量的条件分布来自于指数族分布，对应的激活函数选为标准链接函数(canonical link function)，那么这个结果是一个一般的结果。

我们使用指数族分布的限制形式。注意，我们把指数族分布的假设应用于目标变量$t$，而不是应用于输入向量$\boldsymbol{x}$。于是，我们考虑目标变量的条件分布
\begin{equation}
	p(t|\eta,s)=\frac{1}{s}h\left(\frac{t}{s} \right)g(\eta)\exp\left\{\frac{\eta t}{s} \right\}
\end{equation}
使用与推导结果$\ref{2226}$时相同的过程，我们看到$t$的条件均值(记作y)为
\begin{equation}
	y\equiv [t|\eta]=-s\frac{\mathrm{d}}{\mathrm{d} \eta}\ln g(\eta)
\end{equation}
因此$y$和$\eta$一定相关，我们把这个关系记作$\eta =\psi (y)$。

按照Nelder and Wedderburn的方法，我们将一般线性模型定义为这样的模型：$y$是输入变量(或者特征变量)的线性组合的非线性函数，即
\begin{equation}
	y=f(\boldsymbol{w}^T\phi)
\end{equation}
其中$f^{-1}(\cdot)$在统计学中被称为链接函数(link function)。

现在考虑这个模型的对数似然函数。它是$\eta$的一个函数，形式为
\begin{equation}
	\ln p(\boldsymbol{t}|\eta,s)=\sum_{n=1}^{N}\ln p(t_n|\eta,s)=\sum_{n=1}^{N}\left\{\ln g(\eta_n)+\frac{\eta_nt_n}{s} \right\}+\text{常数}
\end{equation}
其中我们假定所有的观测有一个相同的缩放参数(它对应着例如服从高斯分布的噪声的方差)，因此$s$与$n$无关。对数似然函数关于模型参数$\boldsymbol{w}$的导数为
\begin{equation}
	\begin{aligned}
		\triangledown_{\boldsymbol{w}}\ln p(\boldsymbol{t}|\eta,s)&=\sum_{n=1}^{N}\left\{\frac{\mathrm{d}}{\mathrm{d}\eta_n}\ln g(\eta_n)+\frac{t_n}{s} \right\}\frac{\mathrm{d}\eta_n}{\mathrm{d}y_n}\frac{\mathrm{d}y_n}{\mathrm{d}a_n}\triangledown a_n\\
		&=\sum_{n=1}^{N}\frac{1}{s}\{t_n-y_n \}\psi^{'}(y_n)f^{'}(a_n)\phi_n
	\end{aligned}
\end{equation}
其中$a_n=\boldsymbol{w}^T\phi_n$，并且我们使用了$y_n=f(a_n)$以及$\mathbb{E}[t|\eta]$的结果。我们现在看到，如果我们为链接函数$f^{-1}(y)$选成下面的形式，那么表达式会得到极大的简化。
\begin{equation}
	f^{-1}(y)=\psi(y)
\end{equation}
上式表明$f(\psi(y))=y$，因此$f^{'}(\psi)\psi^{'}(y)=1$。并且，由于$a=f^{-1}(y)$，我们有$a=\psi$，因此$f^{'}(a)\psi^{'}(y)=1$。在这种情况下，误差函数的梯度可以化简为
\begin{equation}
	\triangledown E(\boldsymbol{w})=\frac{1}{s}\sum_{n=1}^{N}\{y_n-t_n \}\phi_n
\end{equation}
对于高斯分布，$s=\beta^{-1}$，而对于logistic模型，$s=1$。
