在前一章中，我们研究了许多基于非线性核的学习算法。这种算法的一个最大的局限性是核函数$k(\boldsymbol{x}_n,boldsymbol{x}_m)$必须对所有可能的训练点对$\boldsymbol{x}$和$\boldsymbol{x}_M$进行求值，这在训练阶段的计算上是不可行的，并且会使得对新的数据点进行预测时也会花费过多的时间。本章中，我们会看到具有稀疏(sparse)解的基于核的算法，从而对新数据的预测只依赖于在训练数据点的一个子集上计算的核函数。

首先，我们详细讨论支持向量机(support vector machine)(SVM)，它在一些年之前变得逐渐流行，可以用来解决分类问题、回归问题以及异常点检测问题。支持向量机的一个重要性质是模型参数的确定对应于一个凸最优化问题，因此许多局部解也是全局最优解。

SVM是一个决策机器，因此不提供后验概率。另一种稀疏核方法，被称为相关向量机(relevance vector machine)(RVM)，基于贝叶斯方法，提供了后验概率的输出，并且通常能产生比SVM更稀疏的解。