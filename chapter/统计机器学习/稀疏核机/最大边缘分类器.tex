\section{最大边缘分类器}
现阶段，我们假设训练数据集在特征空间中是线性可分的，即根据定义，存在至少一个参数$\boldsymbol{w}$和$b$的选择方式，使得对于$t_n=+1$的点，$y(\boldsymbol{x}_n)>0$，对于$t_n=-1$的点，都有$y(\boldsymbol{x}_n)<0$，从而对于所有训练数据点，都有$t_ny(\boldsymbol{x}_n)>0$。

当然，存在许多能够把类别精确分开的解。我们介绍过感知器算法，它能够保证在有限步骤之内找到一个解。然而，它找到的这个解依赖于$\boldsymbol{w}$和$b$的(任意的)初始值选择，还依赖于数据点出现的顺序。如果有多个能够精确分类训练数据点的解，那么我们应该尝试寻找泛化错误最小的那个解。支持向量机解决这个问题的方法是：引入边缘(margin)的概念，这个概念被定义为决策边界与任意样本之间的最小距离。

在支持向量机中，决策边界被选为使边缘最大化的那个决策边界。采用最大边缘解的动机可以通过计算学习理论或者统计学习理论进行理解。对于一个简单的线性可分数据集，在贝叶斯方法中，关于参数的先验概率分布进行积分或求和，可以产生一个决策边界，这个决策边界位于分开数据点的区域中间。最大边缘解有着类似的行为。

对偶问题使得模型能够用核函数重新表示，因此最大边缘分类器可以被高效地应用于维数超过数据点个数的特征空间，包括无穷维特征空间。为了使用训练过的模型分类新的数据点，$y(\boldsymbol{x})$可以根据参数$\{a_n \}$和核函数表示，即
\begin{equation}
	y(\boldsymbol{x})=\sum_{n=1}^{N}a_nt_nk(\boldsymbol{x},\boldsymbol{x}_n)+b
\end{equation}
这种形式的限制的最优化问题满足KKT条件。在这个问题中，下面三个性质要成立 
\begin{flalign}
	a_n\geqslant 0\\
	t_ny(\boldsymbol{x})-1 \geqslant 0\\
	a_n\{t_ny(\boldsymbol{x})-1 \}=0
\end{flalign}
因此对于每个数据点，要么$a_n=0$，要么$t_ny(\boldsymbol{x}_n)=1$。任何使得$a_n=0$的数据点都不会出现在公式的求和式中，因此对新数据点的预测没有作用。剩下的数据点被称为支持向量。这个性质是支持向量机在实际应用中的核心。一旦模型被训练完毕，相当多的数据点都可以被丢弃，只有支持向量被保留。说明了SVM稀疏性的来源。
\subsection*{重叠类分布}
\subsection*{与logistic回归的关系}
\subsection*{多类SVM}
\subsection*{回归问题的SVM}
\subsection*{计算学习理论}